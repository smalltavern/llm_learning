{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, get_linear_schedule_with_warmup, TrainingArguments, BertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "\n",
    "    \"\"\"配置参数\"\"\"\n",
    "    def __init__(self, dataset):\n",
    "        self.model_name = 'bert'\n",
    "        self.train_path = dataset + '/homework/train.txt'                                # 训练集                                  # 验证集\n",
    "        self.test_path = dataset + '/homework/test.txt'                                  # 测试集\n",
    "        self.class_list = [x.strip() for x in open(\n",
    "            dataset + '/homework/class.txt').readlines()]                                # 类别名单\n",
    "        self.save_path = dataset + '/saved_dict/' + self.model_name       # 模型训练结果\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "        self.require_improvement = 1000                                 # 若超过1000batch效果还没提升，则提前结束训练\n",
    "        self.num_classes = len(self.class_list)                         # 类别数\n",
    "        self.num_epochs = 50                                        # epoch数\n",
    "        self.batch_size = 32                                           # mini-batch大小\n",
    "        self.pad_size = 64                                              # 每句话处理成的长度(短填长切)\n",
    "        self.learning_rate = 5e-5                                       # 学习率\n",
    "        self.bert_path = '/opt/data/private/huggingface/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.hidden_size = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "        self.sigmod = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子一个size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        out = self.bert(context, attention_mask=mask)\n",
    "        out = out.last_hidden_state[:, 0]\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmod(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIterater(object):\n",
    "    def __init__(self, batches, batch_size, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = batches\n",
    "        self.n_batches = len(batches) // batch_size\n",
    "        self.residue = False  # 记录batch数量是否为整数\n",
    "        if len(batches) % self.n_batches != 0:\n",
    "            self.residue = True\n",
    "        self.index = 0\n",
    "        self.device = device\n",
    "\n",
    "    def _to_tensor(self, datas):\n",
    "        x = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        y = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "\n",
    "        # pad前的长度(超过pad_size的设为pad_size)\n",
    "        seq_len = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        mask = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "        return (x, seq_len, mask), y\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.residue and self.index == self.n_batches:\n",
    "            batches = self.batches[self.index * self.batch_size: len(self.batches)]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "        elif self.index >= self.n_batches:\n",
    "            self.index = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batches = self.batches[self.index * self.batch_size: (self.index + 1) * self.batch_size]\n",
    "            self.index += 1\n",
    "            batches = self._to_tensor(batches)\n",
    "            return batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.residue:\n",
    "            return self.n_batches + 1\n",
    "        else:\n",
    "            return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD, CLS = '[PAD]', '[CLS]'\n",
    "def build_dataset(config):\n",
    "    def load_dataset(path, pad_size=64):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in f:\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content = lin.split('\\t')[0]\n",
    "                label_list = [w.split('#')[0] for w in lin.split('\\t')[1:]]\n",
    "                label = [0] * config.num_classes\n",
    "                for l in label_list:\n",
    "                    label[config.class_list.index(l)] = 1\n",
    "                token = config.tokenizer.tokenize(content)\n",
    "                token = [CLS] + token\n",
    "                seq_len = len(token)\n",
    "                mask = []\n",
    "                token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "                if pad_size:\n",
    "                    if len(token) < pad_size:\n",
    "                        mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "                        token_ids += ([0] * (pad_size - len(token)))\n",
    "                    else:\n",
    "                        mask = [1] * pad_size\n",
    "                        token_ids = token_ids[:pad_size]\n",
    "                        seq_len = pad_size\n",
    "                contents.append((token_ids, label, seq_len, mask))\n",
    "        return contents\n",
    "    train = load_dataset(config.train_path, config.pad_size)\n",
    "    test = load_dataset(config.test_path, config.pad_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'data'\n",
    "config = Config(dataset)\n",
    "train_data, test_data = build_dataset(config)\n",
    "train_iter = DatasetIterater(train_data, config.batch_size, config.device)\n",
    "test_iter = DatasetIterater(test_data, config.batch_size, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config, model, data_iter, test=False):\n",
    "    model.eval()\n",
    "    loss_total = 0\n",
    "    predict_all = []\n",
    "    labels_all = []\n",
    "    criterion = nn.BCELoss()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_iter:\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss_total += loss\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            predic = (outputs.data > 0.5).int().cpu().numpy()\n",
    "            assert len(labels) == len(predic)\n",
    "            for i in range(len(labels)):\n",
    "                labels_all.append(labels[i])\n",
    "                predict_all.append(predic[i])\n",
    "\n",
    "    acc = metrics.accuracy_score(labels_all, predict_all)\n",
    "    if test:\n",
    "        report = metrics.classification_report(np.array(labels_all), np.array(predict_all), target_names=config.class_list, digits=4)\n",
    "        return acc, loss_total / len(data_iter), report\n",
    "    return acc, loss_total / len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, train_iter, test_iter):\n",
    "    model.train()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                         lr=config.learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    total_batch = 0  # 记录进行到多少batch\n",
    "    flag = False  # 记录是否很久没有效果提升\n",
    "    train_best_loss = float('inf')\n",
    "    model.train()\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print('Epoch [{}/{}]'.format(epoch + 1, config.num_epochs))\n",
    "        for i, (trains, labels) in enumerate(train_iter):\n",
    "            outputs = model(trains)\n",
    "            model.zero_grad()\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if total_batch % 10 == 0:\n",
    "                # 每多少轮输出在训练集和验证集上的效果\n",
    "                true = labels.data.cpu()\n",
    "                predic = (outputs.data > 0.5).int().cpu()\n",
    "                train_acc = metrics.accuracy_score(true, predic)\n",
    "                msg = 'Iter: {0:>3},    Train Loss: {1:>3.2},     Train Acc: {2:>3.2%}'\n",
    "                print(msg.format(total_batch, loss.item(), train_acc))\n",
    "            total_batch += 1\n",
    "        torch.save(model.state_dict(), config.save_path+ '_{}.ckpt'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model, test_iter):\n",
    "    # test\n",
    "    model.load_state_dict(torch.load(config.save_path + '_{}.ckpt'.format(config.num_epochs - 1)))\n",
    "    model.eval()\n",
    "    test_acc, test_loss, test_report = evaluate(config, model, test_iter, test=True)\n",
    "    msg = 'Test Loss: {0:>3.2},  Test Acc: {1:>3.2%}'\n",
    "    print(msg.format(test_loss, test_acc))\n",
    "    print(\"Precision, Recall and F1-Score...\")\n",
    "    print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /opt/data/private/huggingface/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]\n",
      "Iter:   0,    Train Loss: 0.68,     Train Acc: 0.00%\n",
      "Iter:  10,    Train Loss: 0.33,     Train Acc: 0.00%\n",
      "Iter:  20,    Train Loss: 0.32,     Train Acc: 15.62%\n",
      "Iter:  30,    Train Loss: 0.5,     Train Acc: 9.38%\n",
      "Iter:  40,    Train Loss: 0.22,     Train Acc: 53.12%\n",
      "Iter:  50,    Train Loss: 0.25,     Train Acc: 18.75%\n",
      "Iter:  60,    Train Loss: 0.23,     Train Acc: 46.88%\n",
      "Iter:  70,    Train Loss: 0.12,     Train Acc: 81.25%\n",
      "Epoch [2/50]\n",
      "Iter:  80,    Train Loss: 0.11,     Train Acc: 78.12%\n",
      "Iter:  90,    Train Loss: 0.2,     Train Acc: 65.62%\n",
      "Iter: 100,    Train Loss: 0.16,     Train Acc: 68.75%\n",
      "Iter: 110,    Train Loss: 0.1,     Train Acc: 84.38%\n",
      "Iter: 120,    Train Loss: 0.19,     Train Acc: 56.25%\n",
      "Iter: 130,    Train Loss: 0.43,     Train Acc: 12.50%\n",
      "Iter: 140,    Train Loss: 0.16,     Train Acc: 68.75%\n",
      "Iter: 150,    Train Loss: 0.077,     Train Acc: 90.62%\n",
      "Iter: 160,    Train Loss: 0.13,     Train Acc: 68.75%\n",
      "Iter: 170,    Train Loss: 0.12,     Train Acc: 68.75%\n",
      "Iter: 180,    Train Loss: 0.057,     Train Acc: 90.62%\n",
      "Iter: 190,    Train Loss: 0.095,     Train Acc: 78.12%\n",
      "Iter: 200,    Train Loss: 0.087,     Train Acc: 84.38%\n",
      "Iter: 210,    Train Loss: 0.072,     Train Acc: 87.50%\n",
      "Iter: 220,    Train Loss: 0.15,     Train Acc: 62.50%\n",
      "Iter: 230,    Train Loss: 0.15,     Train Acc: 68.75%\n",
      "Iter: 240,    Train Loss: 0.11,     Train Acc: 75.00%\n",
      "Iter: 250,    Train Loss: 0.091,     Train Acc: 84.38%\n",
      "Iter: 260,    Train Loss: 0.16,     Train Acc: 59.38%\n",
      "Iter: 270,    Train Loss: 0.09,     Train Acc: 81.25%\n",
      "Iter: 280,    Train Loss: 0.35,     Train Acc: 21.88%\n",
      "Iter: 290,    Train Loss: 0.24,     Train Acc: 40.62%\n",
      "Iter: 300,    Train Loss: 0.1,     Train Acc: 81.25%\n",
      "Iter: 310,    Train Loss: 0.12,     Train Acc: 71.88%\n",
      "Iter: 320,    Train Loss: 0.054,     Train Acc: 81.25%\n",
      "Epoch [3/50]\n",
      "Iter: 330,    Train Loss: 0.052,     Train Acc: 87.50%\n",
      "Iter: 340,    Train Loss: 0.09,     Train Acc: 81.25%\n",
      "Iter: 350,    Train Loss: 0.1,     Train Acc: 78.12%\n",
      "Iter: 360,    Train Loss: 0.057,     Train Acc: 90.62%\n",
      "Iter: 370,    Train Loss: 0.14,     Train Acc: 62.50%\n",
      "Iter: 380,    Train Loss: 0.3,     Train Acc: 43.75%\n",
      "Iter: 390,    Train Loss: 0.12,     Train Acc: 65.62%\n",
      "Iter: 400,    Train Loss: 0.053,     Train Acc: 93.75%\n",
      "Iter: 410,    Train Loss: 0.12,     Train Acc: 71.88%\n",
      "Iter: 420,    Train Loss: 0.082,     Train Acc: 84.38%\n",
      "Iter: 430,    Train Loss: 0.045,     Train Acc: 93.75%\n",
      "Iter: 440,    Train Loss: 0.079,     Train Acc: 81.25%\n",
      "Iter: 450,    Train Loss: 0.073,     Train Acc: 84.38%\n",
      "Iter: 460,    Train Loss: 0.04,     Train Acc: 96.88%\n",
      "Iter: 470,    Train Loss: 0.15,     Train Acc: 65.62%\n",
      "Iter: 480,    Train Loss: 0.15,     Train Acc: 71.88%\n",
      "Iter: 490,    Train Loss: 0.088,     Train Acc: 75.00%\n",
      "Iter: 500,    Train Loss: 0.072,     Train Acc: 90.62%\n",
      "Iter: 510,    Train Loss: 0.12,     Train Acc: 62.50%\n",
      "Iter: 520,    Train Loss: 0.073,     Train Acc: 81.25%\n",
      "Iter: 530,    Train Loss: 0.26,     Train Acc: 43.75%\n",
      "Iter: 540,    Train Loss: 0.17,     Train Acc: 59.38%\n",
      "Iter: 550,    Train Loss: 0.05,     Train Acc: 90.62%\n",
      "Iter: 560,    Train Loss: 0.091,     Train Acc: 78.12%\n",
      "Iter: 570,    Train Loss: 0.03,     Train Acc: 96.88%\n",
      "Epoch [4/50]\n",
      "Iter: 580,    Train Loss: 0.042,     Train Acc: 87.50%\n",
      "Iter: 590,    Train Loss: 0.083,     Train Acc: 78.12%\n",
      "Iter: 600,    Train Loss: 0.1,     Train Acc: 75.00%\n",
      "Iter: 610,    Train Loss: 0.056,     Train Acc: 87.50%\n",
      "Iter: 620,    Train Loss: 0.098,     Train Acc: 71.88%\n",
      "Iter: 630,    Train Loss: 0.26,     Train Acc: 50.00%\n",
      "Iter: 640,    Train Loss: 0.12,     Train Acc: 68.75%\n",
      "Iter: 650,    Train Loss: 0.044,     Train Acc: 93.75%\n",
      "Iter: 660,    Train Loss: 0.089,     Train Acc: 78.12%\n",
      "Iter: 670,    Train Loss: 0.065,     Train Acc: 87.50%\n",
      "Iter: 680,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 690,    Train Loss: 0.061,     Train Acc: 84.38%\n",
      "Iter: 700,    Train Loss: 0.048,     Train Acc: 90.62%\n",
      "Iter: 710,    Train Loss: 0.038,     Train Acc: 90.62%\n",
      "Iter: 720,    Train Loss: 0.13,     Train Acc: 68.75%\n",
      "Iter: 730,    Train Loss: 0.12,     Train Acc: 75.00%\n",
      "Iter: 740,    Train Loss: 0.074,     Train Acc: 81.25%\n",
      "Iter: 750,    Train Loss: 0.075,     Train Acc: 84.38%\n",
      "Iter: 760,    Train Loss: 0.087,     Train Acc: 78.12%\n",
      "Iter: 770,    Train Loss: 0.066,     Train Acc: 81.25%\n",
      "Iter: 780,    Train Loss: 0.21,     Train Acc: 50.00%\n",
      "Iter: 790,    Train Loss: 0.13,     Train Acc: 75.00%\n",
      "Iter: 800,    Train Loss: 0.049,     Train Acc: 87.50%\n",
      "Iter: 810,    Train Loss: 0.074,     Train Acc: 78.12%\n",
      "Iter: 820,    Train Loss: 0.025,     Train Acc: 93.75%\n",
      "Epoch [5/50]\n",
      "Iter: 830,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 840,    Train Loss: 0.076,     Train Acc: 81.25%\n",
      "Iter: 850,    Train Loss: 0.094,     Train Acc: 75.00%\n",
      "Iter: 860,    Train Loss: 0.048,     Train Acc: 90.62%\n",
      "Iter: 870,    Train Loss: 0.092,     Train Acc: 78.12%\n",
      "Iter: 880,    Train Loss: 0.21,     Train Acc: 53.12%\n",
      "Iter: 890,    Train Loss: 0.095,     Train Acc: 78.12%\n",
      "Iter: 900,    Train Loss: 0.033,     Train Acc: 93.75%\n",
      "Iter: 910,    Train Loss: 0.08,     Train Acc: 78.12%\n",
      "Iter: 920,    Train Loss: 0.062,     Train Acc: 87.50%\n",
      "Iter: 930,    Train Loss: 0.031,     Train Acc: 90.62%\n",
      "Iter: 940,    Train Loss: 0.05,     Train Acc: 87.50%\n",
      "Iter: 950,    Train Loss: 0.05,     Train Acc: 87.50%\n",
      "Iter: 960,    Train Loss: 0.021,     Train Acc: 100.00%\n",
      "Iter: 970,    Train Loss: 0.11,     Train Acc: 75.00%\n",
      "Iter: 980,    Train Loss: 0.097,     Train Acc: 84.38%\n",
      "Iter: 990,    Train Loss: 0.066,     Train Acc: 81.25%\n",
      "Iter: 1000,    Train Loss: 0.086,     Train Acc: 87.50%\n",
      "Iter: 1010,    Train Loss: 0.1,     Train Acc: 71.88%\n",
      "Iter: 1020,    Train Loss: 0.055,     Train Acc: 81.25%\n",
      "Iter: 1030,    Train Loss: 0.19,     Train Acc: 53.12%\n",
      "Iter: 1040,    Train Loss: 0.065,     Train Acc: 81.25%\n",
      "Iter: 1050,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 1060,    Train Loss: 0.058,     Train Acc: 81.25%\n",
      "Iter: 1070,    Train Loss: 0.021,     Train Acc: 96.88%\n",
      "Epoch [6/50]\n",
      "Iter: 1080,    Train Loss: 0.0089,     Train Acc: 100.00%\n",
      "Iter: 1090,    Train Loss: 0.037,     Train Acc: 96.88%\n",
      "Iter: 1100,    Train Loss: 0.059,     Train Acc: 78.12%\n",
      "Iter: 1110,    Train Loss: 0.045,     Train Acc: 90.62%\n",
      "Iter: 1120,    Train Loss: 0.08,     Train Acc: 78.12%\n",
      "Iter: 1130,    Train Loss: 0.19,     Train Acc: 62.50%\n",
      "Iter: 1140,    Train Loss: 0.08,     Train Acc: 78.12%\n",
      "Iter: 1150,    Train Loss: 0.045,     Train Acc: 90.62%\n",
      "Iter: 1160,    Train Loss: 0.081,     Train Acc: 78.12%\n",
      "Iter: 1170,    Train Loss: 0.047,     Train Acc: 87.50%\n",
      "Iter: 1180,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 1190,    Train Loss: 0.042,     Train Acc: 87.50%\n",
      "Iter: 1200,    Train Loss: 0.043,     Train Acc: 90.62%\n",
      "Iter: 1210,    Train Loss: 0.021,     Train Acc: 96.88%\n",
      "Iter: 1220,    Train Loss: 0.1,     Train Acc: 81.25%\n",
      "Iter: 1230,    Train Loss: 0.094,     Train Acc: 75.00%\n",
      "Iter: 1240,    Train Loss: 0.06,     Train Acc: 84.38%\n",
      "Iter: 1250,    Train Loss: 0.06,     Train Acc: 90.62%\n",
      "Iter: 1260,    Train Loss: 0.076,     Train Acc: 75.00%\n",
      "Iter: 1270,    Train Loss: 0.036,     Train Acc: 96.88%\n",
      "Iter: 1280,    Train Loss: 0.17,     Train Acc: 65.62%\n",
      "Iter: 1290,    Train Loss: 0.064,     Train Acc: 81.25%\n",
      "Iter: 1300,    Train Loss: 0.025,     Train Acc: 93.75%\n",
      "Iter: 1310,    Train Loss: 0.045,     Train Acc: 93.75%\n",
      "Iter: 1320,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Epoch [7/50]\n",
      "Iter: 1330,    Train Loss: 0.0071,     Train Acc: 100.00%\n",
      "Iter: 1340,    Train Loss: 0.029,     Train Acc: 93.75%\n",
      "Iter: 1350,    Train Loss: 0.059,     Train Acc: 84.38%\n",
      "Iter: 1360,    Train Loss: 0.029,     Train Acc: 90.62%\n",
      "Iter: 1370,    Train Loss: 0.073,     Train Acc: 71.88%\n",
      "Iter: 1380,    Train Loss: 0.23,     Train Acc: 59.38%\n",
      "Iter: 1390,    Train Loss: 0.078,     Train Acc: 78.12%\n",
      "Iter: 1400,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 1410,    Train Loss: 0.066,     Train Acc: 81.25%\n",
      "Iter: 1420,    Train Loss: 0.046,     Train Acc: 87.50%\n",
      "Iter: 1430,    Train Loss: 0.025,     Train Acc: 93.75%\n",
      "Iter: 1440,    Train Loss: 0.044,     Train Acc: 87.50%\n",
      "Iter: 1450,    Train Loss: 0.039,     Train Acc: 90.62%\n",
      "Iter: 1460,    Train Loss: 0.02,     Train Acc: 96.88%\n",
      "Iter: 1470,    Train Loss: 0.064,     Train Acc: 78.12%\n",
      "Iter: 1480,    Train Loss: 0.1,     Train Acc: 81.25%\n",
      "Iter: 1490,    Train Loss: 0.052,     Train Acc: 84.38%\n",
      "Iter: 1500,    Train Loss: 0.035,     Train Acc: 96.88%\n",
      "Iter: 1510,    Train Loss: 0.066,     Train Acc: 75.00%\n",
      "Iter: 1520,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 1530,    Train Loss: 0.15,     Train Acc: 71.88%\n",
      "Iter: 1540,    Train Loss: 0.11,     Train Acc: 75.00%\n",
      "Iter: 1550,    Train Loss: 0.02,     Train Acc: 96.88%\n",
      "Iter: 1560,    Train Loss: 0.042,     Train Acc: 87.50%\n",
      "Iter: 1570,    Train Loss: 0.021,     Train Acc: 93.75%\n",
      "Epoch [8/50]\n",
      "Iter: 1580,    Train Loss: 0.0061,     Train Acc: 100.00%\n",
      "Iter: 1590,    Train Loss: 0.066,     Train Acc: 87.50%\n",
      "Iter: 1600,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 1610,    Train Loss: 0.017,     Train Acc: 93.75%\n",
      "Iter: 1620,    Train Loss: 0.056,     Train Acc: 81.25%\n",
      "Iter: 1630,    Train Loss: 0.14,     Train Acc: 68.75%\n",
      "Iter: 1640,    Train Loss: 0.057,     Train Acc: 87.50%\n",
      "Iter: 1650,    Train Loss: 0.033,     Train Acc: 96.88%\n",
      "Iter: 1660,    Train Loss: 0.059,     Train Acc: 81.25%\n",
      "Iter: 1670,    Train Loss: 0.026,     Train Acc: 96.88%\n",
      "Iter: 1680,    Train Loss: 0.029,     Train Acc: 90.62%\n",
      "Iter: 1690,    Train Loss: 0.038,     Train Acc: 90.62%\n",
      "Iter: 1700,    Train Loss: 0.039,     Train Acc: 84.38%\n",
      "Iter: 1710,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 1720,    Train Loss: 0.063,     Train Acc: 84.38%\n",
      "Iter: 1730,    Train Loss: 0.088,     Train Acc: 81.25%\n",
      "Iter: 1740,    Train Loss: 0.054,     Train Acc: 84.38%\n",
      "Iter: 1750,    Train Loss: 0.032,     Train Acc: 93.75%\n",
      "Iter: 1760,    Train Loss: 0.053,     Train Acc: 81.25%\n",
      "Iter: 1770,    Train Loss: 0.036,     Train Acc: 93.75%\n",
      "Iter: 1780,    Train Loss: 0.15,     Train Acc: 62.50%\n",
      "Iter: 1790,    Train Loss: 0.039,     Train Acc: 81.25%\n",
      "Iter: 1800,    Train Loss: 0.026,     Train Acc: 96.88%\n",
      "Iter: 1810,    Train Loss: 0.047,     Train Acc: 81.25%\n",
      "Iter: 1820,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Epoch [9/50]\n",
      "Iter: 1830,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 1840,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 1850,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 1860,    Train Loss: 0.023,     Train Acc: 90.62%\n",
      "Iter: 1870,    Train Loss: 0.045,     Train Acc: 81.25%\n",
      "Iter: 1880,    Train Loss: 0.16,     Train Acc: 71.88%\n",
      "Iter: 1890,    Train Loss: 0.039,     Train Acc: 90.62%\n",
      "Iter: 1900,    Train Loss: 0.022,     Train Acc: 96.88%\n",
      "Iter: 1910,    Train Loss: 0.046,     Train Acc: 81.25%\n",
      "Iter: 1920,    Train Loss: 0.028,     Train Acc: 96.88%\n",
      "Iter: 1930,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 1940,    Train Loss: 0.012,     Train Acc: 100.00%\n",
      "Iter: 1950,    Train Loss: 0.036,     Train Acc: 87.50%\n",
      "Iter: 1960,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 1970,    Train Loss: 0.042,     Train Acc: 90.62%\n",
      "Iter: 1980,    Train Loss: 0.068,     Train Acc: 81.25%\n",
      "Iter: 1990,    Train Loss: 0.039,     Train Acc: 93.75%\n",
      "Iter: 2000,    Train Loss: 0.013,     Train Acc: 100.00%\n",
      "Iter: 2010,    Train Loss: 0.071,     Train Acc: 75.00%\n",
      "Iter: 2020,    Train Loss: 0.047,     Train Acc: 87.50%\n",
      "Iter: 2030,    Train Loss: 0.11,     Train Acc: 68.75%\n",
      "Iter: 2040,    Train Loss: 0.036,     Train Acc: 87.50%\n",
      "Iter: 2050,    Train Loss: 0.022,     Train Acc: 96.88%\n",
      "Iter: 2060,    Train Loss: 0.038,     Train Acc: 93.75%\n",
      "Iter: 2070,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Epoch [10/50]\n",
      "Iter: 2080,    Train Loss: 0.0065,     Train Acc: 100.00%\n",
      "Iter: 2090,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 2100,    Train Loss: 0.043,     Train Acc: 87.50%\n",
      "Iter: 2110,    Train Loss: 0.0082,     Train Acc: 100.00%\n",
      "Iter: 2120,    Train Loss: 0.047,     Train Acc: 84.38%\n",
      "Iter: 2130,    Train Loss: 0.21,     Train Acc: 68.75%\n",
      "Iter: 2140,    Train Loss: 0.062,     Train Acc: 71.88%\n",
      "Iter: 2150,    Train Loss: 0.035,     Train Acc: 90.62%\n",
      "Iter: 2160,    Train Loss: 0.044,     Train Acc: 90.62%\n",
      "Iter: 2170,    Train Loss: 0.024,     Train Acc: 96.88%\n",
      "Iter: 2180,    Train Loss: 0.021,     Train Acc: 96.88%\n",
      "Iter: 2190,    Train Loss: 0.043,     Train Acc: 90.62%\n",
      "Iter: 2200,    Train Loss: 0.039,     Train Acc: 87.50%\n",
      "Iter: 2210,    Train Loss: 0.0099,     Train Acc: 100.00%\n",
      "Iter: 2220,    Train Loss: 0.045,     Train Acc: 84.38%\n",
      "Iter: 2230,    Train Loss: 0.056,     Train Acc: 87.50%\n",
      "Iter: 2240,    Train Loss: 0.043,     Train Acc: 87.50%\n",
      "Iter: 2250,    Train Loss: 0.0067,     Train Acc: 100.00%\n",
      "Iter: 2260,    Train Loss: 0.063,     Train Acc: 78.12%\n",
      "Iter: 2270,    Train Loss: 0.046,     Train Acc: 93.75%\n",
      "Iter: 2280,    Train Loss: 0.11,     Train Acc: 78.12%\n",
      "Iter: 2290,    Train Loss: 0.03,     Train Acc: 87.50%\n",
      "Iter: 2300,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 2310,    Train Loss: 0.044,     Train Acc: 87.50%\n",
      "Iter: 2320,    Train Loss: 0.0076,     Train Acc: 100.00%\n",
      "Epoch [11/50]\n",
      "Iter: 2330,    Train Loss: 0.029,     Train Acc: 96.88%\n",
      "Iter: 2340,    Train Loss: 0.012,     Train Acc: 100.00%\n",
      "Iter: 2350,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 2360,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 2370,    Train Loss: 0.039,     Train Acc: 87.50%\n",
      "Iter: 2380,    Train Loss: 0.094,     Train Acc: 81.25%\n",
      "Iter: 2390,    Train Loss: 0.035,     Train Acc: 87.50%\n",
      "Iter: 2400,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 2410,    Train Loss: 0.021,     Train Acc: 90.62%\n",
      "Iter: 2420,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 2430,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 2440,    Train Loss: 0.0081,     Train Acc: 100.00%\n",
      "Iter: 2450,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 2460,    Train Loss: 0.0064,     Train Acc: 100.00%\n",
      "Iter: 2470,    Train Loss: 0.038,     Train Acc: 84.38%\n",
      "Iter: 2480,    Train Loss: 0.08,     Train Acc: 87.50%\n",
      "Iter: 2490,    Train Loss: 0.035,     Train Acc: 87.50%\n",
      "Iter: 2500,    Train Loss: 0.0099,     Train Acc: 100.00%\n",
      "Iter: 2510,    Train Loss: 0.045,     Train Acc: 81.25%\n",
      "Iter: 2520,    Train Loss: 0.024,     Train Acc: 93.75%\n",
      "Iter: 2530,    Train Loss: 0.098,     Train Acc: 75.00%\n",
      "Iter: 2540,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 2550,    Train Loss: 0.034,     Train Acc: 93.75%\n",
      "Iter: 2560,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 2570,    Train Loss: 0.0098,     Train Acc: 96.88%\n",
      "Epoch [12/50]\n",
      "Iter: 2580,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 2590,    Train Loss: 0.0063,     Train Acc: 100.00%\n",
      "Iter: 2600,    Train Loss: 0.026,     Train Acc: 87.50%\n",
      "Iter: 2610,    Train Loss: 0.007,     Train Acc: 100.00%\n",
      "Iter: 2620,    Train Loss: 0.036,     Train Acc: 93.75%\n",
      "Iter: 2630,    Train Loss: 0.12,     Train Acc: 81.25%\n",
      "Iter: 2640,    Train Loss: 0.03,     Train Acc: 93.75%\n",
      "Iter: 2650,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 2660,    Train Loss: 0.036,     Train Acc: 90.62%\n",
      "Iter: 2670,    Train Loss: 0.023,     Train Acc: 96.88%\n",
      "Iter: 2680,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 2690,    Train Loss: 0.027,     Train Acc: 96.88%\n",
      "Iter: 2700,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 2710,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 2720,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 2730,    Train Loss: 0.054,     Train Acc: 90.62%\n",
      "Iter: 2740,    Train Loss: 0.039,     Train Acc: 90.62%\n",
      "Iter: 2750,    Train Loss: 0.0092,     Train Acc: 100.00%\n",
      "Iter: 2760,    Train Loss: 0.024,     Train Acc: 93.75%\n",
      "Iter: 2770,    Train Loss: 0.011,     Train Acc: 100.00%\n",
      "Iter: 2780,    Train Loss: 0.095,     Train Acc: 71.88%\n",
      "Iter: 2790,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 2800,    Train Loss: 0.006,     Train Acc: 100.00%\n",
      "Iter: 2810,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 2820,    Train Loss: 0.0055,     Train Acc: 100.00%\n",
      "Epoch [13/50]\n",
      "Iter: 2830,    Train Loss: 0.0059,     Train Acc: 96.88%\n",
      "Iter: 2840,    Train Loss: 0.0072,     Train Acc: 100.00%\n",
      "Iter: 2850,    Train Loss: 0.05,     Train Acc: 87.50%\n",
      "Iter: 2860,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 2870,    Train Loss: 0.05,     Train Acc: 87.50%\n",
      "Iter: 2880,    Train Loss: 0.11,     Train Acc: 71.88%\n",
      "Iter: 2890,    Train Loss: 0.031,     Train Acc: 90.62%\n",
      "Iter: 2900,    Train Loss: 0.018,     Train Acc: 96.88%\n",
      "Iter: 2910,    Train Loss: 0.027,     Train Acc: 87.50%\n",
      "Iter: 2920,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 2930,    Train Loss: 0.0048,     Train Acc: 100.00%\n",
      "Iter: 2940,    Train Loss: 0.0067,     Train Acc: 100.00%\n",
      "Iter: 2950,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 2960,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 2970,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 2980,    Train Loss: 0.044,     Train Acc: 93.75%\n",
      "Iter: 2990,    Train Loss: 0.034,     Train Acc: 90.62%\n",
      "Iter: 3000,    Train Loss: 0.0068,     Train Acc: 100.00%\n",
      "Iter: 3010,    Train Loss: 0.045,     Train Acc: 78.12%\n",
      "Iter: 3020,    Train Loss: 0.0087,     Train Acc: 100.00%\n",
      "Iter: 3030,    Train Loss: 0.088,     Train Acc: 87.50%\n",
      "Iter: 3040,    Train Loss: 0.039,     Train Acc: 84.38%\n",
      "Iter: 3050,    Train Loss: 0.0056,     Train Acc: 100.00%\n",
      "Iter: 3060,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 3070,    Train Loss: 0.0057,     Train Acc: 100.00%\n",
      "Epoch [14/50]\n",
      "Iter: 3080,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 3090,    Train Loss: 0.0063,     Train Acc: 100.00%\n",
      "Iter: 3100,    Train Loss: 0.011,     Train Acc: 100.00%\n",
      "Iter: 3110,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 3120,    Train Loss: 0.062,     Train Acc: 90.62%\n",
      "Iter: 3130,    Train Loss: 0.16,     Train Acc: 78.12%\n",
      "Iter: 3140,    Train Loss: 0.031,     Train Acc: 90.62%\n",
      "Iter: 3150,    Train Loss: 0.0059,     Train Acc: 100.00%\n",
      "Iter: 3160,    Train Loss: 0.023,     Train Acc: 93.75%\n",
      "Iter: 3170,    Train Loss: 0.046,     Train Acc: 93.75%\n",
      "Iter: 3180,    Train Loss: 0.0043,     Train Acc: 100.00%\n",
      "Iter: 3190,    Train Loss: 0.0059,     Train Acc: 100.00%\n",
      "Iter: 3200,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 3210,    Train Loss: 0.0071,     Train Acc: 96.88%\n",
      "Iter: 3220,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 3230,    Train Loss: 0.046,     Train Acc: 90.62%\n",
      "Iter: 3240,    Train Loss: 0.045,     Train Acc: 87.50%\n",
      "Iter: 3250,    Train Loss: 0.0045,     Train Acc: 100.00%\n",
      "Iter: 3260,    Train Loss: 0.022,     Train Acc: 93.75%\n",
      "Iter: 3270,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 3280,    Train Loss: 0.12,     Train Acc: 78.12%\n",
      "Iter: 3290,    Train Loss: 0.052,     Train Acc: 90.62%\n",
      "Iter: 3300,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 3310,    Train Loss: 0.024,     Train Acc: 90.62%\n",
      "Iter: 3320,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Epoch [15/50]\n",
      "Iter: 3330,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 3340,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 3350,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 3360,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 3370,    Train Loss: 0.028,     Train Acc: 93.75%\n",
      "Iter: 3380,    Train Loss: 0.049,     Train Acc: 90.62%\n",
      "Iter: 3390,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 3400,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 3410,    Train Loss: 0.0093,     Train Acc: 96.88%\n",
      "Iter: 3420,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 3430,    Train Loss: 0.0043,     Train Acc: 100.00%\n",
      "Iter: 3440,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 3450,    Train Loss: 0.0082,     Train Acc: 100.00%\n",
      "Iter: 3460,    Train Loss: 0.0032,     Train Acc: 100.00%\n",
      "Iter: 3470,    Train Loss: 0.0089,     Train Acc: 100.00%\n",
      "Iter: 3480,    Train Loss: 0.044,     Train Acc: 96.88%\n",
      "Iter: 3490,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 3500,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 3510,    Train Loss: 0.029,     Train Acc: 90.62%\n",
      "Iter: 3520,    Train Loss: 0.033,     Train Acc: 93.75%\n",
      "Iter: 3530,    Train Loss: 0.1,     Train Acc: 84.38%\n",
      "Iter: 3540,    Train Loss: 0.019,     Train Acc: 100.00%\n",
      "Iter: 3550,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 3560,    Train Loss: 0.011,     Train Acc: 100.00%\n",
      "Iter: 3570,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Epoch [16/50]\n",
      "Iter: 3580,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 3590,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 3600,    Train Loss: 0.0097,     Train Acc: 100.00%\n",
      "Iter: 3610,    Train Loss: 0.0092,     Train Acc: 96.88%\n",
      "Iter: 3620,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 3630,    Train Loss: 0.047,     Train Acc: 81.25%\n",
      "Iter: 3640,    Train Loss: 0.02,     Train Acc: 93.75%\n",
      "Iter: 3650,    Train Loss: 0.018,     Train Acc: 90.62%\n",
      "Iter: 3660,    Train Loss: 0.017,     Train Acc: 93.75%\n",
      "Iter: 3670,    Train Loss: 0.007,     Train Acc: 96.88%\n",
      "Iter: 3680,    Train Loss: 0.0048,     Train Acc: 100.00%\n",
      "Iter: 3690,    Train Loss: 0.0049,     Train Acc: 100.00%\n",
      "Iter: 3700,    Train Loss: 0.0093,     Train Acc: 96.88%\n",
      "Iter: 3710,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 3720,    Train Loss: 0.03,     Train Acc: 87.50%\n",
      "Iter: 3730,    Train Loss: 0.048,     Train Acc: 93.75%\n",
      "Iter: 3740,    Train Loss: 0.028,     Train Acc: 93.75%\n",
      "Iter: 3750,    Train Loss: 0.0062,     Train Acc: 100.00%\n",
      "Iter: 3760,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 3770,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 3780,    Train Loss: 0.11,     Train Acc: 71.88%\n",
      "Iter: 3790,    Train Loss: 0.0093,     Train Acc: 100.00%\n",
      "Iter: 3800,    Train Loss: 0.0091,     Train Acc: 96.88%\n",
      "Iter: 3810,    Train Loss: 0.01,     Train Acc: 100.00%\n",
      "Iter: 3820,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Epoch [17/50]\n",
      "Iter: 3830,    Train Loss: 0.0058,     Train Acc: 100.00%\n",
      "Iter: 3840,    Train Loss: 0.013,     Train Acc: 93.75%\n",
      "Iter: 3850,    Train Loss: 0.0085,     Train Acc: 96.88%\n",
      "Iter: 3860,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 3870,    Train Loss: 0.02,     Train Acc: 93.75%\n",
      "Iter: 3880,    Train Loss: 0.024,     Train Acc: 93.75%\n",
      "Iter: 3890,    Train Loss: 0.0048,     Train Acc: 100.00%\n",
      "Iter: 3900,    Train Loss: 0.018,     Train Acc: 96.88%\n",
      "Iter: 3910,    Train Loss: 0.025,     Train Acc: 93.75%\n",
      "Iter: 3920,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 3930,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 3940,    Train Loss: 0.0077,     Train Acc: 96.88%\n",
      "Iter: 3950,    Train Loss: 0.0068,     Train Acc: 100.00%\n",
      "Iter: 3960,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 3970,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 3980,    Train Loss: 0.05,     Train Acc: 90.62%\n",
      "Iter: 3990,    Train Loss: 0.026,     Train Acc: 93.75%\n",
      "Iter: 4000,    Train Loss: 0.0057,     Train Acc: 100.00%\n",
      "Iter: 4010,    Train Loss: 0.0097,     Train Acc: 96.88%\n",
      "Iter: 4020,    Train Loss: 0.0075,     Train Acc: 100.00%\n",
      "Iter: 4030,    Train Loss: 0.14,     Train Acc: 78.12%\n",
      "Iter: 4040,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 4050,    Train Loss: 0.0073,     Train Acc: 96.88%\n",
      "Iter: 4060,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 4070,    Train Loss: 0.0036,     Train Acc: 100.00%\n",
      "Epoch [18/50]\n",
      "Iter: 4080,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 4090,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 4100,    Train Loss: 0.0073,     Train Acc: 100.00%\n",
      "Iter: 4110,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 4120,    Train Loss: 0.079,     Train Acc: 90.62%\n",
      "Iter: 4130,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 4140,    Train Loss: 0.0059,     Train Acc: 100.00%\n",
      "Iter: 4150,    Train Loss: 0.0058,     Train Acc: 96.88%\n",
      "Iter: 4160,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 4170,    Train Loss: 0.0077,     Train Acc: 96.88%\n",
      "Iter: 4180,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 4190,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 4200,    Train Loss: 0.0076,     Train Acc: 100.00%\n",
      "Iter: 4210,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 4220,    Train Loss: 0.034,     Train Acc: 87.50%\n",
      "Iter: 4230,    Train Loss: 0.048,     Train Acc: 93.75%\n",
      "Iter: 4240,    Train Loss: 0.033,     Train Acc: 90.62%\n",
      "Iter: 4250,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 4260,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 4270,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 4280,    Train Loss: 0.095,     Train Acc: 81.25%\n",
      "Iter: 4290,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 4300,    Train Loss: 0.0059,     Train Acc: 100.00%\n",
      "Iter: 4310,    Train Loss: 0.018,     Train Acc: 90.62%\n",
      "Iter: 4320,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Epoch [19/50]\n",
      "Iter: 4330,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 4340,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 4350,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 4360,    Train Loss: 0.0045,     Train Acc: 100.00%\n",
      "Iter: 4370,    Train Loss: 0.049,     Train Acc: 93.75%\n",
      "Iter: 4380,    Train Loss: 0.069,     Train Acc: 81.25%\n",
      "Iter: 4390,    Train Loss: 0.0098,     Train Acc: 100.00%\n",
      "Iter: 4400,    Train Loss: 0.0053,     Train Acc: 100.00%\n",
      "Iter: 4410,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 4420,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 4430,    Train Loss: 0.0052,     Train Acc: 100.00%\n",
      "Iter: 4440,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 4450,    Train Loss: 0.0098,     Train Acc: 96.88%\n",
      "Iter: 4460,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 4470,    Train Loss: 0.012,     Train Acc: 100.00%\n",
      "Iter: 4480,    Train Loss: 0.034,     Train Acc: 93.75%\n",
      "Iter: 4490,    Train Loss: 0.024,     Train Acc: 96.88%\n",
      "Iter: 4500,    Train Loss: 0.018,     Train Acc: 96.88%\n",
      "Iter: 4510,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 4520,    Train Loss: 0.028,     Train Acc: 87.50%\n",
      "Iter: 4530,    Train Loss: 0.095,     Train Acc: 84.38%\n",
      "Iter: 4540,    Train Loss: 0.0054,     Train Acc: 100.00%\n",
      "Iter: 4550,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 4560,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 4570,    Train Loss: 0.0035,     Train Acc: 100.00%\n",
      "Epoch [20/50]\n",
      "Iter: 4580,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 4590,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 4600,    Train Loss: 0.0081,     Train Acc: 96.88%\n",
      "Iter: 4610,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 4620,    Train Loss: 0.057,     Train Acc: 90.62%\n",
      "Iter: 4630,    Train Loss: 0.052,     Train Acc: 81.25%\n",
      "Iter: 4640,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 4650,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 4660,    Train Loss: 0.0086,     Train Acc: 96.88%\n",
      "Iter: 4670,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 4680,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 4690,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 4700,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 4710,    Train Loss: 0.0061,     Train Acc: 100.00%\n",
      "Iter: 4720,    Train Loss: 0.017,     Train Acc: 87.50%\n",
      "Iter: 4730,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 4740,    Train Loss: 0.0095,     Train Acc: 96.88%\n",
      "Iter: 4750,    Train Loss: 0.006,     Train Acc: 96.88%\n",
      "Iter: 4760,    Train Loss: 0.0067,     Train Acc: 100.00%\n",
      "Iter: 4770,    Train Loss: 0.021,     Train Acc: 93.75%\n",
      "Iter: 4780,    Train Loss: 0.14,     Train Acc: 75.00%\n",
      "Iter: 4790,    Train Loss: 0.0058,     Train Acc: 100.00%\n",
      "Iter: 4800,    Train Loss: 0.024,     Train Acc: 96.88%\n",
      "Iter: 4810,    Train Loss: 0.0077,     Train Acc: 96.88%\n",
      "Iter: 4820,    Train Loss: 0.008,     Train Acc: 96.88%\n",
      "Epoch [21/50]\n",
      "Iter: 4830,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 4840,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 4850,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 4860,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 4870,    Train Loss: 0.05,     Train Acc: 93.75%\n",
      "Iter: 4880,    Train Loss: 0.038,     Train Acc: 87.50%\n",
      "Iter: 4890,    Train Loss: 0.0091,     Train Acc: 96.88%\n",
      "Iter: 4900,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 4910,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 4920,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 4930,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 4940,    Train Loss: 0.0045,     Train Acc: 100.00%\n",
      "Iter: 4950,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 4960,    Train Loss: 0.004,     Train Acc: 96.88%\n",
      "Iter: 4970,    Train Loss: 0.022,     Train Acc: 96.88%\n",
      "Iter: 4980,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 4990,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 5000,    Train Loss: 0.052,     Train Acc: 93.75%\n",
      "Iter: 5010,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 5020,    Train Loss: 0.022,     Train Acc: 96.88%\n",
      "Iter: 5030,    Train Loss: 0.087,     Train Acc: 81.25%\n",
      "Iter: 5040,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 5050,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 5060,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 5070,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Epoch [22/50]\n",
      "Iter: 5080,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 5090,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 5100,    Train Loss: 0.0067,     Train Acc: 96.88%\n",
      "Iter: 5110,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 5120,    Train Loss: 0.044,     Train Acc: 96.88%\n",
      "Iter: 5130,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 5140,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 5150,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 5160,    Train Loss: 0.0078,     Train Acc: 96.88%\n",
      "Iter: 5170,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Iter: 5180,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 5190,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 5200,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 5210,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 5220,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 5230,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 5240,    Train Loss: 0.0052,     Train Acc: 100.00%\n",
      "Iter: 5250,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 5260,    Train Loss: 0.0055,     Train Acc: 100.00%\n",
      "Iter: 5270,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Iter: 5280,    Train Loss: 0.091,     Train Acc: 84.38%\n",
      "Iter: 5290,    Train Loss: 0.037,     Train Acc: 96.88%\n",
      "Iter: 5300,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 5310,    Train Loss: 0.02,     Train Acc: 96.88%\n",
      "Iter: 5320,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Epoch [23/50]\n",
      "Iter: 5330,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 5340,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 5350,    Train Loss: 0.0064,     Train Acc: 100.00%\n",
      "Iter: 5360,    Train Loss: 0.004,     Train Acc: 96.88%\n",
      "Iter: 5370,    Train Loss: 0.024,     Train Acc: 96.88%\n",
      "Iter: 5380,    Train Loss: 0.021,     Train Acc: 90.62%\n",
      "Iter: 5390,    Train Loss: 0.025,     Train Acc: 96.88%\n",
      "Iter: 5400,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 5410,    Train Loss: 0.009,     Train Acc: 93.75%\n",
      "Iter: 5420,    Train Loss: 0.0062,     Train Acc: 96.88%\n",
      "Iter: 5430,    Train Loss: 0.0043,     Train Acc: 96.88%\n",
      "Iter: 5440,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 5450,    Train Loss: 0.0064,     Train Acc: 96.88%\n",
      "Iter: 5460,    Train Loss: 0.04,     Train Acc: 96.88%\n",
      "Iter: 5470,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Iter: 5480,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 5490,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 5500,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 5510,    Train Loss: 0.044,     Train Acc: 90.62%\n",
      "Iter: 5520,    Train Loss: 0.02,     Train Acc: 87.50%\n",
      "Iter: 5530,    Train Loss: 0.11,     Train Acc: 75.00%\n",
      "Iter: 5540,    Train Loss: 0.0067,     Train Acc: 100.00%\n",
      "Iter: 5550,    Train Loss: 0.0055,     Train Acc: 100.00%\n",
      "Iter: 5560,    Train Loss: 0.0097,     Train Acc: 96.88%\n",
      "Iter: 5570,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Epoch [24/50]\n",
      "Iter: 5580,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 5590,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 5600,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 5610,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 5620,    Train Loss: 0.046,     Train Acc: 87.50%\n",
      "Iter: 5630,    Train Loss: 0.032,     Train Acc: 90.62%\n",
      "Iter: 5640,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 5650,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 5660,    Train Loss: 0.032,     Train Acc: 93.75%\n",
      "Iter: 5670,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 5680,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 5690,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 5700,    Train Loss: 0.0054,     Train Acc: 100.00%\n",
      "Iter: 5710,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 5720,    Train Loss: 0.0038,     Train Acc: 100.00%\n",
      "Iter: 5730,    Train Loss: 0.0055,     Train Acc: 100.00%\n",
      "Iter: 5740,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 5750,    Train Loss: 0.00098,     Train Acc: 100.00%\n",
      "Iter: 5760,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 5770,    Train Loss: 0.0048,     Train Acc: 100.00%\n",
      "Iter: 5780,    Train Loss: 0.09,     Train Acc: 84.38%\n",
      "Iter: 5790,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 5800,    Train Loss: 0.0055,     Train Acc: 100.00%\n",
      "Iter: 5810,    Train Loss: 0.0066,     Train Acc: 100.00%\n",
      "Iter: 5820,    Train Loss: 0.0047,     Train Acc: 96.88%\n",
      "Epoch [25/50]\n",
      "Iter: 5830,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 5840,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 5850,    Train Loss: 0.0036,     Train Acc: 100.00%\n",
      "Iter: 5860,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 5870,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 5880,    Train Loss: 0.02,     Train Acc: 90.62%\n",
      "Iter: 5890,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 5900,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 5910,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 5920,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 5930,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 5940,    Train Loss: 0.0055,     Train Acc: 96.88%\n",
      "Iter: 5950,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 5960,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 5970,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 5980,    Train Loss: 0.009,     Train Acc: 100.00%\n",
      "Iter: 5990,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 6000,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 6010,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 6020,    Train Loss: 0.009,     Train Acc: 96.88%\n",
      "Iter: 6030,    Train Loss: 0.11,     Train Acc: 71.88%\n",
      "Iter: 6040,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 6050,    Train Loss: 0.0048,     Train Acc: 96.88%\n",
      "Iter: 6060,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 6070,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Epoch [26/50]\n",
      "Iter: 6080,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 6090,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 6100,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 6110,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 6120,    Train Loss: 0.044,     Train Acc: 96.88%\n",
      "Iter: 6130,    Train Loss: 0.043,     Train Acc: 93.75%\n",
      "Iter: 6140,    Train Loss: 0.0034,     Train Acc: 100.00%\n",
      "Iter: 6150,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 6160,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 6170,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 6180,    Train Loss: 0.0038,     Train Acc: 100.00%\n",
      "Iter: 6190,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 6200,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 6210,    Train Loss: 0.0097,     Train Acc: 96.88%\n",
      "Iter: 6220,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 6230,    Train Loss: 0.0083,     Train Acc: 96.88%\n",
      "Iter: 6240,    Train Loss: 0.018,     Train Acc: 90.62%\n",
      "Iter: 6250,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 6260,    Train Loss: 0.016,     Train Acc: 93.75%\n",
      "Iter: 6270,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 6280,    Train Loss: 0.096,     Train Acc: 71.88%\n",
      "Iter: 6290,    Train Loss: 0.0053,     Train Acc: 96.88%\n",
      "Iter: 6300,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 6310,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 6320,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Epoch [27/50]\n",
      "Iter: 6330,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 6340,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 6350,    Train Loss: 0.0046,     Train Acc: 100.00%\n",
      "Iter: 6360,    Train Loss: 0.0009,     Train Acc: 100.00%\n",
      "Iter: 6370,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 6380,    Train Loss: 0.027,     Train Acc: 93.75%\n",
      "Iter: 6390,    Train Loss: 0.034,     Train Acc: 93.75%\n",
      "Iter: 6400,    Train Loss: 0.03,     Train Acc: 96.88%\n",
      "Iter: 6410,    Train Loss: 0.0044,     Train Acc: 100.00%\n",
      "Iter: 6420,    Train Loss: 0.0034,     Train Acc: 100.00%\n",
      "Iter: 6430,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 6440,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 6450,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 6460,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 6470,    Train Loss: 0.0047,     Train Acc: 100.00%\n",
      "Iter: 6480,    Train Loss: 0.005,     Train Acc: 100.00%\n",
      "Iter: 6490,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 6500,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 6510,    Train Loss: 0.008,     Train Acc: 96.88%\n",
      "Iter: 6520,    Train Loss: 0.031,     Train Acc: 93.75%\n",
      "Iter: 6530,    Train Loss: 0.064,     Train Acc: 81.25%\n",
      "Iter: 6540,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 6550,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 6560,    Train Loss: 0.02,     Train Acc: 93.75%\n",
      "Iter: 6570,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Epoch [28/50]\n",
      "Iter: 6580,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 6590,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 6600,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 6610,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 6620,    Train Loss: 0.051,     Train Acc: 93.75%\n",
      "Iter: 6630,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 6640,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 6650,    Train Loss: 0.00087,     Train Acc: 100.00%\n",
      "Iter: 6660,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 6670,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 6680,    Train Loss: 0.0047,     Train Acc: 96.88%\n",
      "Iter: 6690,    Train Loss: 0.0052,     Train Acc: 96.88%\n",
      "Iter: 6700,    Train Loss: 0.0081,     Train Acc: 96.88%\n",
      "Iter: 6710,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 6720,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 6730,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 6740,    Train Loss: 0.0067,     Train Acc: 96.88%\n",
      "Iter: 6750,    Train Loss: 0.0063,     Train Acc: 93.75%\n",
      "Iter: 6760,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 6770,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 6780,    Train Loss: 0.069,     Train Acc: 78.12%\n",
      "Iter: 6790,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 6800,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 6810,    Train Loss: 0.0064,     Train Acc: 100.00%\n",
      "Iter: 6820,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Epoch [29/50]\n",
      "Iter: 6830,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 6840,    Train Loss: 0.00079,     Train Acc: 100.00%\n",
      "Iter: 6850,    Train Loss: 0.0067,     Train Acc: 96.88%\n",
      "Iter: 6860,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 6870,    Train Loss: 0.036,     Train Acc: 96.88%\n",
      "Iter: 6880,    Train Loss: 0.1,     Train Acc: 78.12%\n",
      "Iter: 6890,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 6900,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 6910,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 6920,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 6930,    Train Loss: 0.022,     Train Acc: 93.75%\n",
      "Iter: 6940,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 6950,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 6960,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 6970,    Train Loss: 0.027,     Train Acc: 87.50%\n",
      "Iter: 6980,    Train Loss: 0.0085,     Train Acc: 96.88%\n",
      "Iter: 6990,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 7000,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 7010,    Train Loss: 0.0095,     Train Acc: 93.75%\n",
      "Iter: 7020,    Train Loss: 0.0054,     Train Acc: 96.88%\n",
      "Iter: 7030,    Train Loss: 0.074,     Train Acc: 87.50%\n",
      "Iter: 7040,    Train Loss: 0.006,     Train Acc: 96.88%\n",
      "Iter: 7050,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 7060,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 7070,    Train Loss: 0.0047,     Train Acc: 96.88%\n",
      "Epoch [30/50]\n",
      "Iter: 7080,    Train Loss: 0.00059,     Train Acc: 100.00%\n",
      "Iter: 7090,    Train Loss: 0.0034,     Train Acc: 100.00%\n",
      "Iter: 7100,    Train Loss: 0.056,     Train Acc: 90.62%\n",
      "Iter: 7110,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 7120,    Train Loss: 0.044,     Train Acc: 93.75%\n",
      "Iter: 7130,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 7140,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 7150,    Train Loss: 0.0049,     Train Acc: 96.88%\n",
      "Iter: 7160,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 7170,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 7180,    Train Loss: 0.0051,     Train Acc: 96.88%\n",
      "Iter: 7190,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 7200,    Train Loss: 0.0089,     Train Acc: 96.88%\n",
      "Iter: 7210,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 7220,    Train Loss: 0.0072,     Train Acc: 96.88%\n",
      "Iter: 7230,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 7240,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 7250,    Train Loss: 0.037,     Train Acc: 93.75%\n",
      "Iter: 7260,    Train Loss: 0.0081,     Train Acc: 96.88%\n",
      "Iter: 7270,    Train Loss: 0.016,     Train Acc: 93.75%\n",
      "Iter: 7280,    Train Loss: 0.066,     Train Acc: 84.38%\n",
      "Iter: 7290,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 7300,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 7310,    Train Loss: 0.008,     Train Acc: 96.88%\n",
      "Iter: 7320,    Train Loss: 0.0041,     Train Acc: 96.88%\n",
      "Epoch [31/50]\n",
      "Iter: 7330,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 7340,    Train Loss: 0.008,     Train Acc: 96.88%\n",
      "Iter: 7350,    Train Loss: 0.0043,     Train Acc: 100.00%\n",
      "Iter: 7360,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 7370,    Train Loss: 0.047,     Train Acc: 96.88%\n",
      "Iter: 7380,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 7390,    Train Loss: 0.0045,     Train Acc: 96.88%\n",
      "Iter: 7400,    Train Loss: 0.0055,     Train Acc: 96.88%\n",
      "Iter: 7410,    Train Loss: 0.0035,     Train Acc: 100.00%\n",
      "Iter: 7420,    Train Loss: 0.0055,     Train Acc: 96.88%\n",
      "Iter: 7430,    Train Loss: 0.0059,     Train Acc: 96.88%\n",
      "Iter: 7440,    Train Loss: 0.0041,     Train Acc: 96.88%\n",
      "Iter: 7450,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 7460,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 7470,    Train Loss: 0.0064,     Train Acc: 96.88%\n",
      "Iter: 7480,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 7490,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 7500,    Train Loss: 0.0035,     Train Acc: 100.00%\n",
      "Iter: 7510,    Train Loss: 0.0034,     Train Acc: 100.00%\n",
      "Iter: 7520,    Train Loss: 0.0048,     Train Acc: 96.88%\n",
      "Iter: 7530,    Train Loss: 0.095,     Train Acc: 81.25%\n",
      "Iter: 7540,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 7550,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 7560,    Train Loss: 0.025,     Train Acc: 96.88%\n",
      "Iter: 7570,    Train Loss: 0.008,     Train Acc: 93.75%\n",
      "Epoch [32/50]\n",
      "Iter: 7580,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 7590,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 7600,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 7610,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 7620,    Train Loss: 0.046,     Train Acc: 90.62%\n",
      "Iter: 7630,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 7640,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 7650,    Train Loss: 0.026,     Train Acc: 96.88%\n",
      "Iter: 7660,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 7670,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 7680,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 7690,    Train Loss: 0.0067,     Train Acc: 96.88%\n",
      "Iter: 7700,    Train Loss: 0.0059,     Train Acc: 96.88%\n",
      "Iter: 7710,    Train Loss: 0.0009,     Train Acc: 100.00%\n",
      "Iter: 7720,    Train Loss: 0.03,     Train Acc: 90.62%\n",
      "Iter: 7730,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 7740,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 7750,    Train Loss: 0.00067,     Train Acc: 100.00%\n",
      "Iter: 7760,    Train Loss: 0.0046,     Train Acc: 100.00%\n",
      "Iter: 7770,    Train Loss: 0.0097,     Train Acc: 93.75%\n",
      "Iter: 7780,    Train Loss: 0.074,     Train Acc: 81.25%\n",
      "Iter: 7790,    Train Loss: 0.0035,     Train Acc: 96.88%\n",
      "Iter: 7800,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 7810,    Train Loss: 0.0038,     Train Acc: 100.00%\n",
      "Iter: 7820,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Epoch [33/50]\n",
      "Iter: 7830,    Train Loss: 0.00066,     Train Acc: 100.00%\n",
      "Iter: 7840,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 7850,    Train Loss: 0.0031,     Train Acc: 100.00%\n",
      "Iter: 7860,    Train Loss: 0.00067,     Train Acc: 100.00%\n",
      "Iter: 7870,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 7880,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 7890,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 7900,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 7910,    Train Loss: 0.018,     Train Acc: 90.62%\n",
      "Iter: 7920,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 7930,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 7940,    Train Loss: 0.0041,     Train Acc: 96.88%\n",
      "Iter: 7950,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 7960,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 7970,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 7980,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 7990,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 8000,    Train Loss: 0.0044,     Train Acc: 100.00%\n",
      "Iter: 8010,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 8020,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 8030,    Train Loss: 0.071,     Train Acc: 81.25%\n",
      "Iter: 8040,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 8050,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8060,    Train Loss: 0.0046,     Train Acc: 100.00%\n",
      "Iter: 8070,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Epoch [34/50]\n",
      "Iter: 8080,    Train Loss: 0.00063,     Train Acc: 100.00%\n",
      "Iter: 8090,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 8100,    Train Loss: 0.0032,     Train Acc: 100.00%\n",
      "Iter: 8110,    Train Loss: 0.00052,     Train Acc: 100.00%\n",
      "Iter: 8120,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 8130,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Iter: 8140,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 8150,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8160,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 8170,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 8180,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 8190,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 8200,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 8210,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 8220,    Train Loss: 0.0036,     Train Acc: 100.00%\n",
      "Iter: 8230,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 8240,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 8250,    Train Loss: 0.0054,     Train Acc: 96.88%\n",
      "Iter: 8260,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 8270,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 8280,    Train Loss: 0.068,     Train Acc: 84.38%\n",
      "Iter: 8290,    Train Loss: 0.0052,     Train Acc: 100.00%\n",
      "Iter: 8300,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 8310,    Train Loss: 0.0043,     Train Acc: 100.00%\n",
      "Iter: 8320,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Epoch [35/50]\n",
      "Iter: 8330,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 8340,    Train Loss: 0.021,     Train Acc: 96.88%\n",
      "Iter: 8350,    Train Loss: 0.0083,     Train Acc: 100.00%\n",
      "Iter: 8360,    Train Loss: 0.034,     Train Acc: 93.75%\n",
      "Iter: 8370,    Train Loss: 0.037,     Train Acc: 93.75%\n",
      "Iter: 8380,    Train Loss: 0.053,     Train Acc: 90.62%\n",
      "Iter: 8390,    Train Loss: 0.0036,     Train Acc: 100.00%\n",
      "Iter: 8400,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 8410,    Train Loss: 0.0039,     Train Acc: 100.00%\n",
      "Iter: 8420,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 8430,    Train Loss: 0.0062,     Train Acc: 96.88%\n",
      "Iter: 8440,    Train Loss: 0.0009,     Train Acc: 100.00%\n",
      "Iter: 8450,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 8460,    Train Loss: 0.00099,     Train Acc: 100.00%\n",
      "Iter: 8470,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8480,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 8490,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 8500,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 8510,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8520,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8530,    Train Loss: 0.092,     Train Acc: 81.25%\n",
      "Iter: 8540,    Train Loss: 0.0098,     Train Acc: 96.88%\n",
      "Iter: 8550,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 8560,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 8570,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Epoch [36/50]\n",
      "Iter: 8580,    Train Loss: 0.00083,     Train Acc: 100.00%\n",
      "Iter: 8590,    Train Loss: 0.00052,     Train Acc: 100.00%\n",
      "Iter: 8600,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 8610,    Train Loss: 0.00068,     Train Acc: 100.00%\n",
      "Iter: 8620,    Train Loss: 0.03,     Train Acc: 93.75%\n",
      "Iter: 8630,    Train Loss: 0.031,     Train Acc: 90.62%\n",
      "Iter: 8640,    Train Loss: 0.0039,     Train Acc: 96.88%\n",
      "Iter: 8650,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 8660,    Train Loss: 0.004,     Train Acc: 96.88%\n",
      "Iter: 8670,    Train Loss: 0.0079,     Train Acc: 96.88%\n",
      "Iter: 8680,    Train Loss: 0.00072,     Train Acc: 100.00%\n",
      "Iter: 8690,    Train Loss: 0.00086,     Train Acc: 100.00%\n",
      "Iter: 8700,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 8710,    Train Loss: 0.0054,     Train Acc: 96.88%\n",
      "Iter: 8720,    Train Loss: 0.0081,     Train Acc: 96.88%\n",
      "Iter: 8730,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 8740,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 8750,    Train Loss: 0.00097,     Train Acc: 100.00%\n",
      "Iter: 8760,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 8770,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 8780,    Train Loss: 0.068,     Train Acc: 84.38%\n",
      "Iter: 8790,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 8800,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 8810,    Train Loss: 0.009,     Train Acc: 93.75%\n",
      "Iter: 8820,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Epoch [37/50]\n",
      "Iter: 8830,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 8840,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 8850,    Train Loss: 0.037,     Train Acc: 96.88%\n",
      "Iter: 8860,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 8870,    Train Loss: 0.032,     Train Acc: 90.62%\n",
      "Iter: 8880,    Train Loss: 0.02,     Train Acc: 87.50%\n",
      "Iter: 8890,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 8900,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 8910,    Train Loss: 0.0058,     Train Acc: 96.88%\n",
      "Iter: 8920,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 8930,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 8940,    Train Loss: 0.0087,     Train Acc: 96.88%\n",
      "Iter: 8950,    Train Loss: 0.0095,     Train Acc: 96.88%\n",
      "Iter: 8960,    Train Loss: 0.00079,     Train Acc: 100.00%\n",
      "Iter: 8970,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 8980,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 8990,    Train Loss: 0.0057,     Train Acc: 96.88%\n",
      "Iter: 9000,    Train Loss: 0.00066,     Train Acc: 100.00%\n",
      "Iter: 9010,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 9020,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 9030,    Train Loss: 0.066,     Train Acc: 78.12%\n",
      "Iter: 9040,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 9050,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 9060,    Train Loss: 0.0083,     Train Acc: 96.88%\n",
      "Iter: 9070,    Train Loss: 0.0048,     Train Acc: 96.88%\n",
      "Epoch [38/50]\n",
      "Iter: 9080,    Train Loss: 0.0079,     Train Acc: 96.88%\n",
      "Iter: 9090,    Train Loss: 0.0007,     Train Acc: 100.00%\n",
      "Iter: 9100,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 9110,    Train Loss: 0.00051,     Train Acc: 100.00%\n",
      "Iter: 9120,    Train Loss: 0.02,     Train Acc: 96.88%\n",
      "Iter: 9130,    Train Loss: 0.011,     Train Acc: 100.00%\n",
      "Iter: 9140,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 9150,    Train Loss: 0.0053,     Train Acc: 96.88%\n",
      "Iter: 9160,    Train Loss: 0.0041,     Train Acc: 100.00%\n",
      "Iter: 9170,    Train Loss: 0.023,     Train Acc: 93.75%\n",
      "Iter: 9180,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 9190,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 9200,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 9210,    Train Loss: 0.00072,     Train Acc: 100.00%\n",
      "Iter: 9220,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 9230,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 9240,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 9250,    Train Loss: 0.00048,     Train Acc: 100.00%\n",
      "Iter: 9260,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 9270,    Train Loss: 0.0085,     Train Acc: 96.88%\n",
      "Iter: 9280,    Train Loss: 0.058,     Train Acc: 84.38%\n",
      "Iter: 9290,    Train Loss: 0.00045,     Train Acc: 100.00%\n",
      "Iter: 9300,    Train Loss: 0.00056,     Train Acc: 100.00%\n",
      "Iter: 9310,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 9320,    Train Loss: 0.00073,     Train Acc: 100.00%\n",
      "Epoch [39/50]\n",
      "Iter: 9330,    Train Loss: 0.00076,     Train Acc: 100.00%\n",
      "Iter: 9340,    Train Loss: 0.00055,     Train Acc: 100.00%\n",
      "Iter: 9350,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 9360,    Train Loss: 0.0049,     Train Acc: 96.88%\n",
      "Iter: 9370,    Train Loss: 0.056,     Train Acc: 93.75%\n",
      "Iter: 9380,    Train Loss: 0.039,     Train Acc: 90.62%\n",
      "Iter: 9390,    Train Loss: 0.00094,     Train Acc: 100.00%\n",
      "Iter: 9400,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 9410,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 9420,    Train Loss: 0.00089,     Train Acc: 100.00%\n",
      "Iter: 9430,    Train Loss: 0.0029,     Train Acc: 96.88%\n",
      "Iter: 9440,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 9450,    Train Loss: 0.0076,     Train Acc: 96.88%\n",
      "Iter: 9460,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 9470,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 9480,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 9490,    Train Loss: 0.0073,     Train Acc: 96.88%\n",
      "Iter: 9500,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 9510,    Train Loss: 0.021,     Train Acc: 93.75%\n",
      "Iter: 9520,    Train Loss: 0.036,     Train Acc: 90.62%\n",
      "Iter: 9530,    Train Loss: 0.088,     Train Acc: 78.12%\n",
      "Iter: 9540,    Train Loss: 0.006,     Train Acc: 96.88%\n",
      "Iter: 9550,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 9560,    Train Loss: 0.0071,     Train Acc: 96.88%\n",
      "Iter: 9570,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Epoch [40/50]\n",
      "Iter: 9580,    Train Loss: 0.00089,     Train Acc: 100.00%\n",
      "Iter: 9590,    Train Loss: 0.00097,     Train Acc: 100.00%\n",
      "Iter: 9600,    Train Loss: 0.0076,     Train Acc: 96.88%\n",
      "Iter: 9610,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 9620,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 9630,    Train Loss: 0.059,     Train Acc: 87.50%\n",
      "Iter: 9640,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 9650,    Train Loss: 0.00077,     Train Acc: 100.00%\n",
      "Iter: 9660,    Train Loss: 0.0037,     Train Acc: 100.00%\n",
      "Iter: 9670,    Train Loss: 0.00085,     Train Acc: 100.00%\n",
      "Iter: 9680,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 9690,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 9700,    Train Loss: 0.012,     Train Acc: 93.75%\n",
      "Iter: 9710,    Train Loss: 0.00083,     Train Acc: 100.00%\n",
      "Iter: 9720,    Train Loss: 0.019,     Train Acc: 93.75%\n",
      "Iter: 9730,    Train Loss: 0.0071,     Train Acc: 96.88%\n",
      "Iter: 9740,    Train Loss: 0.0052,     Train Acc: 96.88%\n",
      "Iter: 9750,    Train Loss: 0.0051,     Train Acc: 96.88%\n",
      "Iter: 9760,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 9770,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 9780,    Train Loss: 0.061,     Train Acc: 84.38%\n",
      "Iter: 9790,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 9800,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 9810,    Train Loss: 0.0032,     Train Acc: 100.00%\n",
      "Iter: 9820,    Train Loss: 0.00086,     Train Acc: 100.00%\n",
      "Epoch [41/50]\n",
      "Iter: 9830,    Train Loss: 0.0007,     Train Acc: 100.00%\n",
      "Iter: 9840,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 9850,    Train Loss: 0.0052,     Train Acc: 96.88%\n",
      "Iter: 9860,    Train Loss: 0.00047,     Train Acc: 100.00%\n",
      "Iter: 9870,    Train Loss: 0.013,     Train Acc: 93.75%\n",
      "Iter: 9880,    Train Loss: 0.0067,     Train Acc: 100.00%\n",
      "Iter: 9890,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 9900,    Train Loss: 0.0036,     Train Acc: 96.88%\n",
      "Iter: 9910,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 9920,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 9930,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 9940,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 9950,    Train Loss: 0.0039,     Train Acc: 96.88%\n",
      "Iter: 9960,    Train Loss: 0.00065,     Train Acc: 100.00%\n",
      "Iter: 9970,    Train Loss: 0.0088,     Train Acc: 96.88%\n",
      "Iter: 9980,    Train Loss: 0.034,     Train Acc: 93.75%\n",
      "Iter: 9990,    Train Loss: 0.0028,     Train Acc: 100.00%\n",
      "Iter: 10000,    Train Loss: 0.00036,     Train Acc: 100.00%\n",
      "Iter: 10010,    Train Loss: 0.018,     Train Acc: 96.88%\n",
      "Iter: 10020,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 10030,    Train Loss: 0.08,     Train Acc: 84.38%\n",
      "Iter: 10040,    Train Loss: 0.00042,     Train Acc: 100.00%\n",
      "Iter: 10050,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 10060,    Train Loss: 0.0098,     Train Acc: 96.88%\n",
      "Iter: 10070,    Train Loss: 0.004,     Train Acc: 96.88%\n",
      "Epoch [42/50]\n",
      "Iter: 10080,    Train Loss: 0.00037,     Train Acc: 100.00%\n",
      "Iter: 10090,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 10100,    Train Loss: 0.013,     Train Acc: 93.75%\n",
      "Iter: 10110,    Train Loss: 0.0006,     Train Acc: 100.00%\n",
      "Iter: 10120,    Train Loss: 0.047,     Train Acc: 96.88%\n",
      "Iter: 10130,    Train Loss: 0.022,     Train Acc: 90.62%\n",
      "Iter: 10140,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 10150,    Train Loss: 0.00087,     Train Acc: 100.00%\n",
      "Iter: 10160,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 10170,    Train Loss: 0.0089,     Train Acc: 96.88%\n",
      "Iter: 10180,    Train Loss: 0.00056,     Train Acc: 100.00%\n",
      "Iter: 10190,    Train Loss: 0.00078,     Train Acc: 100.00%\n",
      "Iter: 10200,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 10210,    Train Loss: 0.00069,     Train Acc: 100.00%\n",
      "Iter: 10220,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 10230,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 10240,    Train Loss: 0.00082,     Train Acc: 100.00%\n",
      "Iter: 10250,    Train Loss: 0.00049,     Train Acc: 100.00%\n",
      "Iter: 10260,    Train Loss: 0.0029,     Train Acc: 100.00%\n",
      "Iter: 10270,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 10280,    Train Loss: 0.075,     Train Acc: 78.12%\n",
      "Iter: 10290,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 10300,    Train Loss: 0.00076,     Train Acc: 100.00%\n",
      "Iter: 10310,    Train Loss: 0.005,     Train Acc: 100.00%\n",
      "Iter: 10320,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Epoch [43/50]\n",
      "Iter: 10330,    Train Loss: 0.00038,     Train Acc: 100.00%\n",
      "Iter: 10340,    Train Loss: 0.00076,     Train Acc: 100.00%\n",
      "Iter: 10350,    Train Loss: 0.028,     Train Acc: 93.75%\n",
      "Iter: 10360,    Train Loss: 0.00048,     Train Acc: 100.00%\n",
      "Iter: 10370,    Train Loss: 0.027,     Train Acc: 96.88%\n",
      "Iter: 10380,    Train Loss: 0.012,     Train Acc: 100.00%\n",
      "Iter: 10390,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 10400,    Train Loss: 0.00069,     Train Acc: 100.00%\n",
      "Iter: 10410,    Train Loss: 0.018,     Train Acc: 96.88%\n",
      "Iter: 10420,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 10430,    Train Loss: 0.00057,     Train Acc: 100.00%\n",
      "Iter: 10440,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 10450,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 10460,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 10470,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 10480,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 10490,    Train Loss: 0.016,     Train Acc: 93.75%\n",
      "Iter: 10500,    Train Loss: 0.00069,     Train Acc: 100.00%\n",
      "Iter: 10510,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 10520,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 10530,    Train Loss: 0.084,     Train Acc: 78.12%\n",
      "Iter: 10540,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 10550,    Train Loss: 0.00095,     Train Acc: 100.00%\n",
      "Iter: 10560,    Train Loss: 0.0062,     Train Acc: 100.00%\n",
      "Iter: 10570,    Train Loss: 0.00092,     Train Acc: 100.00%\n",
      "Epoch [44/50]\n",
      "Iter: 10580,    Train Loss: 0.00058,     Train Acc: 100.00%\n",
      "Iter: 10590,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 10600,    Train Loss: 0.0092,     Train Acc: 96.88%\n",
      "Iter: 10610,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 10620,    Train Loss: 0.082,     Train Acc: 87.50%\n",
      "Iter: 10630,    Train Loss: 0.0036,     Train Acc: 100.00%\n",
      "Iter: 10640,    Train Loss: 0.0038,     Train Acc: 100.00%\n",
      "Iter: 10650,    Train Loss: 0.0048,     Train Acc: 100.00%\n",
      "Iter: 10660,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 10670,    Train Loss: 0.0085,     Train Acc: 96.88%\n",
      "Iter: 10680,    Train Loss: 0.00084,     Train Acc: 100.00%\n",
      "Iter: 10690,    Train Loss: 0.0027,     Train Acc: 100.00%\n",
      "Iter: 10700,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 10710,    Train Loss: 0.00045,     Train Acc: 100.00%\n",
      "Iter: 10720,    Train Loss: 0.066,     Train Acc: 90.62%\n",
      "Iter: 10730,    Train Loss: 0.0079,     Train Acc: 96.88%\n",
      "Iter: 10740,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 10750,    Train Loss: 0.00071,     Train Acc: 100.00%\n",
      "Iter: 10760,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 10770,    Train Loss: 0.003,     Train Acc: 100.00%\n",
      "Iter: 10780,    Train Loss: 0.05,     Train Acc: 84.38%\n",
      "Iter: 10790,    Train Loss: 0.00058,     Train Acc: 100.00%\n",
      "Iter: 10800,    Train Loss: 0.00082,     Train Acc: 100.00%\n",
      "Iter: 10810,    Train Loss: 0.0033,     Train Acc: 100.00%\n",
      "Iter: 10820,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Epoch [45/50]\n",
      "Iter: 10830,    Train Loss: 0.00037,     Train Acc: 100.00%\n",
      "Iter: 10840,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 10850,    Train Loss: 0.0068,     Train Acc: 96.88%\n",
      "Iter: 10860,    Train Loss: 0.00056,     Train Acc: 100.00%\n",
      "Iter: 10870,    Train Loss: 0.032,     Train Acc: 93.75%\n",
      "Iter: 10880,    Train Loss: 0.0042,     Train Acc: 100.00%\n",
      "Iter: 10890,    Train Loss: 0.057,     Train Acc: 96.88%\n",
      "Iter: 10900,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 10910,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 10920,    Train Loss: 0.028,     Train Acc: 96.88%\n",
      "Iter: 10930,    Train Loss: 0.0024,     Train Acc: 100.00%\n",
      "Iter: 10940,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 10950,    Train Loss: 0.011,     Train Acc: 96.88%\n",
      "Iter: 10960,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 10970,    Train Loss: 0.017,     Train Acc: 93.75%\n",
      "Iter: 10980,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 10990,    Train Loss: 0.00069,     Train Acc: 100.00%\n",
      "Iter: 11000,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 11010,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 11020,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 11030,    Train Loss: 0.065,     Train Acc: 81.25%\n",
      "Iter: 11040,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 11050,    Train Loss: 0.0062,     Train Acc: 96.88%\n",
      "Iter: 11060,    Train Loss: 0.019,     Train Acc: 96.88%\n",
      "Iter: 11070,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Epoch [46/50]\n",
      "Iter: 11080,    Train Loss: 0.00064,     Train Acc: 100.00%\n",
      "Iter: 11090,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 11100,    Train Loss: 0.006,     Train Acc: 100.00%\n",
      "Iter: 11110,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11120,    Train Loss: 0.048,     Train Acc: 87.50%\n",
      "Iter: 11130,    Train Loss: 0.0068,     Train Acc: 96.88%\n",
      "Iter: 11140,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 11150,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 11160,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 11170,    Train Loss: 0.02,     Train Acc: 96.88%\n",
      "Iter: 11180,    Train Loss: 0.026,     Train Acc: 90.62%\n",
      "Iter: 11190,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 11200,    Train Loss: 0.0047,     Train Acc: 96.88%\n",
      "Iter: 11210,    Train Loss: 0.00066,     Train Acc: 100.00%\n",
      "Iter: 11220,    Train Loss: 0.0082,     Train Acc: 96.88%\n",
      "Iter: 11230,    Train Loss: 0.018,     Train Acc: 93.75%\n",
      "Iter: 11240,    Train Loss: 0.00089,     Train Acc: 100.00%\n",
      "Iter: 11250,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11260,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 11270,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 11280,    Train Loss: 0.081,     Train Acc: 87.50%\n",
      "Iter: 11290,    Train Loss: 0.00085,     Train Acc: 100.00%\n",
      "Iter: 11300,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Iter: 11310,    Train Loss: 0.017,     Train Acc: 93.75%\n",
      "Iter: 11320,    Train Loss: 0.00089,     Train Acc: 100.00%\n",
      "Epoch [47/50]\n",
      "Iter: 11330,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11340,    Train Loss: 0.00067,     Train Acc: 100.00%\n",
      "Iter: 11350,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 11360,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11370,    Train Loss: 0.016,     Train Acc: 96.88%\n",
      "Iter: 11380,    Train Loss: 0.0064,     Train Acc: 100.00%\n",
      "Iter: 11390,    Train Loss: 0.0051,     Train Acc: 100.00%\n",
      "Iter: 11400,    Train Loss: 0.0013,     Train Acc: 100.00%\n",
      "Iter: 11410,    Train Loss: 0.0021,     Train Acc: 100.00%\n",
      "Iter: 11420,    Train Loss: 0.0026,     Train Acc: 100.00%\n",
      "Iter: 11430,    Train Loss: 0.00042,     Train Acc: 100.00%\n",
      "Iter: 11440,    Train Loss: 0.00051,     Train Acc: 100.00%\n",
      "Iter: 11450,    Train Loss: 0.0057,     Train Acc: 96.88%\n",
      "Iter: 11460,    Train Loss: 0.0008,     Train Acc: 100.00%\n",
      "Iter: 11470,    Train Loss: 0.0065,     Train Acc: 96.88%\n",
      "Iter: 11480,    Train Loss: 0.0098,     Train Acc: 96.88%\n",
      "Iter: 11490,    Train Loss: 0.028,     Train Acc: 90.62%\n",
      "Iter: 11500,    Train Loss: 0.0022,     Train Acc: 100.00%\n",
      "Iter: 11510,    Train Loss: 0.00092,     Train Acc: 100.00%\n",
      "Iter: 11520,    Train Loss: 0.00085,     Train Acc: 100.00%\n",
      "Iter: 11530,    Train Loss: 0.079,     Train Acc: 84.38%\n",
      "Iter: 11540,    Train Loss: 0.00056,     Train Acc: 100.00%\n",
      "Iter: 11550,    Train Loss: 0.00097,     Train Acc: 100.00%\n",
      "Iter: 11560,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 11570,    Train Loss: 0.0019,     Train Acc: 100.00%\n",
      "Epoch [48/50]\n",
      "Iter: 11580,    Train Loss: 0.0017,     Train Acc: 100.00%\n",
      "Iter: 11590,    Train Loss: 0.00054,     Train Acc: 100.00%\n",
      "Iter: 11600,    Train Loss: 0.0043,     Train Acc: 100.00%\n",
      "Iter: 11610,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 11620,    Train Loss: 0.015,     Train Acc: 93.75%\n",
      "Iter: 11630,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 11640,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11650,    Train Loss: 0.0025,     Train Acc: 100.00%\n",
      "Iter: 11660,    Train Loss: 0.022,     Train Acc: 87.50%\n",
      "Iter: 11670,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 11680,    Train Loss: 0.00075,     Train Acc: 100.00%\n",
      "Iter: 11690,    Train Loss: 0.017,     Train Acc: 96.88%\n",
      "Iter: 11700,    Train Loss: 0.058,     Train Acc: 93.75%\n",
      "Iter: 11710,    Train Loss: 0.00053,     Train Acc: 100.00%\n",
      "Iter: 11720,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 11730,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11740,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 11750,    Train Loss: 0.012,     Train Acc: 96.88%\n",
      "Iter: 11760,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 11770,    Train Loss: 0.0062,     Train Acc: 96.88%\n",
      "Iter: 11780,    Train Loss: 0.064,     Train Acc: 78.12%\n",
      "Iter: 11790,    Train Loss: 0.00072,     Train Acc: 100.00%\n",
      "Iter: 11800,    Train Loss: 0.0007,     Train Acc: 100.00%\n",
      "Iter: 11810,    Train Loss: 0.00082,     Train Acc: 100.00%\n",
      "Iter: 11820,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Epoch [49/50]\n",
      "Iter: 11830,    Train Loss: 0.00049,     Train Acc: 100.00%\n",
      "Iter: 11840,    Train Loss: 0.00093,     Train Acc: 100.00%\n",
      "Iter: 11850,    Train Loss: 0.0023,     Train Acc: 100.00%\n",
      "Iter: 11860,    Train Loss: 0.00051,     Train Acc: 100.00%\n",
      "Iter: 11870,    Train Loss: 0.027,     Train Acc: 96.88%\n",
      "Iter: 11880,    Train Loss: 0.01,     Train Acc: 96.88%\n",
      "Iter: 11890,    Train Loss: 0.0057,     Train Acc: 96.88%\n",
      "Iter: 11900,    Train Loss: 0.0054,     Train Acc: 96.88%\n",
      "Iter: 11910,    Train Loss: 0.0056,     Train Acc: 96.88%\n",
      "Iter: 11920,    Train Loss: 0.0068,     Train Acc: 96.88%\n",
      "Iter: 11930,    Train Loss: 0.011,     Train Acc: 93.75%\n",
      "Iter: 11940,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 11950,    Train Loss: 0.002,     Train Acc: 100.00%\n",
      "Iter: 11960,    Train Loss: 0.00057,     Train Acc: 100.00%\n",
      "Iter: 11970,    Train Loss: 0.0015,     Train Acc: 100.00%\n",
      "Iter: 11980,    Train Loss: 0.0014,     Train Acc: 100.00%\n",
      "Iter: 11990,    Train Loss: 0.0046,     Train Acc: 96.88%\n",
      "Iter: 12000,    Train Loss: 0.0067,     Train Acc: 96.88%\n",
      "Iter: 12010,    Train Loss: 0.014,     Train Acc: 93.75%\n",
      "Iter: 12020,    Train Loss: 0.00084,     Train Acc: 100.00%\n",
      "Iter: 12030,    Train Loss: 0.11,     Train Acc: 78.12%\n",
      "Iter: 12040,    Train Loss: 0.032,     Train Acc: 93.75%\n",
      "Iter: 12050,    Train Loss: 0.0063,     Train Acc: 96.88%\n",
      "Iter: 12060,    Train Loss: 0.013,     Train Acc: 96.88%\n",
      "Iter: 12070,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Epoch [50/50]\n",
      "Iter: 12080,    Train Loss: 0.00024,     Train Acc: 100.00%\n",
      "Iter: 12090,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 12100,    Train Loss: 0.004,     Train Acc: 100.00%\n",
      "Iter: 12110,    Train Loss: 0.017,     Train Acc: 93.75%\n",
      "Iter: 12120,    Train Loss: 0.015,     Train Acc: 96.88%\n",
      "Iter: 12130,    Train Loss: 0.029,     Train Acc: 87.50%\n",
      "Iter: 12140,    Train Loss: 0.00085,     Train Acc: 100.00%\n",
      "Iter: 12150,    Train Loss: 0.00043,     Train Acc: 100.00%\n",
      "Iter: 12160,    Train Loss: 0.014,     Train Acc: 96.88%\n",
      "Iter: 12170,    Train Loss: 0.00048,     Train Acc: 100.00%\n",
      "Iter: 12180,    Train Loss: 0.0037,     Train Acc: 96.88%\n",
      "Iter: 12190,    Train Loss: 0.0018,     Train Acc: 100.00%\n",
      "Iter: 12200,    Train Loss: 0.0012,     Train Acc: 100.00%\n",
      "Iter: 12210,    Train Loss: 0.00063,     Train Acc: 100.00%\n",
      "Iter: 12220,    Train Loss: 0.001,     Train Acc: 100.00%\n",
      "Iter: 12230,    Train Loss: 0.0011,     Train Acc: 100.00%\n",
      "Iter: 12240,    Train Loss: 0.0097,     Train Acc: 96.88%\n",
      "Iter: 12250,    Train Loss: 0.00045,     Train Acc: 100.00%\n",
      "Iter: 12260,    Train Loss: 0.0053,     Train Acc: 96.88%\n",
      "Iter: 12270,    Train Loss: 0.0075,     Train Acc: 96.88%\n",
      "Iter: 12280,    Train Loss: 0.071,     Train Acc: 81.25%\n",
      "Iter: 12290,    Train Loss: 0.065,     Train Acc: 84.38%\n",
      "Iter: 12300,    Train Loss: 0.00071,     Train Acc: 100.00%\n",
      "Iter: 12310,    Train Loss: 0.0016,     Train Acc: 100.00%\n",
      "Iter: 12320,    Train Loss: 0.00082,     Train Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "model = BModel(config).to(config.device)\n",
    "train(config, model, train_iter, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.17,  Test Acc: 80.66%\n",
      "Precision, Recall and F1-Score...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          动力     0.8698    0.9556    0.9107       811\n",
      "          价格     0.9645    0.8892    0.9253       397\n",
      "          内饰     0.8208    0.9221    0.8685       154\n",
      "          配置     0.8577    0.8707    0.8642       263\n",
      "         安全性     0.7216    0.9246    0.8106       199\n",
      "          外观     0.8175    0.8819    0.8485       127\n",
      "          操控     0.8431    0.8165    0.8296       316\n",
      "          油耗     0.9320    0.9375    0.9347       336\n",
      "          空间     0.9043    0.8333    0.8673       102\n",
      "         舒适性     0.7105    0.8120    0.7579       266\n",
      "\n",
      "   micro avg     0.8524    0.8984    0.8748      2971\n",
      "   macro avg     0.8442    0.8843    0.8617      2971\n",
      "weighted avg     0.8578    0.8984    0.8759      2971\n",
      " samples avg     0.8817    0.9086    0.8856      2971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test(config, model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ADBSdataset(config):\n",
    "    def load_dataset(path, pad_size=64):\n",
    "        contents = []\n",
    "        with open(path, 'r', encoding='UTF-8') as f:\n",
    "            for line in f:\n",
    "                lin = line.strip()\n",
    "                if not lin:\n",
    "                    continue\n",
    "                content = lin.split('\\t')[0]\n",
    "                aspect_list = [w.split('#')[0] for w in lin.split('\\t')[1:]]\n",
    "                label = [int(w.split('#')[1]) + 1 for w in lin.split('\\t')[1:]]\n",
    "                for i in range(len(aspect_list)): \n",
    "                    token = config.tokenizer.tokenize(aspect_list[i] + content)\n",
    "                    token = [CLS] + token\n",
    "                    seq_len = len(token)\n",
    "                    mask = []\n",
    "                    token_ids = config.tokenizer.convert_tokens_to_ids(token)\n",
    "\n",
    "                    if pad_size:\n",
    "                        if len(token) < pad_size:\n",
    "                            mask = [1] * len(token_ids) + [0] * (pad_size - len(token))\n",
    "                            token_ids += ([0] * (pad_size - len(token)))\n",
    "                        else:\n",
    "                            mask = [1] * pad_size\n",
    "                            token_ids = token_ids[:pad_size]\n",
    "                            seq_len = pad_size\n",
    "                    contents.append([token_ids, np.eye(3)[label[i]], seq_len, mask])\n",
    "        return contents\n",
    "    train = load_dataset(config.train_path, config.pad_size)\n",
    "    test = load_dataset(config.test_path, config.pad_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ADBSdata, test_ADBSdata = build_ADBSdataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        super(ABSADataset, self).__init__()\n",
    "        self.dataset = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.LongTensor(self.dataset[idx][0]).to(self.device)\n",
    "        label = torch.LongTensor([self.dataset[idx][1]]).to(self.device)\n",
    "        seq_len = torch.LongTensor([self.dataset[idx][2]]).to(self.device)\n",
    "        mask = torch.LongTensor(self.dataset[idx][3]).to(self.device)\n",
    "\n",
    "        return text, seq_len, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ADBSdataset = ABSADataset(train_ADBSdata, config.device)\n",
    "train_loader = DataLoader(train_ADBSdataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_ADBSdataset = ABSADataset(test_ADBSdata, config.device)\n",
    "test_loader = DataLoader(test_ADBSdataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /opt/data/private/huggingface/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /opt/data/private/huggingface/models--bert-base-chinese/snapshots/c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(config.bert_path, num_labels=1)\n",
    "model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_loader) * config.num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 1.0637214183807373\n",
      "Epoch: 0, Step: 10, Loss: 0.39778560400009155\n",
      "Epoch: 0, Step: 20, Loss: 0.3881106376647949\n",
      "Epoch: 0, Step: 30, Loss: 0.3352685570716858\n",
      "Epoch: 0, Step: 40, Loss: 0.3070780634880066\n",
      "Epoch: 0, Step: 50, Loss: 0.3166796863079071\n",
      "Epoch: 0, Step: 60, Loss: 0.4969061017036438\n",
      "Epoch: 0, Step: 70, Loss: 0.22093801200389862\n",
      "Epoch: 0, Step: 80, Loss: 0.471129834651947\n",
      "Epoch: 0, Step: 90, Loss: 0.22954508662223816\n",
      "Epoch: 0, Step: 100, Loss: 0.3182803988456726\n",
      "Epoch: 0, Step: 110, Loss: 0.24896395206451416\n",
      "Epoch: 0, Step: 120, Loss: 0.32637643814086914\n",
      "Epoch: 0, Step: 130, Loss: 0.3014448881149292\n",
      "Epoch: 0, Step: 140, Loss: 0.3703632354736328\n",
      "Epoch: 0, Step: 150, Loss: 0.44550827145576477\n",
      "Epoch: 0, Step: 160, Loss: 0.2635398209095001\n",
      "Epoch: 0, Step: 170, Loss: 0.3031829297542572\n",
      "Epoch: 0, Step: 180, Loss: 0.3802475929260254\n",
      "Epoch: 0, Step: 190, Loss: 0.23148801922798157\n",
      "Epoch: 0, Step: 200, Loss: 0.27418023347854614\n",
      "Epoch: 0, Step: 210, Loss: 0.24941760301589966\n",
      "Epoch: 0, Step: 220, Loss: 0.25340771675109863\n",
      "Epoch: 0, Step: 230, Loss: 0.28329116106033325\n",
      "Epoch: 0, Step: 240, Loss: 0.26940667629241943\n",
      "Epoch: 0, Step: 250, Loss: 0.38387590646743774\n",
      "Epoch: 0, Step: 260, Loss: 0.2150212675333023\n",
      "Epoch: 0, Step: 270, Loss: 0.25010591745376587\n",
      "Epoch: 0, Step: 280, Loss: 0.31061434745788574\n",
      "Epoch: 0, Step: 290, Loss: 0.22858503460884094\n",
      "Epoch: 0, Step: 300, Loss: 0.07364548742771149\n",
      "Epoch: 1, Step: 0, Loss: 0.14854471385478973\n",
      "Epoch: 1, Step: 10, Loss: 0.29186350107192993\n",
      "Epoch: 1, Step: 20, Loss: 0.382083535194397\n",
      "Epoch: 1, Step: 30, Loss: 0.267894446849823\n",
      "Epoch: 1, Step: 40, Loss: 0.3041640520095825\n",
      "Epoch: 1, Step: 50, Loss: 0.2867208421230316\n",
      "Epoch: 1, Step: 60, Loss: 0.3205462098121643\n",
      "Epoch: 1, Step: 70, Loss: 0.14863811433315277\n",
      "Epoch: 1, Step: 80, Loss: 0.3970277011394501\n",
      "Epoch: 1, Step: 90, Loss: 0.20806445181369781\n",
      "Epoch: 1, Step: 100, Loss: 0.14658987522125244\n",
      "Epoch: 1, Step: 110, Loss: 0.4095526933670044\n",
      "Epoch: 1, Step: 120, Loss: 0.23280978202819824\n",
      "Epoch: 1, Step: 130, Loss: 0.28154242038726807\n",
      "Epoch: 1, Step: 140, Loss: 0.3107299208641052\n",
      "Epoch: 1, Step: 150, Loss: 0.2243427038192749\n",
      "Epoch: 1, Step: 160, Loss: 0.17073938250541687\n",
      "Epoch: 1, Step: 170, Loss: 0.24341173470020294\n",
      "Epoch: 1, Step: 180, Loss: 0.2784496545791626\n",
      "Epoch: 1, Step: 190, Loss: 0.26011618971824646\n",
      "Epoch: 1, Step: 200, Loss: 0.10563408583402634\n",
      "Epoch: 1, Step: 210, Loss: 0.28956127166748047\n",
      "Epoch: 1, Step: 220, Loss: 0.1561100035905838\n",
      "Epoch: 1, Step: 230, Loss: 0.16843777894973755\n",
      "Epoch: 1, Step: 240, Loss: 0.2681427597999573\n",
      "Epoch: 1, Step: 250, Loss: 0.3957393169403076\n",
      "Epoch: 1, Step: 260, Loss: 0.2849188446998596\n",
      "Epoch: 1, Step: 270, Loss: 0.26612138748168945\n",
      "Epoch: 1, Step: 280, Loss: 0.18891532719135284\n",
      "Epoch: 1, Step: 290, Loss: 0.13541650772094727\n",
      "Epoch: 1, Step: 300, Loss: 0.006044427398592234\n",
      "Epoch: 2, Step: 0, Loss: 0.23313584923744202\n",
      "Epoch: 2, Step: 10, Loss: 0.17461618781089783\n",
      "Epoch: 2, Step: 20, Loss: 0.25912749767303467\n",
      "Epoch: 2, Step: 30, Loss: 0.17004919052124023\n",
      "Epoch: 2, Step: 40, Loss: 0.32387635111808777\n",
      "Epoch: 2, Step: 50, Loss: 0.14725062251091003\n",
      "Epoch: 2, Step: 60, Loss: 0.2335004359483719\n",
      "Epoch: 2, Step: 70, Loss: 0.24594640731811523\n",
      "Epoch: 2, Step: 80, Loss: 0.26826781034469604\n",
      "Epoch: 2, Step: 90, Loss: 0.16969817876815796\n",
      "Epoch: 2, Step: 100, Loss: 0.0882171243429184\n",
      "Epoch: 2, Step: 110, Loss: 0.2154133915901184\n",
      "Epoch: 2, Step: 120, Loss: 0.13085128366947174\n",
      "Epoch: 2, Step: 130, Loss: 0.27220651507377625\n",
      "Epoch: 2, Step: 140, Loss: 0.13088303804397583\n",
      "Epoch: 2, Step: 150, Loss: 0.08550885319709778\n",
      "Epoch: 2, Step: 160, Loss: 0.2322278767824173\n",
      "Epoch: 2, Step: 170, Loss: 0.1788087636232376\n",
      "Epoch: 2, Step: 180, Loss: 0.20925089716911316\n",
      "Epoch: 2, Step: 190, Loss: 0.23140370845794678\n",
      "Epoch: 2, Step: 200, Loss: 0.26920264959335327\n",
      "Epoch: 2, Step: 210, Loss: 0.2195383906364441\n",
      "Epoch: 2, Step: 220, Loss: 0.19719843566417694\n",
      "Epoch: 2, Step: 230, Loss: 0.2875666618347168\n",
      "Epoch: 2, Step: 240, Loss: 0.1786731481552124\n",
      "Epoch: 2, Step: 250, Loss: 0.3153236508369446\n",
      "Epoch: 2, Step: 260, Loss: 0.24416139721870422\n",
      "Epoch: 2, Step: 270, Loss: 0.21186934411525726\n",
      "Epoch: 2, Step: 280, Loss: 0.27122962474823\n",
      "Epoch: 2, Step: 290, Loss: 0.14548811316490173\n",
      "Epoch: 2, Step: 300, Loss: 0.0031306876335293055\n",
      "Epoch: 3, Step: 0, Loss: 0.13671067357063293\n",
      "Epoch: 3, Step: 10, Loss: 0.12242817878723145\n",
      "Epoch: 3, Step: 20, Loss: 0.11072458326816559\n",
      "Epoch: 3, Step: 30, Loss: 0.049727603793144226\n",
      "Epoch: 3, Step: 40, Loss: 0.1765613853931427\n",
      "Epoch: 3, Step: 50, Loss: 0.09097661077976227\n",
      "Epoch: 3, Step: 60, Loss: 0.17503398656845093\n",
      "Epoch: 3, Step: 70, Loss: 0.2978021502494812\n",
      "Epoch: 3, Step: 80, Loss: 0.11951358616352081\n",
      "Epoch: 3, Step: 90, Loss: 0.06912174075841904\n",
      "Epoch: 3, Step: 100, Loss: 0.15383601188659668\n",
      "Epoch: 3, Step: 110, Loss: 0.09026453644037247\n",
      "Epoch: 3, Step: 120, Loss: 0.18469592928886414\n",
      "Epoch: 3, Step: 130, Loss: 0.12441293895244598\n",
      "Epoch: 3, Step: 140, Loss: 0.136656254529953\n",
      "Epoch: 3, Step: 150, Loss: 0.12072055786848068\n",
      "Epoch: 3, Step: 160, Loss: 0.18312278389930725\n",
      "Epoch: 3, Step: 170, Loss: 0.13624237477779388\n",
      "Epoch: 3, Step: 180, Loss: 0.2168554663658142\n",
      "Epoch: 3, Step: 190, Loss: 0.22994521260261536\n",
      "Epoch: 3, Step: 200, Loss: 0.1484988033771515\n",
      "Epoch: 3, Step: 210, Loss: 0.20241326093673706\n",
      "Epoch: 3, Step: 220, Loss: 0.1618223786354065\n",
      "Epoch: 3, Step: 230, Loss: 0.1429707109928131\n",
      "Epoch: 3, Step: 240, Loss: 0.052422285079956055\n",
      "Epoch: 3, Step: 250, Loss: 0.06792118400335312\n",
      "Epoch: 3, Step: 260, Loss: 0.2793048918247223\n",
      "Epoch: 3, Step: 270, Loss: 0.09611393511295319\n",
      "Epoch: 3, Step: 280, Loss: 0.20538371801376343\n",
      "Epoch: 3, Step: 290, Loss: 0.2198767066001892\n",
      "Epoch: 3, Step: 300, Loss: 0.46309003233909607\n",
      "Epoch: 4, Step: 0, Loss: 0.10108346492052078\n",
      "Epoch: 4, Step: 10, Loss: 0.2206171751022339\n",
      "Epoch: 4, Step: 20, Loss: 0.22936666011810303\n",
      "Epoch: 4, Step: 30, Loss: 0.14532765746116638\n",
      "Epoch: 4, Step: 40, Loss: 0.21127957105636597\n",
      "Epoch: 4, Step: 50, Loss: 0.13966865837574005\n",
      "Epoch: 4, Step: 60, Loss: 0.09199114888906479\n",
      "Epoch: 4, Step: 70, Loss: 0.19913864135742188\n",
      "Epoch: 4, Step: 80, Loss: 0.09678328782320023\n",
      "Epoch: 4, Step: 90, Loss: 0.11749275773763657\n",
      "Epoch: 4, Step: 100, Loss: 0.03436759114265442\n",
      "Epoch: 4, Step: 110, Loss: 0.10618814080953598\n",
      "Epoch: 4, Step: 120, Loss: 0.2675703167915344\n",
      "Epoch: 4, Step: 130, Loss: 0.13378503918647766\n",
      "Epoch: 4, Step: 140, Loss: 0.13663601875305176\n",
      "Epoch: 4, Step: 150, Loss: 0.17382152378559113\n",
      "Epoch: 4, Step: 160, Loss: 0.189749076962471\n",
      "Epoch: 4, Step: 170, Loss: 0.09762229025363922\n",
      "Epoch: 4, Step: 180, Loss: 0.1269550621509552\n",
      "Epoch: 4, Step: 190, Loss: 0.11238476634025574\n",
      "Epoch: 4, Step: 200, Loss: 0.10342980921268463\n",
      "Epoch: 4, Step: 210, Loss: 0.19644460082054138\n",
      "Epoch: 4, Step: 220, Loss: 0.13323621451854706\n",
      "Epoch: 4, Step: 230, Loss: 0.12896443903446198\n",
      "Epoch: 4, Step: 240, Loss: 0.2100401222705841\n",
      "Epoch: 4, Step: 250, Loss: 0.02918297052383423\n",
      "Epoch: 4, Step: 260, Loss: 0.2372899353504181\n",
      "Epoch: 4, Step: 270, Loss: 0.17287492752075195\n",
      "Epoch: 4, Step: 280, Loss: 0.18551994860172272\n",
      "Epoch: 4, Step: 290, Loss: 0.14176011085510254\n",
      "Epoch: 4, Step: 300, Loss: 0.10808198153972626\n",
      "Epoch: 5, Step: 0, Loss: 0.08186059445142746\n",
      "Epoch: 5, Step: 10, Loss: 0.17324557900428772\n",
      "Epoch: 5, Step: 20, Loss: 0.14175935089588165\n",
      "Epoch: 5, Step: 30, Loss: 0.18268784880638123\n",
      "Epoch: 5, Step: 40, Loss: 0.09118615090847015\n",
      "Epoch: 5, Step: 50, Loss: 0.0930560827255249\n",
      "Epoch: 5, Step: 60, Loss: 0.09289799630641937\n",
      "Epoch: 5, Step: 70, Loss: 0.06661117076873779\n",
      "Epoch: 5, Step: 80, Loss: 0.14783185720443726\n",
      "Epoch: 5, Step: 90, Loss: 0.18726986646652222\n",
      "Epoch: 5, Step: 100, Loss: 0.21633735299110413\n",
      "Epoch: 5, Step: 110, Loss: 0.08307094871997833\n",
      "Epoch: 5, Step: 120, Loss: 0.12461492419242859\n",
      "Epoch: 5, Step: 130, Loss: 0.0898800864815712\n",
      "Epoch: 5, Step: 140, Loss: 0.06650412827730179\n",
      "Epoch: 5, Step: 150, Loss: 0.13565421104431152\n",
      "Epoch: 5, Step: 160, Loss: 0.043514177203178406\n",
      "Epoch: 5, Step: 170, Loss: 0.12371706962585449\n",
      "Epoch: 5, Step: 180, Loss: 0.12039222568273544\n",
      "Epoch: 5, Step: 190, Loss: 0.1295984983444214\n",
      "Epoch: 5, Step: 200, Loss: 0.15280009806156158\n",
      "Epoch: 5, Step: 210, Loss: 0.1814122498035431\n",
      "Epoch: 5, Step: 220, Loss: 0.1501227766275406\n",
      "Epoch: 5, Step: 230, Loss: 0.11757171899080276\n",
      "Epoch: 5, Step: 240, Loss: 0.08536332100629807\n",
      "Epoch: 5, Step: 250, Loss: 0.09022243320941925\n",
      "Epoch: 5, Step: 260, Loss: 0.11582200229167938\n",
      "Epoch: 5, Step: 270, Loss: 0.14342454075813293\n",
      "Epoch: 5, Step: 280, Loss: 0.10110140591859818\n",
      "Epoch: 5, Step: 290, Loss: 0.24856266379356384\n",
      "Epoch: 5, Step: 300, Loss: 0.036128703504800797\n",
      "Epoch: 6, Step: 0, Loss: 0.08919504284858704\n",
      "Epoch: 6, Step: 10, Loss: 0.13735046982765198\n",
      "Epoch: 6, Step: 20, Loss: 0.05868755653500557\n",
      "Epoch: 6, Step: 30, Loss: 0.15440590679645538\n",
      "Epoch: 6, Step: 40, Loss: 0.07001753151416779\n",
      "Epoch: 6, Step: 50, Loss: 0.05938161164522171\n",
      "Epoch: 6, Step: 60, Loss: 0.14473815262317657\n",
      "Epoch: 6, Step: 70, Loss: 0.10899541527032852\n",
      "Epoch: 6, Step: 80, Loss: 0.13686279952526093\n",
      "Epoch: 6, Step: 90, Loss: 0.25867533683776855\n",
      "Epoch: 6, Step: 100, Loss: 0.028079699724912643\n",
      "Epoch: 6, Step: 110, Loss: 0.05585290491580963\n",
      "Epoch: 6, Step: 120, Loss: 0.09877210855484009\n",
      "Epoch: 6, Step: 130, Loss: 0.12802845239639282\n",
      "Epoch: 6, Step: 140, Loss: 0.05438285693526268\n",
      "Epoch: 6, Step: 150, Loss: 0.05926454812288284\n",
      "Epoch: 6, Step: 160, Loss: 0.17105896770954132\n",
      "Epoch: 6, Step: 170, Loss: 0.15727922320365906\n",
      "Epoch: 6, Step: 180, Loss: 0.09056513011455536\n",
      "Epoch: 6, Step: 190, Loss: 0.08183524012565613\n",
      "Epoch: 6, Step: 200, Loss: 0.1048789769411087\n",
      "Epoch: 6, Step: 210, Loss: 0.07079203426837921\n",
      "Epoch: 6, Step: 220, Loss: 0.03505636006593704\n",
      "Epoch: 6, Step: 230, Loss: 0.05547782778739929\n",
      "Epoch: 6, Step: 240, Loss: 0.16631489992141724\n",
      "Epoch: 6, Step: 250, Loss: 0.11639542132616043\n",
      "Epoch: 6, Step: 260, Loss: 0.17469905316829681\n",
      "Epoch: 6, Step: 270, Loss: 0.1844673454761505\n",
      "Epoch: 6, Step: 280, Loss: 0.07897752523422241\n",
      "Epoch: 6, Step: 290, Loss: 0.0921168178319931\n",
      "Epoch: 6, Step: 300, Loss: 0.0003058802685700357\n",
      "Epoch: 7, Step: 0, Loss: 0.09042266756296158\n",
      "Epoch: 7, Step: 10, Loss: 0.06076100468635559\n",
      "Epoch: 7, Step: 20, Loss: 0.03549351543188095\n",
      "Epoch: 7, Step: 30, Loss: 0.037141237407922745\n",
      "Epoch: 7, Step: 40, Loss: 0.09782044589519501\n",
      "Epoch: 7, Step: 50, Loss: 0.08166411519050598\n",
      "Epoch: 7, Step: 60, Loss: 0.09890168905258179\n",
      "Epoch: 7, Step: 70, Loss: 0.13821271061897278\n",
      "Epoch: 7, Step: 80, Loss: 0.12357701361179352\n",
      "Epoch: 7, Step: 90, Loss: 0.08597640693187714\n",
      "Epoch: 7, Step: 100, Loss: 0.08587707579135895\n",
      "Epoch: 7, Step: 110, Loss: 0.045922234654426575\n",
      "Epoch: 7, Step: 120, Loss: 0.07694551348686218\n",
      "Epoch: 7, Step: 130, Loss: 0.0671718642115593\n",
      "Epoch: 7, Step: 140, Loss: 0.08776072412729263\n",
      "Epoch: 7, Step: 150, Loss: 0.17958003282546997\n",
      "Epoch: 7, Step: 160, Loss: 0.059047210961580276\n",
      "Epoch: 7, Step: 170, Loss: 0.04415978863835335\n",
      "Epoch: 7, Step: 180, Loss: 0.02923792228102684\n",
      "Epoch: 7, Step: 190, Loss: 0.041411153972148895\n",
      "Epoch: 7, Step: 200, Loss: 0.12567541003227234\n",
      "Epoch: 7, Step: 210, Loss: 0.019737912341952324\n",
      "Epoch: 7, Step: 220, Loss: 0.05785394087433815\n",
      "Epoch: 7, Step: 230, Loss: 0.10072647780179977\n",
      "Epoch: 7, Step: 240, Loss: 0.058577269315719604\n",
      "Epoch: 7, Step: 250, Loss: 0.05159788578748703\n",
      "Epoch: 7, Step: 260, Loss: 0.059872206300497055\n",
      "Epoch: 7, Step: 270, Loss: 0.053561851382255554\n",
      "Epoch: 7, Step: 280, Loss: 0.06501930207014084\n",
      "Epoch: 7, Step: 290, Loss: 0.02880682609975338\n",
      "Epoch: 7, Step: 300, Loss: 0.016672834753990173\n",
      "Epoch: 8, Step: 0, Loss: 0.05683364346623421\n",
      "Epoch: 8, Step: 10, Loss: 0.06501464545726776\n",
      "Epoch: 8, Step: 20, Loss: 0.04334532469511032\n",
      "Epoch: 8, Step: 30, Loss: 0.08995455503463745\n",
      "Epoch: 8, Step: 40, Loss: 0.05684451386332512\n",
      "Epoch: 8, Step: 50, Loss: 0.06183259189128876\n",
      "Epoch: 8, Step: 60, Loss: 0.07501702010631561\n",
      "Epoch: 8, Step: 70, Loss: 0.09079140424728394\n",
      "Epoch: 8, Step: 80, Loss: 0.06517073512077332\n",
      "Epoch: 8, Step: 90, Loss: 0.07216988503932953\n",
      "Epoch: 8, Step: 100, Loss: 0.06142047420144081\n",
      "Epoch: 8, Step: 110, Loss: 0.06668359041213989\n",
      "Epoch: 8, Step: 120, Loss: 0.07227849960327148\n",
      "Epoch: 8, Step: 130, Loss: 0.09563186019659042\n",
      "Epoch: 8, Step: 140, Loss: 0.014503140933811665\n",
      "Epoch: 8, Step: 150, Loss: 0.014443648979067802\n",
      "Epoch: 8, Step: 160, Loss: 0.07284311950206757\n",
      "Epoch: 8, Step: 170, Loss: 0.06151828542351723\n",
      "Epoch: 8, Step: 180, Loss: 0.04085854813456535\n",
      "Epoch: 8, Step: 190, Loss: 0.12200678884983063\n",
      "Epoch: 8, Step: 200, Loss: 0.1520199179649353\n",
      "Epoch: 8, Step: 210, Loss: 0.03649640083312988\n",
      "Epoch: 8, Step: 220, Loss: 0.12370516359806061\n",
      "Epoch: 8, Step: 230, Loss: 0.05220899358391762\n",
      "Epoch: 8, Step: 240, Loss: 0.019008943811058998\n",
      "Epoch: 8, Step: 250, Loss: 0.10260288417339325\n",
      "Epoch: 8, Step: 260, Loss: 0.05070091784000397\n",
      "Epoch: 8, Step: 270, Loss: 0.0865306705236435\n",
      "Epoch: 8, Step: 280, Loss: 0.12106794118881226\n",
      "Epoch: 8, Step: 290, Loss: 0.074711874127388\n",
      "Epoch: 8, Step: 300, Loss: 0.008880112320184708\n",
      "Epoch: 9, Step: 0, Loss: 0.07595067471265793\n",
      "Epoch: 9, Step: 10, Loss: 0.0729985460639\n",
      "Epoch: 9, Step: 20, Loss: 0.05770549923181534\n",
      "Epoch: 9, Step: 30, Loss: 0.06293634325265884\n",
      "Epoch: 9, Step: 40, Loss: 0.04605531692504883\n",
      "Epoch: 9, Step: 50, Loss: 0.007490483112633228\n",
      "Epoch: 9, Step: 60, Loss: 0.08423765003681183\n",
      "Epoch: 9, Step: 70, Loss: 0.09197291731834412\n",
      "Epoch: 9, Step: 80, Loss: 0.04551725089550018\n",
      "Epoch: 9, Step: 90, Loss: 0.07250304520130157\n",
      "Epoch: 9, Step: 100, Loss: 0.051882367581129074\n",
      "Epoch: 9, Step: 110, Loss: 0.03711366280913353\n",
      "Epoch: 9, Step: 120, Loss: 0.12343409657478333\n",
      "Epoch: 9, Step: 130, Loss: 0.10742072016000748\n",
      "Epoch: 9, Step: 140, Loss: 0.05726628750562668\n",
      "Epoch: 9, Step: 150, Loss: 0.029522575438022614\n",
      "Epoch: 9, Step: 160, Loss: 0.04426019638776779\n",
      "Epoch: 9, Step: 170, Loss: 0.04105882719159126\n",
      "Epoch: 9, Step: 180, Loss: 0.05007199943065643\n",
      "Epoch: 9, Step: 190, Loss: 0.011451744474470615\n",
      "Epoch: 9, Step: 200, Loss: 0.06949770450592041\n",
      "Epoch: 9, Step: 210, Loss: 0.04215044900774956\n",
      "Epoch: 9, Step: 220, Loss: 0.09397606551647186\n",
      "Epoch: 9, Step: 230, Loss: 0.03889096528291702\n",
      "Epoch: 9, Step: 240, Loss: 0.044742193073034286\n",
      "Epoch: 9, Step: 250, Loss: 0.07667352259159088\n",
      "Epoch: 9, Step: 260, Loss: 0.05330280214548111\n",
      "Epoch: 9, Step: 270, Loss: 0.01820369064807892\n",
      "Epoch: 9, Step: 280, Loss: 0.04160093888640404\n",
      "Epoch: 9, Step: 290, Loss: 0.14869600534439087\n",
      "Epoch: 9, Step: 300, Loss: 0.0007311489898711443\n",
      "Epoch: 10, Step: 0, Loss: 0.046188835054636\n",
      "Epoch: 10, Step: 10, Loss: 0.0630902498960495\n",
      "Epoch: 10, Step: 20, Loss: 0.13123643398284912\n",
      "Epoch: 10, Step: 30, Loss: 0.015651773661375046\n",
      "Epoch: 10, Step: 40, Loss: 0.011553416028618813\n",
      "Epoch: 10, Step: 50, Loss: 0.03661108762025833\n",
      "Epoch: 10, Step: 60, Loss: 0.1119607612490654\n",
      "Epoch: 10, Step: 70, Loss: 0.018776383250951767\n",
      "Epoch: 10, Step: 80, Loss: 0.00784902274608612\n",
      "Epoch: 10, Step: 90, Loss: 0.04025205597281456\n",
      "Epoch: 10, Step: 100, Loss: 0.058946043252944946\n",
      "Epoch: 10, Step: 110, Loss: 0.008096386678516865\n",
      "Epoch: 10, Step: 120, Loss: 0.027151577174663544\n",
      "Epoch: 10, Step: 130, Loss: 0.07259905338287354\n",
      "Epoch: 10, Step: 140, Loss: 0.04761312156915665\n",
      "Epoch: 10, Step: 150, Loss: 0.053998641669750214\n",
      "Epoch: 10, Step: 160, Loss: 0.06134407967329025\n",
      "Epoch: 10, Step: 170, Loss: 0.12791909277439117\n",
      "Epoch: 10, Step: 180, Loss: 0.02648324891924858\n",
      "Epoch: 10, Step: 190, Loss: 0.072568379342556\n",
      "Epoch: 10, Step: 200, Loss: 0.07240565866231918\n",
      "Epoch: 10, Step: 210, Loss: 0.029874861240386963\n",
      "Epoch: 10, Step: 220, Loss: 0.08420495688915253\n",
      "Epoch: 10, Step: 230, Loss: 0.0819711834192276\n",
      "Epoch: 10, Step: 240, Loss: 0.045313410460948944\n",
      "Epoch: 10, Step: 250, Loss: 0.039104826748371124\n",
      "Epoch: 10, Step: 260, Loss: 0.02638668566942215\n",
      "Epoch: 10, Step: 270, Loss: 0.14927127957344055\n",
      "Epoch: 10, Step: 280, Loss: 0.24390965700149536\n",
      "Epoch: 10, Step: 290, Loss: 0.06345531344413757\n",
      "Epoch: 10, Step: 300, Loss: 0.01689934730529785\n",
      "Epoch: 11, Step: 0, Loss: 0.05850280076265335\n",
      "Epoch: 11, Step: 10, Loss: 0.13096867501735687\n",
      "Epoch: 11, Step: 20, Loss: 0.020298097282648087\n",
      "Epoch: 11, Step: 30, Loss: 0.039168212562799454\n",
      "Epoch: 11, Step: 40, Loss: 0.22814832627773285\n",
      "Epoch: 11, Step: 50, Loss: 0.05189640820026398\n",
      "Epoch: 11, Step: 60, Loss: 0.11880920827388763\n",
      "Epoch: 11, Step: 70, Loss: 0.013555614277720451\n",
      "Epoch: 11, Step: 80, Loss: 0.06473999470472336\n",
      "Epoch: 11, Step: 90, Loss: 0.037762485444545746\n",
      "Epoch: 11, Step: 100, Loss: 0.036949481815099716\n",
      "Epoch: 11, Step: 110, Loss: 0.10589458048343658\n",
      "Epoch: 11, Step: 120, Loss: 0.015607167035341263\n",
      "Epoch: 11, Step: 130, Loss: 0.07577662169933319\n",
      "Epoch: 11, Step: 140, Loss: 0.06007913500070572\n",
      "Epoch: 11, Step: 150, Loss: 0.14863714575767517\n",
      "Epoch: 11, Step: 160, Loss: 0.06042507290840149\n",
      "Epoch: 11, Step: 170, Loss: 0.05734030157327652\n",
      "Epoch: 11, Step: 180, Loss: 0.12029782682657242\n",
      "Epoch: 11, Step: 190, Loss: 0.026289619505405426\n",
      "Epoch: 11, Step: 200, Loss: 0.062375906854867935\n",
      "Epoch: 11, Step: 210, Loss: 0.010061725974082947\n",
      "Epoch: 11, Step: 220, Loss: 0.034811656922101974\n",
      "Epoch: 11, Step: 230, Loss: 0.038017638027668\n",
      "Epoch: 11, Step: 240, Loss: 0.007200511172413826\n",
      "Epoch: 11, Step: 250, Loss: 0.06632466614246368\n",
      "Epoch: 11, Step: 260, Loss: 0.1154899150133133\n",
      "Epoch: 11, Step: 270, Loss: 0.04163500666618347\n",
      "Epoch: 11, Step: 280, Loss: 0.04370967298746109\n",
      "Epoch: 11, Step: 290, Loss: 0.04707175865769386\n",
      "Epoch: 11, Step: 300, Loss: 0.005175544414669275\n",
      "Epoch: 12, Step: 0, Loss: 0.054991066455841064\n",
      "Epoch: 12, Step: 10, Loss: 0.04477819800376892\n",
      "Epoch: 12, Step: 20, Loss: 0.05105479806661606\n",
      "Epoch: 12, Step: 30, Loss: 0.05546921119093895\n",
      "Epoch: 12, Step: 40, Loss: 0.03796130791306496\n",
      "Epoch: 12, Step: 50, Loss: 0.02606760524213314\n",
      "Epoch: 12, Step: 60, Loss: 0.03177182748913765\n",
      "Epoch: 12, Step: 70, Loss: 0.007195238023996353\n",
      "Epoch: 12, Step: 80, Loss: 0.13750308752059937\n",
      "Epoch: 12, Step: 90, Loss: 0.044300660490989685\n",
      "Epoch: 12, Step: 100, Loss: 0.06777579337358475\n",
      "Epoch: 12, Step: 110, Loss: 0.016946617513895035\n",
      "Epoch: 12, Step: 120, Loss: 0.028431447222828865\n",
      "Epoch: 12, Step: 130, Loss: 0.08052901923656464\n",
      "Epoch: 12, Step: 140, Loss: 0.06290528178215027\n",
      "Epoch: 12, Step: 150, Loss: 0.036402780562639236\n",
      "Epoch: 12, Step: 160, Loss: 0.09047074615955353\n",
      "Epoch: 12, Step: 170, Loss: 0.06859792768955231\n",
      "Epoch: 12, Step: 180, Loss: 0.026865819469094276\n",
      "Epoch: 12, Step: 190, Loss: 0.00442887470126152\n",
      "Epoch: 12, Step: 200, Loss: 0.010962508618831635\n",
      "Epoch: 12, Step: 210, Loss: 0.06158572435379028\n",
      "Epoch: 12, Step: 220, Loss: 0.060038045048713684\n",
      "Epoch: 12, Step: 230, Loss: 0.06815220415592194\n",
      "Epoch: 12, Step: 240, Loss: 0.017448022961616516\n",
      "Epoch: 12, Step: 250, Loss: 0.03898847848176956\n",
      "Epoch: 12, Step: 260, Loss: 0.007686357945203781\n",
      "Epoch: 12, Step: 270, Loss: 0.0899180918931961\n",
      "Epoch: 12, Step: 280, Loss: 0.02147674560546875\n",
      "Epoch: 12, Step: 290, Loss: 0.0964648649096489\n",
      "Epoch: 12, Step: 300, Loss: 2.057482379314024e-05\n",
      "Epoch: 13, Step: 0, Loss: 0.06585165858268738\n",
      "Epoch: 13, Step: 10, Loss: 0.04558056965470314\n",
      "Epoch: 13, Step: 20, Loss: 0.023669864982366562\n",
      "Epoch: 13, Step: 30, Loss: 0.05053485557436943\n",
      "Epoch: 13, Step: 40, Loss: 0.05386705696582794\n",
      "Epoch: 13, Step: 50, Loss: 0.03589007630944252\n",
      "Epoch: 13, Step: 60, Loss: 0.1349852830171585\n",
      "Epoch: 13, Step: 70, Loss: 0.0636480301618576\n",
      "Epoch: 13, Step: 80, Loss: 0.02607758343219757\n",
      "Epoch: 13, Step: 90, Loss: 0.010793084278702736\n",
      "Epoch: 13, Step: 100, Loss: 0.014634417369961739\n",
      "Epoch: 13, Step: 110, Loss: 0.07202658802270889\n",
      "Epoch: 13, Step: 120, Loss: 0.04447251185774803\n",
      "Epoch: 13, Step: 130, Loss: 0.037523627281188965\n",
      "Epoch: 13, Step: 140, Loss: 0.09875614941120148\n",
      "Epoch: 13, Step: 150, Loss: 0.02541418746113777\n",
      "Epoch: 13, Step: 160, Loss: 0.042626652866601944\n",
      "Epoch: 13, Step: 170, Loss: 0.1236075833439827\n",
      "Epoch: 13, Step: 180, Loss: 0.10414406657218933\n",
      "Epoch: 13, Step: 190, Loss: 0.10720349103212357\n",
      "Epoch: 13, Step: 200, Loss: 0.08441454917192459\n",
      "Epoch: 13, Step: 210, Loss: 0.025520507246255875\n",
      "Epoch: 13, Step: 220, Loss: 0.010089430958032608\n",
      "Epoch: 13, Step: 230, Loss: 0.007140319794416428\n",
      "Epoch: 13, Step: 240, Loss: 0.04733296111226082\n",
      "Epoch: 13, Step: 250, Loss: 0.008596155792474747\n",
      "Epoch: 13, Step: 260, Loss: 0.02324272133409977\n",
      "Epoch: 13, Step: 270, Loss: 0.08439293503761292\n",
      "Epoch: 13, Step: 280, Loss: 0.051654018461704254\n",
      "Epoch: 13, Step: 290, Loss: 0.020159775391221046\n",
      "Epoch: 13, Step: 300, Loss: 0.035993825644254684\n",
      "Epoch: 14, Step: 0, Loss: 0.00804231408983469\n",
      "Epoch: 14, Step: 10, Loss: 0.024672986939549446\n",
      "Epoch: 14, Step: 20, Loss: 0.05412966012954712\n",
      "Epoch: 14, Step: 30, Loss: 0.06736765801906586\n",
      "Epoch: 14, Step: 40, Loss: 0.011410575360059738\n",
      "Epoch: 14, Step: 50, Loss: 0.06130409240722656\n",
      "Epoch: 14, Step: 60, Loss: 0.007450352422893047\n",
      "Epoch: 14, Step: 70, Loss: 0.007675565779209137\n",
      "Epoch: 14, Step: 80, Loss: 0.04171900823712349\n",
      "Epoch: 14, Step: 90, Loss: 0.07195709645748138\n",
      "Epoch: 14, Step: 100, Loss: 0.005299795418977737\n",
      "Epoch: 14, Step: 110, Loss: 0.03406907618045807\n",
      "Epoch: 14, Step: 120, Loss: 0.07325168699026108\n",
      "Epoch: 14, Step: 130, Loss: 0.06152427941560745\n",
      "Epoch: 14, Step: 140, Loss: 0.04306154325604439\n",
      "Epoch: 14, Step: 150, Loss: 0.0626271516084671\n",
      "Epoch: 14, Step: 160, Loss: 0.04177745059132576\n",
      "Epoch: 14, Step: 170, Loss: 0.034964419901371\n",
      "Epoch: 14, Step: 180, Loss: 0.027045801281929016\n",
      "Epoch: 14, Step: 190, Loss: 0.07310175150632858\n",
      "Epoch: 14, Step: 200, Loss: 0.03193693608045578\n",
      "Epoch: 14, Step: 210, Loss: 0.008494614623486996\n",
      "Epoch: 14, Step: 220, Loss: 0.08601859956979752\n",
      "Epoch: 14, Step: 230, Loss: 0.011048508808016777\n",
      "Epoch: 14, Step: 240, Loss: 0.004690444562584162\n",
      "Epoch: 14, Step: 250, Loss: 0.039219051599502563\n",
      "Epoch: 14, Step: 260, Loss: 0.07485363632440567\n",
      "Epoch: 14, Step: 270, Loss: 0.006070716306567192\n",
      "Epoch: 14, Step: 280, Loss: 0.11671433597803116\n",
      "Epoch: 14, Step: 290, Loss: 0.03179733082652092\n",
      "Epoch: 14, Step: 300, Loss: 0.0028305132873356342\n",
      "Epoch: 15, Step: 0, Loss: 0.016624286770820618\n",
      "Epoch: 15, Step: 10, Loss: 0.006720534525811672\n",
      "Epoch: 15, Step: 20, Loss: 0.030448105186223984\n",
      "Epoch: 15, Step: 30, Loss: 0.02205621637403965\n",
      "Epoch: 15, Step: 40, Loss: 0.047631315886974335\n",
      "Epoch: 15, Step: 50, Loss: 0.00797007605433464\n",
      "Epoch: 15, Step: 60, Loss: 0.004402158781886101\n",
      "Epoch: 15, Step: 70, Loss: 0.007123420014977455\n",
      "Epoch: 15, Step: 80, Loss: 0.10922186076641083\n",
      "Epoch: 15, Step: 90, Loss: 0.07690536230802536\n",
      "Epoch: 15, Step: 100, Loss: 0.0329412966966629\n",
      "Epoch: 15, Step: 110, Loss: 0.03019758313894272\n",
      "Epoch: 15, Step: 120, Loss: 0.036935921758413315\n",
      "Epoch: 15, Step: 130, Loss: 0.07670789957046509\n",
      "Epoch: 15, Step: 140, Loss: 0.03419235721230507\n",
      "Epoch: 15, Step: 150, Loss: 0.026465099304914474\n",
      "Epoch: 15, Step: 160, Loss: 0.06508911401033401\n",
      "Epoch: 15, Step: 170, Loss: 0.03277011215686798\n",
      "Epoch: 15, Step: 180, Loss: 0.06603820621967316\n",
      "Epoch: 15, Step: 190, Loss: 0.010067395865917206\n",
      "Epoch: 15, Step: 200, Loss: 0.039327241480350494\n",
      "Epoch: 15, Step: 210, Loss: 0.057447146624326706\n",
      "Epoch: 15, Step: 220, Loss: 0.006303663365542889\n",
      "Epoch: 15, Step: 230, Loss: 0.003968413453549147\n",
      "Epoch: 15, Step: 240, Loss: 0.02719954214990139\n",
      "Epoch: 15, Step: 250, Loss: 0.04380101338028908\n",
      "Epoch: 15, Step: 260, Loss: 0.06726215034723282\n",
      "Epoch: 15, Step: 270, Loss: 0.05010802671313286\n",
      "Epoch: 15, Step: 280, Loss: 0.023189984261989594\n",
      "Epoch: 15, Step: 290, Loss: 0.007216387428343296\n",
      "Epoch: 15, Step: 300, Loss: 0.5157366991043091\n",
      "Epoch: 16, Step: 0, Loss: 0.022906241938471794\n",
      "Epoch: 16, Step: 10, Loss: 0.23377326130867004\n",
      "Epoch: 16, Step: 20, Loss: 0.1178729385137558\n",
      "Epoch: 16, Step: 30, Loss: 0.05180194228887558\n",
      "Epoch: 16, Step: 40, Loss: 0.045084334909915924\n",
      "Epoch: 16, Step: 50, Loss: 0.078160360455513\n",
      "Epoch: 16, Step: 60, Loss: 0.06327508389949799\n",
      "Epoch: 16, Step: 70, Loss: 0.07379167526960373\n",
      "Epoch: 16, Step: 80, Loss: 0.05514642968773842\n",
      "Epoch: 16, Step: 90, Loss: 0.04573787748813629\n",
      "Epoch: 16, Step: 100, Loss: 0.007343897130340338\n",
      "Epoch: 16, Step: 110, Loss: 0.007238553371280432\n",
      "Epoch: 16, Step: 120, Loss: 0.06905344873666763\n",
      "Epoch: 16, Step: 130, Loss: 0.05001014471054077\n",
      "Epoch: 16, Step: 140, Loss: 0.013525488786399364\n",
      "Epoch: 16, Step: 150, Loss: 0.016608230769634247\n",
      "Epoch: 16, Step: 160, Loss: 0.0599239282310009\n",
      "Epoch: 16, Step: 170, Loss: 0.06731253862380981\n",
      "Epoch: 16, Step: 180, Loss: 0.012300509959459305\n",
      "Epoch: 16, Step: 190, Loss: 0.04729011654853821\n",
      "Epoch: 16, Step: 200, Loss: 0.11382855474948883\n",
      "Epoch: 16, Step: 210, Loss: 0.06375167518854141\n",
      "Epoch: 16, Step: 220, Loss: 0.05244943127036095\n",
      "Epoch: 16, Step: 230, Loss: 0.009591193869709969\n",
      "Epoch: 16, Step: 240, Loss: 0.013121583499014378\n",
      "Epoch: 16, Step: 250, Loss: 0.012725920416414738\n",
      "Epoch: 16, Step: 260, Loss: 0.006600675638765097\n",
      "Epoch: 16, Step: 270, Loss: 0.17481276392936707\n",
      "Epoch: 16, Step: 280, Loss: 0.14185979962348938\n",
      "Epoch: 16, Step: 290, Loss: 0.04902898520231247\n",
      "Epoch: 16, Step: 300, Loss: 0.9626293182373047\n",
      "Epoch: 17, Step: 0, Loss: 0.013033241033554077\n",
      "Epoch: 17, Step: 10, Loss: 0.04542212188243866\n",
      "Epoch: 17, Step: 20, Loss: 0.030007379129529\n",
      "Epoch: 17, Step: 30, Loss: 0.009291893802583218\n",
      "Epoch: 17, Step: 40, Loss: 0.05760379880666733\n",
      "Epoch: 17, Step: 50, Loss: 0.06807434558868408\n",
      "Epoch: 17, Step: 60, Loss: 0.037980999797582626\n",
      "Epoch: 17, Step: 70, Loss: 0.05004662275314331\n",
      "Epoch: 17, Step: 80, Loss: 0.06041194498538971\n",
      "Epoch: 17, Step: 90, Loss: 0.06305179744958878\n",
      "Epoch: 17, Step: 100, Loss: 0.0700751543045044\n",
      "Epoch: 17, Step: 110, Loss: 0.12330517172813416\n",
      "Epoch: 17, Step: 120, Loss: 0.07500414550304413\n",
      "Epoch: 17, Step: 130, Loss: 0.00713718868792057\n",
      "Epoch: 17, Step: 140, Loss: 0.04065030813217163\n",
      "Epoch: 17, Step: 150, Loss: 0.006737814284861088\n",
      "Epoch: 17, Step: 160, Loss: 0.03179914131760597\n",
      "Epoch: 17, Step: 170, Loss: 0.04616045951843262\n",
      "Epoch: 17, Step: 180, Loss: 0.006341635249555111\n",
      "Epoch: 17, Step: 190, Loss: 0.08697354793548584\n",
      "Epoch: 17, Step: 200, Loss: 0.17096421122550964\n",
      "Epoch: 17, Step: 210, Loss: 0.02691984362900257\n",
      "Epoch: 17, Step: 220, Loss: 0.056131042540073395\n",
      "Epoch: 17, Step: 230, Loss: 0.07158611714839935\n",
      "Epoch: 17, Step: 240, Loss: 0.014066818170249462\n",
      "Epoch: 17, Step: 250, Loss: 0.0392712727189064\n",
      "Epoch: 17, Step: 260, Loss: 0.04153896123170853\n",
      "Epoch: 17, Step: 270, Loss: 0.010500917211174965\n",
      "Epoch: 17, Step: 280, Loss: 0.06480105221271515\n",
      "Epoch: 17, Step: 290, Loss: 0.060626689344644547\n",
      "Epoch: 17, Step: 300, Loss: 0.0011325017549097538\n",
      "Epoch: 18, Step: 0, Loss: 0.010811196640133858\n",
      "Epoch: 18, Step: 10, Loss: 0.1072746142745018\n",
      "Epoch: 18, Step: 20, Loss: 0.006168663501739502\n",
      "Epoch: 18, Step: 30, Loss: 0.004177482333034277\n",
      "Epoch: 18, Step: 40, Loss: 0.06041017174720764\n",
      "Epoch: 18, Step: 50, Loss: 0.005508627276867628\n",
      "Epoch: 18, Step: 60, Loss: 0.08812256902456284\n",
      "Epoch: 18, Step: 70, Loss: 0.0042884089052677155\n",
      "Epoch: 18, Step: 80, Loss: 0.005475088953971863\n",
      "Epoch: 18, Step: 90, Loss: 0.007460655644536018\n",
      "Epoch: 18, Step: 100, Loss: 0.007951054722070694\n",
      "Epoch: 18, Step: 110, Loss: 0.06675952672958374\n",
      "Epoch: 18, Step: 120, Loss: 0.007837818004190922\n",
      "Epoch: 18, Step: 130, Loss: 0.04462030902504921\n",
      "Epoch: 18, Step: 140, Loss: 0.005382333882153034\n",
      "Epoch: 18, Step: 150, Loss: 0.0048193614929914474\n",
      "Epoch: 18, Step: 160, Loss: 0.044057782739400864\n",
      "Epoch: 18, Step: 170, Loss: 0.0170277189463377\n",
      "Epoch: 18, Step: 180, Loss: 0.011369539424777031\n",
      "Epoch: 18, Step: 190, Loss: 0.04211299866437912\n",
      "Epoch: 18, Step: 200, Loss: 0.06358416378498077\n",
      "Epoch: 18, Step: 210, Loss: 0.056467827409505844\n",
      "Epoch: 18, Step: 220, Loss: 0.003086454700678587\n",
      "Epoch: 18, Step: 230, Loss: 0.03599557280540466\n",
      "Epoch: 18, Step: 240, Loss: 0.029736828058958054\n",
      "Epoch: 18, Step: 250, Loss: 0.03769231587648392\n",
      "Epoch: 18, Step: 260, Loss: 0.025561681017279625\n",
      "Epoch: 18, Step: 270, Loss: 0.048815809190273285\n",
      "Epoch: 18, Step: 280, Loss: 0.007857688702642918\n",
      "Epoch: 18, Step: 290, Loss: 0.008692702278494835\n",
      "Epoch: 18, Step: 300, Loss: 0.0014001474482938647\n",
      "Epoch: 19, Step: 0, Loss: 0.055414095520973206\n",
      "Epoch: 19, Step: 10, Loss: 0.031840208917856216\n",
      "Epoch: 19, Step: 20, Loss: 0.004595075733959675\n",
      "Epoch: 19, Step: 30, Loss: 0.004983559250831604\n",
      "Epoch: 19, Step: 40, Loss: 0.007960977964103222\n",
      "Epoch: 19, Step: 50, Loss: 0.08482086658477783\n",
      "Epoch: 19, Step: 60, Loss: 0.011939313262701035\n",
      "Epoch: 19, Step: 70, Loss: 0.006368900649249554\n",
      "Epoch: 19, Step: 80, Loss: 0.006665376480668783\n",
      "Epoch: 19, Step: 90, Loss: 0.029926840215921402\n",
      "Epoch: 19, Step: 100, Loss: 0.1011451706290245\n",
      "Epoch: 19, Step: 110, Loss: 0.05661136656999588\n",
      "Epoch: 19, Step: 120, Loss: 0.005382557865232229\n",
      "Epoch: 19, Step: 130, Loss: 0.007043918594717979\n",
      "Epoch: 19, Step: 140, Loss: 0.032313186675310135\n",
      "Epoch: 19, Step: 150, Loss: 0.02548189088702202\n",
      "Epoch: 19, Step: 160, Loss: 0.021351587027311325\n",
      "Epoch: 19, Step: 170, Loss: 0.03387321159243584\n",
      "Epoch: 19, Step: 180, Loss: 0.02251894399523735\n",
      "Epoch: 19, Step: 190, Loss: 0.03464367985725403\n",
      "Epoch: 19, Step: 200, Loss: 0.005751198157668114\n",
      "Epoch: 19, Step: 210, Loss: 0.004602247849106789\n",
      "Epoch: 19, Step: 220, Loss: 0.007336456328630447\n",
      "Epoch: 19, Step: 230, Loss: 0.006728800013661385\n",
      "Epoch: 19, Step: 240, Loss: 0.004525212105363607\n",
      "Epoch: 19, Step: 250, Loss: 0.04582720994949341\n",
      "Epoch: 19, Step: 260, Loss: 0.009082252159714699\n",
      "Epoch: 19, Step: 270, Loss: 0.00793415680527687\n",
      "Epoch: 19, Step: 280, Loss: 0.03257507085800171\n",
      "Epoch: 19, Step: 290, Loss: 0.002766749355942011\n",
      "Epoch: 19, Step: 300, Loss: 0.0025530236307531595\n",
      "Epoch: 20, Step: 0, Loss: 0.037273816764354706\n",
      "Epoch: 20, Step: 10, Loss: 0.005607780069112778\n",
      "Epoch: 20, Step: 20, Loss: 0.0032314530108124018\n",
      "Epoch: 20, Step: 30, Loss: 0.09497454017400742\n",
      "Epoch: 20, Step: 40, Loss: 0.004474429413676262\n",
      "Epoch: 20, Step: 50, Loss: 0.003635335247963667\n",
      "Epoch: 20, Step: 60, Loss: 0.006011727266013622\n",
      "Epoch: 20, Step: 70, Loss: 0.004352255258709192\n",
      "Epoch: 20, Step: 80, Loss: 0.0041059586219489574\n",
      "Epoch: 20, Step: 90, Loss: 0.04165160283446312\n",
      "Epoch: 20, Step: 100, Loss: 0.01851343736052513\n",
      "Epoch: 20, Step: 110, Loss: 0.07812492549419403\n",
      "Epoch: 20, Step: 120, Loss: 0.05879437178373337\n",
      "Epoch: 20, Step: 130, Loss: 0.031418852508068085\n",
      "Epoch: 20, Step: 140, Loss: 0.15196767449378967\n",
      "Epoch: 20, Step: 150, Loss: 0.005722676403820515\n",
      "Epoch: 20, Step: 160, Loss: 0.034117963165044785\n",
      "Epoch: 20, Step: 170, Loss: 0.007687629200518131\n",
      "Epoch: 20, Step: 180, Loss: 0.009311165660619736\n",
      "Epoch: 20, Step: 190, Loss: 0.024403652176260948\n",
      "Epoch: 20, Step: 200, Loss: 0.08618297427892685\n",
      "Epoch: 20, Step: 210, Loss: 0.007771555334329605\n",
      "Epoch: 20, Step: 220, Loss: 0.05042078346014023\n",
      "Epoch: 20, Step: 230, Loss: 0.035635411739349365\n",
      "Epoch: 20, Step: 240, Loss: 0.039717767387628555\n",
      "Epoch: 20, Step: 250, Loss: 0.0026180753484368324\n",
      "Epoch: 20, Step: 260, Loss: 0.006726360414177179\n",
      "Epoch: 20, Step: 270, Loss: 0.039229974150657654\n",
      "Epoch: 20, Step: 280, Loss: 0.004839232191443443\n",
      "Epoch: 20, Step: 290, Loss: 0.005496581085026264\n",
      "Epoch: 20, Step: 300, Loss: 0.8865339159965515\n",
      "Epoch: 21, Step: 0, Loss: 0.0689309760928154\n",
      "Epoch: 21, Step: 10, Loss: 0.19448670744895935\n",
      "Epoch: 21, Step: 20, Loss: 0.035973675549030304\n",
      "Epoch: 21, Step: 30, Loss: 0.021653566509485245\n",
      "Epoch: 21, Step: 40, Loss: 0.00985758751630783\n",
      "Epoch: 21, Step: 50, Loss: 0.007503652013838291\n",
      "Epoch: 21, Step: 60, Loss: 0.014767846092581749\n",
      "Epoch: 21, Step: 70, Loss: 0.010970492847263813\n",
      "Epoch: 21, Step: 80, Loss: 0.045062124729156494\n",
      "Epoch: 21, Step: 90, Loss: 0.06379394978284836\n",
      "Epoch: 21, Step: 100, Loss: 0.01007323618978262\n",
      "Epoch: 21, Step: 110, Loss: 0.026866795495152473\n",
      "Epoch: 21, Step: 120, Loss: 0.008423876017332077\n",
      "Epoch: 21, Step: 130, Loss: 0.02365807443857193\n",
      "Epoch: 21, Step: 140, Loss: 0.00532872136682272\n",
      "Epoch: 21, Step: 150, Loss: 0.04099495708942413\n",
      "Epoch: 21, Step: 160, Loss: 0.008973530493676662\n",
      "Epoch: 21, Step: 170, Loss: 0.052045367658138275\n",
      "Epoch: 21, Step: 180, Loss: 0.0835283175110817\n",
      "Epoch: 21, Step: 190, Loss: 0.011595252901315689\n",
      "Epoch: 21, Step: 200, Loss: 0.06254033744335175\n",
      "Epoch: 21, Step: 210, Loss: 0.03802637383341789\n",
      "Epoch: 21, Step: 220, Loss: 0.03755581006407738\n",
      "Epoch: 21, Step: 230, Loss: 0.04743572324514389\n",
      "Epoch: 21, Step: 240, Loss: 0.03669975697994232\n",
      "Epoch: 21, Step: 250, Loss: 0.08146065473556519\n",
      "Epoch: 21, Step: 260, Loss: 0.032575853168964386\n",
      "Epoch: 21, Step: 270, Loss: 0.008040065877139568\n",
      "Epoch: 21, Step: 280, Loss: 0.004033688455820084\n",
      "Epoch: 21, Step: 290, Loss: 0.00250473665073514\n",
      "Epoch: 21, Step: 300, Loss: 6.995008152443916e-05\n",
      "Epoch: 22, Step: 0, Loss: 0.039090853184461594\n",
      "Epoch: 22, Step: 10, Loss: 0.003665420925244689\n",
      "Epoch: 22, Step: 20, Loss: 0.043635886162519455\n",
      "Epoch: 22, Step: 30, Loss: 0.02957269549369812\n",
      "Epoch: 22, Step: 40, Loss: 0.004751652013510466\n",
      "Epoch: 22, Step: 50, Loss: 0.02450409159064293\n",
      "Epoch: 22, Step: 60, Loss: 0.008933375589549541\n",
      "Epoch: 22, Step: 70, Loss: 0.005147811025381088\n",
      "Epoch: 22, Step: 80, Loss: 0.04580084979534149\n",
      "Epoch: 22, Step: 90, Loss: 0.006236153654754162\n",
      "Epoch: 22, Step: 100, Loss: 0.06464213132858276\n",
      "Epoch: 22, Step: 110, Loss: 0.015718810260295868\n",
      "Epoch: 22, Step: 120, Loss: 0.03606436029076576\n",
      "Epoch: 22, Step: 130, Loss: 0.006859221495687962\n",
      "Epoch: 22, Step: 140, Loss: 0.00997084379196167\n",
      "Epoch: 22, Step: 150, Loss: 0.030550599098205566\n",
      "Epoch: 22, Step: 160, Loss: 0.03606044873595238\n",
      "Epoch: 22, Step: 170, Loss: 0.0031709736213088036\n",
      "Epoch: 22, Step: 180, Loss: 0.03475430980324745\n",
      "Epoch: 22, Step: 190, Loss: 0.005521739367395639\n",
      "Epoch: 22, Step: 200, Loss: 0.003060908056795597\n",
      "Epoch: 22, Step: 210, Loss: 0.006585749797523022\n",
      "Epoch: 22, Step: 220, Loss: 0.0051533072255551815\n",
      "Epoch: 22, Step: 230, Loss: 0.04748730733990669\n",
      "Epoch: 22, Step: 240, Loss: 0.0036312430165708065\n",
      "Epoch: 22, Step: 250, Loss: 0.004917841404676437\n",
      "Epoch: 22, Step: 260, Loss: 0.06796418875455856\n",
      "Epoch: 22, Step: 270, Loss: 0.06329154968261719\n",
      "Epoch: 22, Step: 280, Loss: 0.053802620619535446\n",
      "Epoch: 22, Step: 290, Loss: 0.010719580575823784\n",
      "Epoch: 22, Step: 300, Loss: 0.0020187192130833864\n",
      "Epoch: 23, Step: 0, Loss: 0.01094131637364626\n",
      "Epoch: 23, Step: 10, Loss: 0.003049498423933983\n",
      "Epoch: 23, Step: 20, Loss: 0.01128716766834259\n",
      "Epoch: 23, Step: 30, Loss: 0.00448827026411891\n",
      "Epoch: 23, Step: 40, Loss: 0.002688061911612749\n",
      "Epoch: 23, Step: 50, Loss: 0.00957650039345026\n",
      "Epoch: 23, Step: 60, Loss: 0.1242561936378479\n",
      "Epoch: 23, Step: 70, Loss: 0.032564230263233185\n",
      "Epoch: 23, Step: 80, Loss: 0.005481581669300795\n",
      "Epoch: 23, Step: 90, Loss: 0.0071337572298944\n",
      "Epoch: 23, Step: 100, Loss: 0.032615482807159424\n",
      "Epoch: 23, Step: 110, Loss: 0.002511964412406087\n",
      "Epoch: 23, Step: 120, Loss: 0.006919276900589466\n",
      "Epoch: 23, Step: 130, Loss: 0.005076949950307608\n",
      "Epoch: 23, Step: 140, Loss: 0.03239334374666214\n",
      "Epoch: 23, Step: 150, Loss: 0.03730136528611183\n",
      "Epoch: 23, Step: 160, Loss: 0.04196567088365555\n",
      "Epoch: 23, Step: 170, Loss: 0.056661203503608704\n",
      "Epoch: 23, Step: 180, Loss: 0.0024504722096025944\n",
      "Epoch: 23, Step: 190, Loss: 0.06504833698272705\n",
      "Epoch: 23, Step: 200, Loss: 0.0379754975438118\n",
      "Epoch: 23, Step: 210, Loss: 0.04771112650632858\n",
      "Epoch: 23, Step: 220, Loss: 0.009329590946435928\n",
      "Epoch: 23, Step: 230, Loss: 0.0062471190467476845\n",
      "Epoch: 23, Step: 240, Loss: 0.0048758648335933685\n",
      "Epoch: 23, Step: 250, Loss: 0.004703282378613949\n",
      "Epoch: 23, Step: 260, Loss: 0.010600035078823566\n",
      "Epoch: 23, Step: 270, Loss: 0.033868208527565\n",
      "Epoch: 23, Step: 280, Loss: 0.003105000127106905\n",
      "Epoch: 23, Step: 290, Loss: 0.003297459799796343\n",
      "Epoch: 23, Step: 300, Loss: 0.010621353052556515\n",
      "Epoch: 24, Step: 0, Loss: 0.03916706517338753\n",
      "Epoch: 24, Step: 10, Loss: 0.004242704715579748\n",
      "Epoch: 24, Step: 20, Loss: 0.04309367761015892\n",
      "Epoch: 24, Step: 30, Loss: 0.004636479541659355\n",
      "Epoch: 24, Step: 40, Loss: 0.013083202764391899\n",
      "Epoch: 24, Step: 50, Loss: 0.028914261609315872\n",
      "Epoch: 24, Step: 60, Loss: 0.04087528958916664\n",
      "Epoch: 24, Step: 70, Loss: 0.003575208829715848\n",
      "Epoch: 24, Step: 80, Loss: 0.0064175548031926155\n",
      "Epoch: 24, Step: 90, Loss: 0.0038848500698804855\n",
      "Epoch: 24, Step: 100, Loss: 0.025953926146030426\n",
      "Epoch: 24, Step: 110, Loss: 0.0034128581173717976\n",
      "Epoch: 24, Step: 120, Loss: 0.004811552353203297\n",
      "Epoch: 24, Step: 130, Loss: 0.00456548947840929\n",
      "Epoch: 24, Step: 140, Loss: 0.005521267652511597\n",
      "Epoch: 24, Step: 150, Loss: 0.020608799532055855\n",
      "Epoch: 24, Step: 160, Loss: 0.012906016781926155\n",
      "Epoch: 24, Step: 170, Loss: 0.005976091139018536\n",
      "Epoch: 24, Step: 180, Loss: 0.003384900977835059\n",
      "Epoch: 24, Step: 190, Loss: 0.005802560597658157\n",
      "Epoch: 24, Step: 200, Loss: 0.0028483206406235695\n",
      "Epoch: 24, Step: 210, Loss: 0.0032879156060516834\n",
      "Epoch: 24, Step: 220, Loss: 0.00784199871122837\n",
      "Epoch: 24, Step: 230, Loss: 0.033559560775756836\n",
      "Epoch: 24, Step: 240, Loss: 0.030202709138393402\n",
      "Epoch: 24, Step: 250, Loss: 0.0029444280080497265\n",
      "Epoch: 24, Step: 260, Loss: 0.031149033457040787\n",
      "Epoch: 24, Step: 270, Loss: 0.003129011020064354\n",
      "Epoch: 24, Step: 280, Loss: 0.03212442994117737\n",
      "Epoch: 24, Step: 290, Loss: 0.03297381103038788\n",
      "Epoch: 24, Step: 300, Loss: 0.0008192479726858437\n",
      "Epoch: 25, Step: 0, Loss: 0.0021147741936147213\n",
      "Epoch: 25, Step: 10, Loss: 0.002551110228523612\n",
      "Epoch: 25, Step: 20, Loss: 0.004292570985853672\n",
      "Epoch: 25, Step: 30, Loss: 0.00517081655561924\n",
      "Epoch: 25, Step: 40, Loss: 0.01998368464410305\n",
      "Epoch: 25, Step: 50, Loss: 0.010966505855321884\n",
      "Epoch: 25, Step: 60, Loss: 0.0036208031233400106\n",
      "Epoch: 25, Step: 70, Loss: 0.017514804378151894\n",
      "Epoch: 25, Step: 80, Loss: 0.04692339524626732\n",
      "Epoch: 25, Step: 90, Loss: 0.003751331940293312\n",
      "Epoch: 25, Step: 100, Loss: 0.0025279601104557514\n",
      "Epoch: 25, Step: 110, Loss: 0.00234867911785841\n",
      "Epoch: 25, Step: 120, Loss: 0.009868808090686798\n",
      "Epoch: 25, Step: 130, Loss: 0.00545981340110302\n",
      "Epoch: 25, Step: 140, Loss: 0.02342045120894909\n",
      "Epoch: 25, Step: 150, Loss: 0.0484166294336319\n",
      "Epoch: 25, Step: 160, Loss: 0.0018204511143267155\n",
      "Epoch: 25, Step: 170, Loss: 0.04175248742103577\n",
      "Epoch: 25, Step: 180, Loss: 0.030914142727851868\n",
      "Epoch: 25, Step: 190, Loss: 0.004605224821716547\n",
      "Epoch: 25, Step: 200, Loss: 0.0027652131393551826\n",
      "Epoch: 25, Step: 210, Loss: 0.002287966897711158\n",
      "Epoch: 25, Step: 220, Loss: 0.07823018729686737\n",
      "Epoch: 25, Step: 230, Loss: 0.01913129910826683\n",
      "Epoch: 25, Step: 240, Loss: 0.002213812433183193\n",
      "Epoch: 25, Step: 250, Loss: 0.015841159969568253\n",
      "Epoch: 25, Step: 260, Loss: 0.003452163189649582\n",
      "Epoch: 25, Step: 270, Loss: 0.031601354479789734\n",
      "Epoch: 25, Step: 280, Loss: 0.03027275763452053\n",
      "Epoch: 25, Step: 290, Loss: 0.002740629715844989\n",
      "Epoch: 25, Step: 300, Loss: 0.008725251071155071\n",
      "Epoch: 26, Step: 0, Loss: 0.004169219173491001\n",
      "Epoch: 26, Step: 10, Loss: 0.014546881429851055\n",
      "Epoch: 26, Step: 20, Loss: 0.01584761030972004\n",
      "Epoch: 26, Step: 30, Loss: 0.027273397892713547\n",
      "Epoch: 26, Step: 40, Loss: 0.002044254681095481\n",
      "Epoch: 26, Step: 50, Loss: 0.054743774235248566\n",
      "Epoch: 26, Step: 60, Loss: 0.0047637722454965115\n",
      "Epoch: 26, Step: 70, Loss: 0.005581853445619345\n",
      "Epoch: 26, Step: 80, Loss: 0.03370252624154091\n",
      "Epoch: 26, Step: 90, Loss: 0.03536614775657654\n",
      "Epoch: 26, Step: 100, Loss: 0.0032053841277956963\n",
      "Epoch: 26, Step: 110, Loss: 0.0034665793646126986\n",
      "Epoch: 26, Step: 120, Loss: 0.004246930591762066\n",
      "Epoch: 26, Step: 130, Loss: 0.05168355256319046\n",
      "Epoch: 26, Step: 140, Loss: 0.006709793582558632\n",
      "Epoch: 26, Step: 150, Loss: 0.025002509355545044\n",
      "Epoch: 26, Step: 160, Loss: 0.004721875302493572\n",
      "Epoch: 26, Step: 170, Loss: 0.01938328519463539\n",
      "Epoch: 26, Step: 180, Loss: 0.0025060493499040604\n",
      "Epoch: 26, Step: 190, Loss: 0.0027077216655015945\n",
      "Epoch: 26, Step: 200, Loss: 0.06491106748580933\n",
      "Epoch: 26, Step: 210, Loss: 0.0033965702168643475\n",
      "Epoch: 26, Step: 220, Loss: 0.021320847794413567\n",
      "Epoch: 26, Step: 230, Loss: 0.03884831815958023\n",
      "Epoch: 26, Step: 240, Loss: 0.04709735885262489\n",
      "Epoch: 26, Step: 250, Loss: 0.003931730054318905\n",
      "Epoch: 26, Step: 260, Loss: 0.0032803528010845184\n",
      "Epoch: 26, Step: 270, Loss: 0.023684285581111908\n",
      "Epoch: 26, Step: 280, Loss: 0.002954457886517048\n",
      "Epoch: 26, Step: 290, Loss: 0.005724403075873852\n",
      "Epoch: 26, Step: 300, Loss: 7.038325566099957e-05\n",
      "Epoch: 27, Step: 0, Loss: 0.002396618016064167\n",
      "Epoch: 27, Step: 10, Loss: 0.004712631925940514\n",
      "Epoch: 27, Step: 20, Loss: 0.0017721407348290086\n",
      "Epoch: 27, Step: 30, Loss: 0.0033964463509619236\n",
      "Epoch: 27, Step: 40, Loss: 0.024816211313009262\n",
      "Epoch: 27, Step: 50, Loss: 0.0021891596261411905\n",
      "Epoch: 27, Step: 60, Loss: 0.0752866342663765\n",
      "Epoch: 27, Step: 70, Loss: 0.03153407573699951\n",
      "Epoch: 27, Step: 80, Loss: 0.0031012543477118015\n",
      "Epoch: 27, Step: 90, Loss: 0.001905321259982884\n",
      "Epoch: 27, Step: 100, Loss: 0.03641211986541748\n",
      "Epoch: 27, Step: 110, Loss: 0.004481449257582426\n",
      "Epoch: 27, Step: 120, Loss: 0.003967350348830223\n",
      "Epoch: 27, Step: 130, Loss: 0.008034802973270416\n",
      "Epoch: 27, Step: 140, Loss: 0.0034579443745315075\n",
      "Epoch: 27, Step: 150, Loss: 0.004202313721179962\n",
      "Epoch: 27, Step: 160, Loss: 0.0027130036614835262\n",
      "Epoch: 27, Step: 170, Loss: 0.039282459765672684\n",
      "Epoch: 27, Step: 180, Loss: 0.03129831328988075\n",
      "Epoch: 27, Step: 190, Loss: 0.003527919529005885\n",
      "Epoch: 27, Step: 200, Loss: 0.0038367833476513624\n",
      "Epoch: 27, Step: 210, Loss: 0.03167280554771423\n",
      "Epoch: 27, Step: 220, Loss: 0.0038393596187233925\n",
      "Epoch: 27, Step: 230, Loss: 0.025399813428521156\n",
      "Epoch: 27, Step: 240, Loss: 0.003601361997425556\n",
      "Epoch: 27, Step: 250, Loss: 0.006383693777024746\n",
      "Epoch: 27, Step: 260, Loss: 0.06466872990131378\n",
      "Epoch: 27, Step: 270, Loss: 0.006472461391240358\n",
      "Epoch: 27, Step: 280, Loss: 0.003490219358354807\n",
      "Epoch: 27, Step: 290, Loss: 0.007946203462779522\n",
      "Epoch: 27, Step: 300, Loss: 0.005301112774759531\n",
      "Epoch: 28, Step: 0, Loss: 0.011384284123778343\n",
      "Epoch: 28, Step: 10, Loss: 0.0191635861992836\n",
      "Epoch: 28, Step: 20, Loss: 0.034332357347011566\n",
      "Epoch: 28, Step: 30, Loss: 0.0044497959315776825\n",
      "Epoch: 28, Step: 40, Loss: 0.00392570998519659\n",
      "Epoch: 28, Step: 50, Loss: 0.030688464641571045\n",
      "Epoch: 28, Step: 60, Loss: 0.03288429230451584\n",
      "Epoch: 28, Step: 70, Loss: 0.01628168299794197\n",
      "Epoch: 28, Step: 80, Loss: 0.029803814366459846\n",
      "Epoch: 28, Step: 90, Loss: 0.006714579649269581\n",
      "Epoch: 28, Step: 100, Loss: 0.03719119727611542\n",
      "Epoch: 28, Step: 110, Loss: 0.010375197976827621\n",
      "Epoch: 28, Step: 120, Loss: 0.004759685602039099\n",
      "Epoch: 28, Step: 130, Loss: 0.0036227190867066383\n",
      "Epoch: 28, Step: 140, Loss: 0.010236550122499466\n",
      "Epoch: 28, Step: 150, Loss: 0.001876537804491818\n",
      "Epoch: 28, Step: 160, Loss: 0.0025683031417429447\n",
      "Epoch: 28, Step: 170, Loss: 0.0021952935494482517\n",
      "Epoch: 28, Step: 180, Loss: 0.002775020431727171\n",
      "Epoch: 28, Step: 190, Loss: 0.004592780023813248\n",
      "Epoch: 28, Step: 200, Loss: 0.037171706557273865\n",
      "Epoch: 28, Step: 210, Loss: 0.005225616507232189\n",
      "Epoch: 28, Step: 220, Loss: 0.023951269686222076\n",
      "Epoch: 28, Step: 230, Loss: 0.0028287337627261877\n",
      "Epoch: 28, Step: 240, Loss: 0.00447519775480032\n",
      "Epoch: 28, Step: 250, Loss: 0.004166934639215469\n",
      "Epoch: 28, Step: 260, Loss: 0.017669757828116417\n",
      "Epoch: 28, Step: 270, Loss: 0.03035832569003105\n",
      "Epoch: 28, Step: 280, Loss: 0.029356082901358604\n",
      "Epoch: 28, Step: 290, Loss: 0.0045487405732274055\n",
      "Epoch: 28, Step: 300, Loss: 0.006189593113958836\n",
      "Epoch: 29, Step: 0, Loss: 0.00724016223102808\n",
      "Epoch: 29, Step: 10, Loss: 0.003366566263139248\n",
      "Epoch: 29, Step: 20, Loss: 0.004729838576167822\n",
      "Epoch: 29, Step: 30, Loss: 0.003729112446308136\n",
      "Epoch: 29, Step: 40, Loss: 0.0023456010967493057\n",
      "Epoch: 29, Step: 50, Loss: 0.002441275864839554\n",
      "Epoch: 29, Step: 60, Loss: 0.005907413549721241\n",
      "Epoch: 29, Step: 70, Loss: 0.005207785405218601\n",
      "Epoch: 29, Step: 80, Loss: 0.03743225336074829\n",
      "Epoch: 29, Step: 90, Loss: 0.02911396697163582\n",
      "Epoch: 29, Step: 100, Loss: 0.00420438963919878\n",
      "Epoch: 29, Step: 110, Loss: 0.025318756699562073\n",
      "Epoch: 29, Step: 120, Loss: 0.002906600246205926\n",
      "Epoch: 29, Step: 130, Loss: 0.008744778111577034\n",
      "Epoch: 29, Step: 140, Loss: 0.005828806199133396\n",
      "Epoch: 29, Step: 150, Loss: 0.03437507897615433\n",
      "Epoch: 29, Step: 160, Loss: 0.0027309306897222996\n",
      "Epoch: 29, Step: 170, Loss: 0.001601849915459752\n",
      "Epoch: 29, Step: 180, Loss: 0.007723196875303984\n",
      "Epoch: 29, Step: 190, Loss: 0.032580725848674774\n",
      "Epoch: 29, Step: 200, Loss: 0.03484354540705681\n",
      "Epoch: 29, Step: 210, Loss: 0.0041043078526854515\n",
      "Epoch: 29, Step: 220, Loss: 0.025531316176056862\n",
      "Epoch: 29, Step: 230, Loss: 0.04927598312497139\n",
      "Epoch: 29, Step: 240, Loss: 0.0032360716722905636\n",
      "Epoch: 29, Step: 250, Loss: 0.03862747550010681\n",
      "Epoch: 29, Step: 260, Loss: 0.009860163554549217\n",
      "Epoch: 29, Step: 270, Loss: 0.005640746094286442\n",
      "Epoch: 29, Step: 280, Loss: 0.0035139257088303566\n",
      "Epoch: 29, Step: 290, Loss: 0.004994983784854412\n",
      "Epoch: 29, Step: 300, Loss: 0.005573991686105728\n",
      "Epoch: 30, Step: 0, Loss: 0.0029798620380461216\n",
      "Epoch: 30, Step: 10, Loss: 0.01385774277150631\n",
      "Epoch: 30, Step: 20, Loss: 0.0216740183532238\n",
      "Epoch: 30, Step: 30, Loss: 0.041151855140924454\n",
      "Epoch: 30, Step: 40, Loss: 0.003683744464069605\n",
      "Epoch: 30, Step: 50, Loss: 0.006975595839321613\n",
      "Epoch: 30, Step: 60, Loss: 0.053769491612911224\n",
      "Epoch: 30, Step: 70, Loss: 0.0034649758599698544\n",
      "Epoch: 30, Step: 80, Loss: 0.032296791672706604\n",
      "Epoch: 30, Step: 90, Loss: 0.038861095905303955\n",
      "Epoch: 30, Step: 100, Loss: 0.006210701540112495\n",
      "Epoch: 30, Step: 110, Loss: 0.0316200889647007\n",
      "Epoch: 30, Step: 120, Loss: 0.0015069814398884773\n",
      "Epoch: 30, Step: 130, Loss: 0.03128677234053612\n",
      "Epoch: 30, Step: 140, Loss: 0.0031082904897630215\n",
      "Epoch: 30, Step: 150, Loss: 0.01975754275918007\n",
      "Epoch: 30, Step: 160, Loss: 0.005765736568719149\n",
      "Epoch: 30, Step: 170, Loss: 0.0015625074738636613\n",
      "Epoch: 30, Step: 180, Loss: 0.0055068559013307095\n",
      "Epoch: 30, Step: 190, Loss: 0.0014892127364873886\n",
      "Epoch: 30, Step: 200, Loss: 0.0029352218843996525\n",
      "Epoch: 30, Step: 210, Loss: 0.007902394980192184\n",
      "Epoch: 30, Step: 220, Loss: 0.004590247292071581\n",
      "Epoch: 30, Step: 230, Loss: 0.0014581921277567744\n",
      "Epoch: 30, Step: 240, Loss: 0.04215686023235321\n",
      "Epoch: 30, Step: 250, Loss: 0.03959895297884941\n",
      "Epoch: 30, Step: 260, Loss: 0.03828197345137596\n",
      "Epoch: 30, Step: 270, Loss: 0.003989999182522297\n",
      "Epoch: 30, Step: 280, Loss: 0.06875644624233246\n",
      "Epoch: 30, Step: 290, Loss: 0.017384687438607216\n",
      "Epoch: 30, Step: 300, Loss: 0.029863735660910606\n",
      "Epoch: 31, Step: 0, Loss: 0.0016245247097685933\n",
      "Epoch: 31, Step: 10, Loss: 0.03979136049747467\n",
      "Epoch: 31, Step: 20, Loss: 0.0018073017708957195\n",
      "Epoch: 31, Step: 30, Loss: 0.03023587167263031\n",
      "Epoch: 31, Step: 40, Loss: 0.03355542942881584\n",
      "Epoch: 31, Step: 50, Loss: 0.002011094707995653\n",
      "Epoch: 31, Step: 60, Loss: 0.0017208425560966134\n",
      "Epoch: 31, Step: 70, Loss: 0.005427556578069925\n",
      "Epoch: 31, Step: 80, Loss: 0.0063761393539607525\n",
      "Epoch: 31, Step: 90, Loss: 0.0019828120712190866\n",
      "Epoch: 31, Step: 100, Loss: 0.03302425146102905\n",
      "Epoch: 31, Step: 110, Loss: 0.003173498436808586\n",
      "Epoch: 31, Step: 120, Loss: 0.0026800648774951696\n",
      "Epoch: 31, Step: 130, Loss: 0.004962710663676262\n",
      "Epoch: 31, Step: 140, Loss: 0.061125267297029495\n",
      "Epoch: 31, Step: 150, Loss: 0.030966853722929955\n",
      "Epoch: 31, Step: 160, Loss: 0.002390549285337329\n",
      "Epoch: 31, Step: 170, Loss: 0.04221886396408081\n",
      "Epoch: 31, Step: 180, Loss: 0.002168826526030898\n",
      "Epoch: 31, Step: 190, Loss: 0.1240227222442627\n",
      "Epoch: 31, Step: 200, Loss: 0.003470195923000574\n",
      "Epoch: 31, Step: 210, Loss: 0.012728400528430939\n",
      "Epoch: 31, Step: 220, Loss: 0.05752325803041458\n",
      "Epoch: 31, Step: 230, Loss: 0.004154815338551998\n",
      "Epoch: 31, Step: 240, Loss: 0.003448218572884798\n",
      "Epoch: 31, Step: 250, Loss: 0.0020009614527225494\n",
      "Epoch: 31, Step: 260, Loss: 0.003169956151396036\n",
      "Epoch: 31, Step: 270, Loss: 0.0022514676675200462\n",
      "Epoch: 31, Step: 280, Loss: 0.003081988077610731\n",
      "Epoch: 31, Step: 290, Loss: 0.03212890401482582\n",
      "Epoch: 31, Step: 300, Loss: 0.014638704247772694\n",
      "Epoch: 32, Step: 0, Loss: 0.002503485418856144\n",
      "Epoch: 32, Step: 10, Loss: 0.0025232182815670967\n",
      "Epoch: 32, Step: 20, Loss: 0.02404017001390457\n",
      "Epoch: 32, Step: 30, Loss: 0.02301115356385708\n",
      "Epoch: 32, Step: 40, Loss: 0.0023771715350449085\n",
      "Epoch: 32, Step: 50, Loss: 0.003283555619418621\n",
      "Epoch: 32, Step: 60, Loss: 0.006221084389835596\n",
      "Epoch: 32, Step: 70, Loss: 0.002330880146473646\n",
      "Epoch: 32, Step: 80, Loss: 0.002208658028393984\n",
      "Epoch: 32, Step: 90, Loss: 0.00906141847372055\n",
      "Epoch: 32, Step: 100, Loss: 0.0026880998630076647\n",
      "Epoch: 32, Step: 110, Loss: 0.03766562417149544\n",
      "Epoch: 32, Step: 120, Loss: 0.002204448450356722\n",
      "Epoch: 32, Step: 130, Loss: 0.001662716967985034\n",
      "Epoch: 32, Step: 140, Loss: 0.003120073350146413\n",
      "Epoch: 32, Step: 150, Loss: 0.002508892212063074\n",
      "Epoch: 32, Step: 160, Loss: 0.02267388068139553\n",
      "Epoch: 32, Step: 170, Loss: 0.003261060919612646\n",
      "Epoch: 32, Step: 180, Loss: 0.0027216640301048756\n",
      "Epoch: 32, Step: 190, Loss: 0.03209611773490906\n",
      "Epoch: 32, Step: 200, Loss: 0.06331557035446167\n",
      "Epoch: 32, Step: 210, Loss: 0.0016285069286823273\n",
      "Epoch: 32, Step: 220, Loss: 0.05774269625544548\n",
      "Epoch: 32, Step: 230, Loss: 0.054846830666065216\n",
      "Epoch: 32, Step: 240, Loss: 0.00395077932626009\n",
      "Epoch: 32, Step: 250, Loss: 0.03504262864589691\n",
      "Epoch: 32, Step: 260, Loss: 0.0031827744096517563\n",
      "Epoch: 32, Step: 270, Loss: 0.003523891093209386\n",
      "Epoch: 32, Step: 280, Loss: 0.030615488067269325\n",
      "Epoch: 32, Step: 290, Loss: 0.17080669105052948\n",
      "Epoch: 32, Step: 300, Loss: 7.668314538022969e-06\n",
      "Epoch: 33, Step: 0, Loss: 0.03978586196899414\n",
      "Epoch: 33, Step: 10, Loss: 0.031896840780973434\n",
      "Epoch: 33, Step: 20, Loss: 0.011588199995458126\n",
      "Epoch: 33, Step: 30, Loss: 0.0026986703742295504\n",
      "Epoch: 33, Step: 40, Loss: 0.003572756890207529\n",
      "Epoch: 33, Step: 50, Loss: 0.004275319166481495\n",
      "Epoch: 33, Step: 60, Loss: 0.0027899453416466713\n",
      "Epoch: 33, Step: 70, Loss: 0.03117777407169342\n",
      "Epoch: 33, Step: 80, Loss: 0.01696019247174263\n",
      "Epoch: 33, Step: 90, Loss: 0.06673504412174225\n",
      "Epoch: 33, Step: 100, Loss: 0.011192960664629936\n",
      "Epoch: 33, Step: 110, Loss: 0.003666150849312544\n",
      "Epoch: 33, Step: 120, Loss: 0.006354730110615492\n",
      "Epoch: 33, Step: 130, Loss: 0.0022622866090387106\n",
      "Epoch: 33, Step: 140, Loss: 0.033688321709632874\n",
      "Epoch: 33, Step: 150, Loss: 0.02756662853062153\n",
      "Epoch: 33, Step: 160, Loss: 0.002094786148518324\n",
      "Epoch: 33, Step: 170, Loss: 0.01625537872314453\n",
      "Epoch: 33, Step: 180, Loss: 0.06666258722543716\n",
      "Epoch: 33, Step: 190, Loss: 0.011820869520306587\n",
      "Epoch: 33, Step: 200, Loss: 0.004211341962218285\n",
      "Epoch: 33, Step: 210, Loss: 0.012868986465036869\n",
      "Epoch: 33, Step: 220, Loss: 0.015387856401503086\n",
      "Epoch: 33, Step: 230, Loss: 0.0045952992513775826\n",
      "Epoch: 33, Step: 240, Loss: 0.002094632713124156\n",
      "Epoch: 33, Step: 250, Loss: 0.0019539049826562405\n",
      "Epoch: 33, Step: 260, Loss: 0.002085587941110134\n",
      "Epoch: 33, Step: 270, Loss: 0.015627993270754814\n",
      "Epoch: 33, Step: 280, Loss: 0.0036148023791611195\n",
      "Epoch: 33, Step: 290, Loss: 0.003944924101233482\n",
      "Epoch: 33, Step: 300, Loss: 0.0033661676570773125\n",
      "Epoch: 34, Step: 0, Loss: 0.0039028245955705643\n",
      "Epoch: 34, Step: 10, Loss: 0.0020462418906390667\n",
      "Epoch: 34, Step: 20, Loss: 0.012362186796963215\n",
      "Epoch: 34, Step: 30, Loss: 0.008423573337495327\n",
      "Epoch: 34, Step: 40, Loss: 0.03156976029276848\n",
      "Epoch: 34, Step: 50, Loss: 0.0016848433297127485\n",
      "Epoch: 34, Step: 60, Loss: 0.026255257427692413\n",
      "Epoch: 34, Step: 70, Loss: 0.036491021513938904\n",
      "Epoch: 34, Step: 80, Loss: 0.0026202555745840073\n",
      "Epoch: 34, Step: 90, Loss: 0.0020725924987345934\n",
      "Epoch: 34, Step: 100, Loss: 0.012241268530488014\n",
      "Epoch: 34, Step: 110, Loss: 0.008638983592391014\n",
      "Epoch: 34, Step: 120, Loss: 0.058884792029857635\n",
      "Epoch: 34, Step: 130, Loss: 0.0028513306751847267\n",
      "Epoch: 34, Step: 140, Loss: 0.009301259182393551\n",
      "Epoch: 34, Step: 150, Loss: 0.002620588755235076\n",
      "Epoch: 34, Step: 160, Loss: 0.0014436983037739992\n",
      "Epoch: 34, Step: 170, Loss: 0.002205776982009411\n",
      "Epoch: 34, Step: 180, Loss: 0.03560632839798927\n",
      "Epoch: 34, Step: 190, Loss: 0.003775899764150381\n",
      "Epoch: 34, Step: 200, Loss: 0.03428536280989647\n",
      "Epoch: 34, Step: 210, Loss: 0.03564212843775749\n",
      "Epoch: 34, Step: 220, Loss: 0.002521689748391509\n",
      "Epoch: 34, Step: 230, Loss: 0.0016472747083753347\n",
      "Epoch: 34, Step: 240, Loss: 0.00784376822412014\n",
      "Epoch: 34, Step: 250, Loss: 0.002697605639696121\n",
      "Epoch: 34, Step: 260, Loss: 0.05656500160694122\n",
      "Epoch: 34, Step: 270, Loss: 0.0036326907575130463\n",
      "Epoch: 34, Step: 280, Loss: 0.03340034559369087\n",
      "Epoch: 34, Step: 290, Loss: 0.0013862852938473225\n",
      "Epoch: 34, Step: 300, Loss: 0.003922695759683847\n",
      "Epoch: 35, Step: 0, Loss: 0.004265147261321545\n",
      "Epoch: 35, Step: 10, Loss: 0.0040099541656672955\n",
      "Epoch: 35, Step: 20, Loss: 0.06786610186100006\n",
      "Epoch: 35, Step: 30, Loss: 0.0018433812074363232\n",
      "Epoch: 35, Step: 40, Loss: 0.0021210776176303625\n",
      "Epoch: 35, Step: 50, Loss: 0.002114271279424429\n",
      "Epoch: 35, Step: 60, Loss: 0.06593768298625946\n",
      "Epoch: 35, Step: 70, Loss: 0.0021887975744903088\n",
      "Epoch: 35, Step: 80, Loss: 0.002107167849317193\n",
      "Epoch: 35, Step: 90, Loss: 0.028912467882037163\n",
      "Epoch: 35, Step: 100, Loss: 0.03477363660931587\n",
      "Epoch: 35, Step: 110, Loss: 0.02207382768392563\n",
      "Epoch: 35, Step: 120, Loss: 0.0018220565980300307\n",
      "Epoch: 35, Step: 130, Loss: 0.03612001985311508\n",
      "Epoch: 35, Step: 140, Loss: 0.0033864066936075687\n",
      "Epoch: 35, Step: 150, Loss: 0.0019032213604077697\n",
      "Epoch: 35, Step: 160, Loss: 0.0012591786216944456\n",
      "Epoch: 35, Step: 170, Loss: 0.0036245924420654774\n",
      "Epoch: 35, Step: 180, Loss: 0.001566361403092742\n",
      "Epoch: 35, Step: 190, Loss: 0.0018445742316544056\n",
      "Epoch: 35, Step: 200, Loss: 0.0019749044440686703\n",
      "Epoch: 35, Step: 210, Loss: 0.003883607452735305\n",
      "Epoch: 35, Step: 220, Loss: 0.0028233444318175316\n",
      "Epoch: 35, Step: 230, Loss: 0.0018385511357337236\n",
      "Epoch: 35, Step: 240, Loss: 0.002130869310349226\n",
      "Epoch: 35, Step: 250, Loss: 0.0016920806374400854\n",
      "Epoch: 35, Step: 260, Loss: 0.0029590909834951162\n",
      "Epoch: 35, Step: 270, Loss: 0.03436530753970146\n",
      "Epoch: 35, Step: 280, Loss: 0.004894658923149109\n",
      "Epoch: 35, Step: 290, Loss: 0.0013531337026506662\n",
      "Epoch: 35, Step: 300, Loss: 0.004982580430805683\n",
      "Epoch: 36, Step: 0, Loss: 0.0024464980233460665\n",
      "Epoch: 36, Step: 10, Loss: 0.028009340167045593\n",
      "Epoch: 36, Step: 20, Loss: 0.001302656950429082\n",
      "Epoch: 36, Step: 30, Loss: 0.00151536101475358\n",
      "Epoch: 36, Step: 40, Loss: 0.03234895318746567\n",
      "Epoch: 36, Step: 50, Loss: 0.0020020226947963238\n",
      "Epoch: 36, Step: 60, Loss: 0.00409918325021863\n",
      "Epoch: 36, Step: 70, Loss: 0.0023397356271743774\n",
      "Epoch: 36, Step: 80, Loss: 0.0026966524310410023\n",
      "Epoch: 36, Step: 90, Loss: 0.034121040254831314\n",
      "Epoch: 36, Step: 100, Loss: 0.034885551780462265\n",
      "Epoch: 36, Step: 110, Loss: 0.004593036137521267\n",
      "Epoch: 36, Step: 120, Loss: 0.001403537462465465\n",
      "Epoch: 36, Step: 130, Loss: 0.025747837498784065\n",
      "Epoch: 36, Step: 140, Loss: 0.023022586479783058\n",
      "Epoch: 36, Step: 150, Loss: 0.002130483277142048\n",
      "Epoch: 36, Step: 160, Loss: 0.005976426415145397\n",
      "Epoch: 36, Step: 170, Loss: 0.002652659546583891\n",
      "Epoch: 36, Step: 180, Loss: 0.0015780783724039793\n",
      "Epoch: 36, Step: 190, Loss: 0.0017711028922349215\n",
      "Epoch: 36, Step: 200, Loss: 0.0013299037236720324\n",
      "Epoch: 36, Step: 210, Loss: 0.0014610267244279385\n",
      "Epoch: 36, Step: 220, Loss: 0.0019031943520531058\n",
      "Epoch: 36, Step: 230, Loss: 0.002331245457753539\n",
      "Epoch: 36, Step: 240, Loss: 0.0020012895110994577\n",
      "Epoch: 36, Step: 250, Loss: 0.03032464161515236\n",
      "Epoch: 36, Step: 260, Loss: 0.007486042566597462\n",
      "Epoch: 36, Step: 270, Loss: 0.0038053153548389673\n",
      "Epoch: 36, Step: 280, Loss: 0.001908711390569806\n",
      "Epoch: 36, Step: 290, Loss: 0.0021994183771312237\n",
      "Epoch: 36, Step: 300, Loss: 4.635755976778455e-05\n",
      "Epoch: 37, Step: 0, Loss: 0.0021616911981254816\n",
      "Epoch: 37, Step: 10, Loss: 0.0019412615802139044\n",
      "Epoch: 37, Step: 20, Loss: 0.002459944924339652\n",
      "Epoch: 37, Step: 30, Loss: 0.0021799327805638313\n",
      "Epoch: 37, Step: 40, Loss: 0.0015661960933357477\n",
      "Epoch: 37, Step: 50, Loss: 0.001773683587089181\n",
      "Epoch: 37, Step: 60, Loss: 0.0019967099651694298\n",
      "Epoch: 37, Step: 70, Loss: 0.0042482404969632626\n",
      "Epoch: 37, Step: 80, Loss: 0.010762299410998821\n",
      "Epoch: 37, Step: 90, Loss: 0.003131181700155139\n",
      "Epoch: 37, Step: 100, Loss: 0.002363307401537895\n",
      "Epoch: 37, Step: 110, Loss: 0.0014353303704410791\n",
      "Epoch: 37, Step: 120, Loss: 0.001394658349454403\n",
      "Epoch: 37, Step: 130, Loss: 0.0017609833739697933\n",
      "Epoch: 37, Step: 140, Loss: 0.0018420754931867123\n",
      "Epoch: 37, Step: 150, Loss: 0.010286121629178524\n",
      "Epoch: 37, Step: 160, Loss: 0.001888028346002102\n",
      "Epoch: 37, Step: 170, Loss: 0.0019991938024759293\n",
      "Epoch: 37, Step: 180, Loss: 0.0022444864735007286\n",
      "Epoch: 37, Step: 190, Loss: 0.0029010700527578592\n",
      "Epoch: 37, Step: 200, Loss: 0.003330943640321493\n",
      "Epoch: 37, Step: 210, Loss: 0.009129934944212437\n",
      "Epoch: 37, Step: 220, Loss: 0.0014176942640915513\n",
      "Epoch: 37, Step: 230, Loss: 0.002907120157033205\n",
      "Epoch: 37, Step: 240, Loss: 0.026669448241591454\n",
      "Epoch: 37, Step: 250, Loss: 0.002118200296536088\n",
      "Epoch: 37, Step: 260, Loss: 0.031538546085357666\n",
      "Epoch: 37, Step: 270, Loss: 0.0024506717454642057\n",
      "Epoch: 37, Step: 280, Loss: 0.001823029830120504\n",
      "Epoch: 37, Step: 290, Loss: 0.0031427626963704824\n",
      "Epoch: 37, Step: 300, Loss: 0.0008500775438733399\n",
      "Epoch: 38, Step: 0, Loss: 0.0019891841802746058\n",
      "Epoch: 38, Step: 10, Loss: 0.030447401106357574\n",
      "Epoch: 38, Step: 20, Loss: 0.0012781679397448897\n",
      "Epoch: 38, Step: 30, Loss: 0.030437937006354332\n",
      "Epoch: 38, Step: 40, Loss: 0.00220898212864995\n",
      "Epoch: 38, Step: 50, Loss: 0.0007593493792228401\n",
      "Epoch: 38, Step: 60, Loss: 0.0014123466098681092\n",
      "Epoch: 38, Step: 70, Loss: 0.0022274749353528023\n",
      "Epoch: 38, Step: 80, Loss: 0.0016102532390505075\n",
      "Epoch: 38, Step: 90, Loss: 0.05483708530664444\n",
      "Epoch: 38, Step: 100, Loss: 0.0015235194005072117\n",
      "Epoch: 38, Step: 110, Loss: 0.033685021102428436\n",
      "Epoch: 38, Step: 120, Loss: 0.005678869783878326\n",
      "Epoch: 38, Step: 130, Loss: 0.061657316982746124\n",
      "Epoch: 38, Step: 140, Loss: 0.007807833608239889\n",
      "Epoch: 38, Step: 150, Loss: 0.03168565034866333\n",
      "Epoch: 38, Step: 160, Loss: 0.002025107853114605\n",
      "Epoch: 38, Step: 170, Loss: 0.0024828165769577026\n",
      "Epoch: 38, Step: 180, Loss: 0.03413171321153641\n",
      "Epoch: 38, Step: 190, Loss: 0.0015705390833318233\n",
      "Epoch: 38, Step: 200, Loss: 0.0017878771759569645\n",
      "Epoch: 38, Step: 210, Loss: 0.00854518637061119\n",
      "Epoch: 38, Step: 220, Loss: 0.002421939978376031\n",
      "Epoch: 38, Step: 230, Loss: 0.029583200812339783\n",
      "Epoch: 38, Step: 240, Loss: 0.003861234290525317\n",
      "Epoch: 38, Step: 250, Loss: 0.0012042245361953974\n",
      "Epoch: 38, Step: 260, Loss: 0.04240527004003525\n",
      "Epoch: 38, Step: 270, Loss: 0.0032277237623929977\n",
      "Epoch: 38, Step: 280, Loss: 0.003605359233915806\n",
      "Epoch: 38, Step: 290, Loss: 0.002411898924037814\n",
      "Epoch: 38, Step: 300, Loss: 0.0025226722937077284\n",
      "Epoch: 39, Step: 0, Loss: 0.0014864790719002485\n",
      "Epoch: 39, Step: 10, Loss: 0.0014669543597847223\n",
      "Epoch: 39, Step: 20, Loss: 0.0022028025705367327\n",
      "Epoch: 39, Step: 30, Loss: 0.0024638287723064423\n",
      "Epoch: 39, Step: 40, Loss: 0.004211634397506714\n",
      "Epoch: 39, Step: 50, Loss: 0.0021469106432050467\n",
      "Epoch: 39, Step: 60, Loss: 0.0027172095142304897\n",
      "Epoch: 39, Step: 70, Loss: 0.0018619255861267447\n",
      "Epoch: 39, Step: 80, Loss: 0.004053725861012936\n",
      "Epoch: 39, Step: 90, Loss: 0.03300159052014351\n",
      "Epoch: 39, Step: 100, Loss: 0.0037218905054032803\n",
      "Epoch: 39, Step: 110, Loss: 0.0012803657446056604\n",
      "Epoch: 39, Step: 120, Loss: 0.0019027895759791136\n",
      "Epoch: 39, Step: 130, Loss: 0.001990282442420721\n",
      "Epoch: 39, Step: 140, Loss: 0.0020144188310950994\n",
      "Epoch: 39, Step: 150, Loss: 0.0019509231206029654\n",
      "Epoch: 39, Step: 160, Loss: 0.0017514073988422751\n",
      "Epoch: 39, Step: 170, Loss: 0.002951878122985363\n",
      "Epoch: 39, Step: 180, Loss: 0.0019287110771983862\n",
      "Epoch: 39, Step: 190, Loss: 0.004410759080201387\n",
      "Epoch: 39, Step: 200, Loss: 0.011874380521476269\n",
      "Epoch: 39, Step: 210, Loss: 0.002200352493673563\n",
      "Epoch: 39, Step: 220, Loss: 0.003093091771006584\n",
      "Epoch: 39, Step: 230, Loss: 0.002718148287385702\n",
      "Epoch: 39, Step: 240, Loss: 0.0013907768297940493\n",
      "Epoch: 39, Step: 250, Loss: 0.0022460068576037884\n",
      "Epoch: 39, Step: 260, Loss: 0.0013881983468309045\n",
      "Epoch: 39, Step: 270, Loss: 0.0026852362789213657\n",
      "Epoch: 39, Step: 280, Loss: 0.002806018339470029\n",
      "Epoch: 39, Step: 290, Loss: 0.0024474146775901318\n",
      "Epoch: 39, Step: 300, Loss: 0.00026494316989555955\n",
      "Epoch: 40, Step: 0, Loss: 0.0021817567758262157\n",
      "Epoch: 40, Step: 10, Loss: 0.0015927786007523537\n",
      "Epoch: 40, Step: 20, Loss: 0.0018284376710653305\n",
      "Epoch: 40, Step: 30, Loss: 0.0016959368949756026\n",
      "Epoch: 40, Step: 40, Loss: 0.002197477500885725\n",
      "Epoch: 40, Step: 50, Loss: 0.002170044928789139\n",
      "Epoch: 40, Step: 60, Loss: 0.033966366201639175\n",
      "Epoch: 40, Step: 70, Loss: 0.0021834038197994232\n",
      "Epoch: 40, Step: 80, Loss: 0.0023017949424684048\n",
      "Epoch: 40, Step: 90, Loss: 0.005401514936238527\n",
      "Epoch: 40, Step: 100, Loss: 0.02274342253804207\n",
      "Epoch: 40, Step: 110, Loss: 0.0037965853698551655\n",
      "Epoch: 40, Step: 120, Loss: 0.0021022309083491564\n",
      "Epoch: 40, Step: 130, Loss: 0.002426513470709324\n",
      "Epoch: 40, Step: 140, Loss: 0.001641878392547369\n",
      "Epoch: 40, Step: 150, Loss: 0.0011647070059552789\n",
      "Epoch: 40, Step: 160, Loss: 0.0015421375865116715\n",
      "Epoch: 40, Step: 170, Loss: 0.02230006456375122\n",
      "Epoch: 40, Step: 180, Loss: 0.003088141791522503\n",
      "Epoch: 40, Step: 190, Loss: 0.0027458793483674526\n",
      "Epoch: 40, Step: 200, Loss: 0.0013182754628360271\n",
      "Epoch: 40, Step: 210, Loss: 0.001304542995058\n",
      "Epoch: 40, Step: 220, Loss: 0.03280196711421013\n",
      "Epoch: 40, Step: 230, Loss: 0.001725835376419127\n",
      "Epoch: 40, Step: 240, Loss: 0.0016896356828510761\n",
      "Epoch: 40, Step: 250, Loss: 0.030319109559059143\n",
      "Epoch: 40, Step: 260, Loss: 0.008908172138035297\n",
      "Epoch: 40, Step: 270, Loss: 0.029660899192094803\n",
      "Epoch: 40, Step: 280, Loss: 0.0014954435173422098\n",
      "Epoch: 40, Step: 290, Loss: 0.0011162154842168093\n",
      "Epoch: 40, Step: 300, Loss: 0.0015542801702395082\n",
      "Epoch: 41, Step: 0, Loss: 0.006363105960190296\n",
      "Epoch: 41, Step: 10, Loss: 0.0013752068625763059\n",
      "Epoch: 41, Step: 20, Loss: 0.0006913540419191122\n",
      "Epoch: 41, Step: 30, Loss: 0.061000458896160126\n",
      "Epoch: 41, Step: 40, Loss: 0.004329866264015436\n",
      "Epoch: 41, Step: 50, Loss: 0.001336212269961834\n",
      "Epoch: 41, Step: 60, Loss: 0.0020077608060091734\n",
      "Epoch: 41, Step: 70, Loss: 0.0022706545423716307\n",
      "Epoch: 41, Step: 80, Loss: 0.0022609871812164783\n",
      "Epoch: 41, Step: 90, Loss: 0.028002580627799034\n",
      "Epoch: 41, Step: 100, Loss: 0.0027662948705255985\n",
      "Epoch: 41, Step: 110, Loss: 0.001258808420971036\n",
      "Epoch: 41, Step: 120, Loss: 0.0018191079143434763\n",
      "Epoch: 41, Step: 130, Loss: 0.00282321497797966\n",
      "Epoch: 41, Step: 140, Loss: 0.0014457819052040577\n",
      "Epoch: 41, Step: 150, Loss: 0.03160247579216957\n",
      "Epoch: 41, Step: 160, Loss: 0.0015819740947335958\n",
      "Epoch: 41, Step: 170, Loss: 0.0028030730318278074\n",
      "Epoch: 41, Step: 180, Loss: 0.004028819501399994\n",
      "Epoch: 41, Step: 190, Loss: 0.0013739117421209812\n",
      "Epoch: 41, Step: 200, Loss: 0.0032251295633614063\n",
      "Epoch: 41, Step: 210, Loss: 0.001783078070729971\n",
      "Epoch: 41, Step: 220, Loss: 0.032984450459480286\n",
      "Epoch: 41, Step: 230, Loss: 0.0018941035959869623\n",
      "Epoch: 41, Step: 240, Loss: 0.036881495267152786\n",
      "Epoch: 41, Step: 250, Loss: 0.0020309367682784796\n",
      "Epoch: 41, Step: 260, Loss: 0.002190999686717987\n",
      "Epoch: 41, Step: 270, Loss: 0.03226460516452789\n",
      "Epoch: 41, Step: 280, Loss: 0.002473067259415984\n",
      "Epoch: 41, Step: 290, Loss: 0.030091531574726105\n",
      "Epoch: 41, Step: 300, Loss: 0.00012465640611480922\n",
      "Epoch: 42, Step: 0, Loss: 0.002065313281491399\n",
      "Epoch: 42, Step: 10, Loss: 0.0015491354279220104\n",
      "Epoch: 42, Step: 20, Loss: 0.002464176621288061\n",
      "Epoch: 42, Step: 30, Loss: 0.0011592414230108261\n",
      "Epoch: 42, Step: 40, Loss: 0.0023439833894371986\n",
      "Epoch: 42, Step: 50, Loss: 0.0030090762302279472\n",
      "Epoch: 42, Step: 60, Loss: 0.07402706146240234\n",
      "Epoch: 42, Step: 70, Loss: 0.0019462618511170149\n",
      "Epoch: 42, Step: 80, Loss: 0.0013289120979607105\n",
      "Epoch: 42, Step: 90, Loss: 0.0012903929455205798\n",
      "Epoch: 42, Step: 100, Loss: 0.01549499947577715\n",
      "Epoch: 42, Step: 110, Loss: 0.0014209211803972721\n",
      "Epoch: 42, Step: 120, Loss: 0.0132538927718997\n",
      "Epoch: 42, Step: 130, Loss: 0.0017421005759388208\n",
      "Epoch: 42, Step: 140, Loss: 0.034012384712696075\n",
      "Epoch: 42, Step: 150, Loss: 0.0014709641691297293\n",
      "Epoch: 42, Step: 160, Loss: 0.001640872098505497\n",
      "Epoch: 42, Step: 170, Loss: 0.02985072322189808\n",
      "Epoch: 42, Step: 180, Loss: 0.05567802116274834\n",
      "Epoch: 42, Step: 190, Loss: 0.0024084895849227905\n",
      "Epoch: 42, Step: 200, Loss: 0.0015615131705999374\n",
      "Epoch: 42, Step: 210, Loss: 0.00386845413595438\n",
      "Epoch: 42, Step: 220, Loss: 0.0012997323647141457\n",
      "Epoch: 42, Step: 230, Loss: 0.0014180154539644718\n",
      "Epoch: 42, Step: 240, Loss: 0.0013388199731707573\n",
      "Epoch: 42, Step: 250, Loss: 0.0023651053197681904\n",
      "Epoch: 42, Step: 260, Loss: 0.03471975773572922\n",
      "Epoch: 42, Step: 270, Loss: 0.0015964596532285213\n",
      "Epoch: 42, Step: 280, Loss: 0.0032668719068169594\n",
      "Epoch: 42, Step: 290, Loss: 0.007649894338101149\n",
      "Epoch: 42, Step: 300, Loss: 0.0005809108261018991\n",
      "Epoch: 43, Step: 0, Loss: 0.0019408753141760826\n",
      "Epoch: 43, Step: 10, Loss: 0.0014522194396704435\n",
      "Epoch: 43, Step: 20, Loss: 0.0008851353777572513\n",
      "Epoch: 43, Step: 30, Loss: 0.0025309007614851\n",
      "Epoch: 43, Step: 40, Loss: 0.0019148876890540123\n",
      "Epoch: 43, Step: 50, Loss: 0.0026308749802410603\n",
      "Epoch: 43, Step: 60, Loss: 0.0020511101465672255\n",
      "Epoch: 43, Step: 70, Loss: 0.0016377794090658426\n",
      "Epoch: 43, Step: 80, Loss: 0.0008666102075949311\n",
      "Epoch: 43, Step: 90, Loss: 0.0019044411601498723\n",
      "Epoch: 43, Step: 100, Loss: 0.0020104211289435625\n",
      "Epoch: 43, Step: 110, Loss: 0.001494081923738122\n",
      "Epoch: 43, Step: 120, Loss: 0.002274425234645605\n",
      "Epoch: 43, Step: 130, Loss: 0.0018806648440659046\n",
      "Epoch: 43, Step: 140, Loss: 0.0013671315973624587\n",
      "Epoch: 43, Step: 150, Loss: 0.002501283073797822\n",
      "Epoch: 43, Step: 160, Loss: 0.0015413434011861682\n",
      "Epoch: 43, Step: 170, Loss: 0.0054357317276299\n",
      "Epoch: 43, Step: 180, Loss: 0.0014189861249178648\n",
      "Epoch: 43, Step: 190, Loss: 0.0011018074583262205\n",
      "Epoch: 43, Step: 200, Loss: 0.0017150985077023506\n",
      "Epoch: 43, Step: 210, Loss: 0.0023476143833249807\n",
      "Epoch: 43, Step: 220, Loss: 0.0019260295666754246\n",
      "Epoch: 43, Step: 230, Loss: 0.0011722841300070286\n",
      "Epoch: 43, Step: 240, Loss: 0.001570856780745089\n",
      "Epoch: 43, Step: 250, Loss: 0.0008031070465222001\n",
      "Epoch: 43, Step: 260, Loss: 0.0022731530480086803\n",
      "Epoch: 43, Step: 270, Loss: 0.0009492285316810012\n",
      "Epoch: 43, Step: 280, Loss: 0.03432867303490639\n",
      "Epoch: 43, Step: 290, Loss: 0.0015416560927405953\n",
      "Epoch: 43, Step: 300, Loss: 3.112745616817847e-06\n",
      "Epoch: 44, Step: 0, Loss: 0.002629014663398266\n",
      "Epoch: 44, Step: 10, Loss: 0.001951955840922892\n",
      "Epoch: 44, Step: 20, Loss: 0.001579264528118074\n",
      "Epoch: 44, Step: 30, Loss: 0.0020598904229700565\n",
      "Epoch: 44, Step: 40, Loss: 0.0022015641443431377\n",
      "Epoch: 44, Step: 50, Loss: 0.002426463644951582\n",
      "Epoch: 44, Step: 60, Loss: 0.00164728460367769\n",
      "Epoch: 44, Step: 70, Loss: 0.0025158370845019817\n",
      "Epoch: 44, Step: 80, Loss: 0.001980279106646776\n",
      "Epoch: 44, Step: 90, Loss: 0.0034831964876502752\n",
      "Epoch: 44, Step: 100, Loss: 0.0017314060823991895\n",
      "Epoch: 44, Step: 110, Loss: 0.0013199176173657179\n",
      "Epoch: 44, Step: 120, Loss: 0.001389245968312025\n",
      "Epoch: 44, Step: 130, Loss: 0.0016338317655026913\n",
      "Epoch: 44, Step: 140, Loss: 0.01466650515794754\n",
      "Epoch: 44, Step: 150, Loss: 0.0012262575328350067\n",
      "Epoch: 44, Step: 160, Loss: 0.0008686424698680639\n",
      "Epoch: 44, Step: 170, Loss: 0.0011066687293350697\n",
      "Epoch: 44, Step: 180, Loss: 0.003702467307448387\n",
      "Epoch: 44, Step: 190, Loss: 0.0019409706583246589\n",
      "Epoch: 44, Step: 200, Loss: 0.03578314185142517\n",
      "Epoch: 44, Step: 210, Loss: 0.031698159873485565\n",
      "Epoch: 44, Step: 220, Loss: 0.0012965947389602661\n",
      "Epoch: 44, Step: 230, Loss: 0.0024123184848576784\n",
      "Epoch: 44, Step: 240, Loss: 0.0011594397947192192\n",
      "Epoch: 44, Step: 250, Loss: 0.0017143823206424713\n",
      "Epoch: 44, Step: 260, Loss: 0.0013350613880902529\n",
      "Epoch: 44, Step: 270, Loss: 0.0016228251624852419\n",
      "Epoch: 44, Step: 280, Loss: 0.001812205882743001\n",
      "Epoch: 44, Step: 290, Loss: 0.002029511146247387\n",
      "Epoch: 44, Step: 300, Loss: 0.024598026648163795\n",
      "Epoch: 45, Step: 0, Loss: 0.001353696919977665\n",
      "Epoch: 45, Step: 10, Loss: 0.03779901936650276\n",
      "Epoch: 45, Step: 20, Loss: 0.0020710702519863844\n",
      "Epoch: 45, Step: 30, Loss: 0.001494376570917666\n",
      "Epoch: 45, Step: 40, Loss: 0.0009756640065461397\n",
      "Epoch: 45, Step: 50, Loss: 0.03165816888213158\n",
      "Epoch: 45, Step: 60, Loss: 0.0015168297104537487\n",
      "Epoch: 45, Step: 70, Loss: 0.0018080571899190545\n",
      "Epoch: 45, Step: 80, Loss: 0.002636996563524008\n",
      "Epoch: 45, Step: 90, Loss: 0.0013693240471184254\n",
      "Epoch: 45, Step: 100, Loss: 0.009729956276714802\n",
      "Epoch: 45, Step: 110, Loss: 0.0009164692601189017\n",
      "Epoch: 45, Step: 120, Loss: 0.0010443564970046282\n",
      "Epoch: 45, Step: 130, Loss: 0.001590890227816999\n",
      "Epoch: 45, Step: 140, Loss: 0.0010737415868788958\n",
      "Epoch: 45, Step: 150, Loss: 0.0010684514418244362\n",
      "Epoch: 45, Step: 160, Loss: 0.030848411843180656\n",
      "Epoch: 45, Step: 170, Loss: 0.0009523858316242695\n",
      "Epoch: 45, Step: 180, Loss: 0.030510567128658295\n",
      "Epoch: 45, Step: 190, Loss: 0.0006395047530531883\n",
      "Epoch: 45, Step: 200, Loss: 0.0009397317189723253\n",
      "Epoch: 45, Step: 210, Loss: 0.0013916208408772945\n",
      "Epoch: 45, Step: 220, Loss: 0.0014234739355742931\n",
      "Epoch: 45, Step: 230, Loss: 0.006286629941314459\n",
      "Epoch: 45, Step: 240, Loss: 0.0014646316412836313\n",
      "Epoch: 45, Step: 250, Loss: 0.0017670756205916405\n",
      "Epoch: 45, Step: 260, Loss: 0.0019591101445257664\n",
      "Epoch: 45, Step: 270, Loss: 0.002995593473315239\n",
      "Epoch: 45, Step: 280, Loss: 0.001358384033665061\n",
      "Epoch: 45, Step: 290, Loss: 0.0024532186798751354\n",
      "Epoch: 45, Step: 300, Loss: 0.00020200162543915212\n",
      "Epoch: 46, Step: 0, Loss: 0.0014535065274685621\n",
      "Epoch: 46, Step: 10, Loss: 0.002721240511164069\n",
      "Epoch: 46, Step: 20, Loss: 0.0012468069326132536\n",
      "Epoch: 46, Step: 30, Loss: 0.0010733816307038069\n",
      "Epoch: 46, Step: 40, Loss: 0.0019038720056414604\n",
      "Epoch: 46, Step: 50, Loss: 0.025940611958503723\n",
      "Epoch: 46, Step: 60, Loss: 0.0016614412888884544\n",
      "Epoch: 46, Step: 70, Loss: 0.0014468056615442038\n",
      "Epoch: 46, Step: 80, Loss: 0.0011374509194865823\n",
      "Epoch: 46, Step: 90, Loss: 0.0012136977165937424\n",
      "Epoch: 46, Step: 100, Loss: 0.02658098191022873\n",
      "Epoch: 46, Step: 110, Loss: 0.0018722660606727004\n",
      "Epoch: 46, Step: 120, Loss: 0.003122144378721714\n",
      "Epoch: 46, Step: 130, Loss: 0.0024758432991802692\n",
      "Epoch: 46, Step: 140, Loss: 0.0010796358110383153\n",
      "Epoch: 46, Step: 150, Loss: 0.0010989964939653873\n",
      "Epoch: 46, Step: 160, Loss: 0.0015023568412289023\n",
      "Epoch: 46, Step: 170, Loss: 0.0026741651818156242\n",
      "Epoch: 46, Step: 180, Loss: 0.0025934483855962753\n",
      "Epoch: 46, Step: 190, Loss: 0.0009458066779188812\n",
      "Epoch: 46, Step: 200, Loss: 0.0016417743172496557\n",
      "Epoch: 46, Step: 210, Loss: 0.002239413559436798\n",
      "Epoch: 46, Step: 220, Loss: 0.0014617303386330605\n",
      "Epoch: 46, Step: 230, Loss: 0.0013432640116661787\n",
      "Epoch: 46, Step: 240, Loss: 0.0011798858176916838\n",
      "Epoch: 46, Step: 250, Loss: 0.001542265759781003\n",
      "Epoch: 46, Step: 260, Loss: 0.001263630809262395\n",
      "Epoch: 46, Step: 270, Loss: 0.0010588661534711719\n",
      "Epoch: 46, Step: 280, Loss: 0.0015063731698319316\n",
      "Epoch: 46, Step: 290, Loss: 0.0012219585478305817\n",
      "Epoch: 46, Step: 300, Loss: 0.0010886139934882522\n",
      "Epoch: 47, Step: 0, Loss: 0.0012862901203334332\n",
      "Epoch: 47, Step: 10, Loss: 0.018056726083159447\n",
      "Epoch: 47, Step: 20, Loss: 0.0018368158489465714\n",
      "Epoch: 47, Step: 30, Loss: 0.0012042447924613953\n",
      "Epoch: 47, Step: 40, Loss: 0.0012616736348718405\n",
      "Epoch: 47, Step: 50, Loss: 0.001113677630200982\n",
      "Epoch: 47, Step: 60, Loss: 0.0008822700474411249\n",
      "Epoch: 47, Step: 70, Loss: 0.0010844878852367401\n",
      "Epoch: 47, Step: 80, Loss: 0.001583521836437285\n",
      "Epoch: 47, Step: 90, Loss: 0.002451420994475484\n",
      "Epoch: 47, Step: 100, Loss: 0.0018773953197523952\n",
      "Epoch: 47, Step: 110, Loss: 0.0021409629844129086\n",
      "Epoch: 47, Step: 120, Loss: 0.0007600373355671763\n",
      "Epoch: 47, Step: 130, Loss: 0.0024022567085921764\n",
      "Epoch: 47, Step: 140, Loss: 0.0016474744770675898\n",
      "Epoch: 47, Step: 150, Loss: 0.0017418682109564543\n",
      "Epoch: 47, Step: 160, Loss: 0.002215066459029913\n",
      "Epoch: 47, Step: 170, Loss: 0.0024095908738672733\n",
      "Epoch: 47, Step: 180, Loss: 0.0020459864754229784\n",
      "Epoch: 47, Step: 190, Loss: 0.0014541244599968195\n",
      "Epoch: 47, Step: 200, Loss: 0.001332729822024703\n",
      "Epoch: 47, Step: 210, Loss: 0.0019949967972934246\n",
      "Epoch: 47, Step: 220, Loss: 0.002139934804290533\n",
      "Epoch: 47, Step: 230, Loss: 0.0010732433293014765\n",
      "Epoch: 47, Step: 240, Loss: 0.002435157774016261\n",
      "Epoch: 47, Step: 250, Loss: 0.000987286097370088\n",
      "Epoch: 47, Step: 260, Loss: 0.0018398038810119033\n",
      "Epoch: 47, Step: 270, Loss: 0.0015540949534624815\n",
      "Epoch: 47, Step: 280, Loss: 0.004182619974017143\n",
      "Epoch: 47, Step: 290, Loss: 0.0012406232999637723\n",
      "Epoch: 47, Step: 300, Loss: 0.00033785676350817084\n",
      "Epoch: 48, Step: 0, Loss: 0.0011296388693153858\n",
      "Epoch: 48, Step: 10, Loss: 0.0027253490407019854\n",
      "Epoch: 48, Step: 20, Loss: 0.0015538892475888133\n",
      "Epoch: 48, Step: 30, Loss: 0.0019813445396721363\n",
      "Epoch: 48, Step: 40, Loss: 0.0019395190756767988\n",
      "Epoch: 48, Step: 50, Loss: 0.001032636733725667\n",
      "Epoch: 48, Step: 60, Loss: 0.0017741729971021414\n",
      "Epoch: 48, Step: 70, Loss: 0.0015315073542296886\n",
      "Epoch: 48, Step: 80, Loss: 0.0012626717798411846\n",
      "Epoch: 48, Step: 90, Loss: 0.034418705850839615\n",
      "Epoch: 48, Step: 100, Loss: 0.001673352555371821\n",
      "Epoch: 48, Step: 110, Loss: 0.0013699450064450502\n",
      "Epoch: 48, Step: 120, Loss: 0.0023113712668418884\n",
      "Epoch: 48, Step: 130, Loss: 0.0017212640959769487\n",
      "Epoch: 48, Step: 140, Loss: 0.0012402038555592299\n",
      "Epoch: 48, Step: 150, Loss: 0.001630383194424212\n",
      "Epoch: 48, Step: 160, Loss: 0.0012750867754220963\n",
      "Epoch: 48, Step: 170, Loss: 0.001364131923764944\n",
      "Epoch: 48, Step: 180, Loss: 0.060941431671381\n",
      "Epoch: 48, Step: 190, Loss: 0.0014110360061749816\n",
      "Epoch: 48, Step: 200, Loss: 0.0011439123190939426\n",
      "Epoch: 48, Step: 210, Loss: 0.0011050947941839695\n",
      "Epoch: 48, Step: 220, Loss: 0.0019094508606940508\n",
      "Epoch: 48, Step: 230, Loss: 0.0008740314515307546\n",
      "Epoch: 48, Step: 240, Loss: 0.0011920816032215953\n",
      "Epoch: 48, Step: 250, Loss: 0.0005946563906036317\n",
      "Epoch: 48, Step: 260, Loss: 0.0015731058083474636\n",
      "Epoch: 48, Step: 270, Loss: 0.0011129268677905202\n",
      "Epoch: 48, Step: 280, Loss: 0.0012499000877141953\n",
      "Epoch: 48, Step: 290, Loss: 0.001490558497607708\n",
      "Epoch: 48, Step: 300, Loss: 0.003062811680138111\n",
      "Epoch: 49, Step: 0, Loss: 0.0019505013478919864\n",
      "Epoch: 49, Step: 10, Loss: 0.002258503343909979\n",
      "Epoch: 49, Step: 20, Loss: 0.030267935246229172\n",
      "Epoch: 49, Step: 30, Loss: 0.0009502033935859799\n",
      "Epoch: 49, Step: 40, Loss: 0.0014347787946462631\n",
      "Epoch: 49, Step: 50, Loss: 0.0021681534126400948\n",
      "Epoch: 49, Step: 60, Loss: 0.0010613050544634461\n",
      "Epoch: 49, Step: 70, Loss: 0.0015205899253487587\n",
      "Epoch: 49, Step: 80, Loss: 0.001239411300048232\n",
      "Epoch: 49, Step: 90, Loss: 0.0017902578692883253\n",
      "Epoch: 49, Step: 100, Loss: 0.00144859217107296\n",
      "Epoch: 49, Step: 110, Loss: 0.0008596855914220214\n",
      "Epoch: 49, Step: 120, Loss: 0.0354449637234211\n",
      "Epoch: 49, Step: 130, Loss: 0.0016063724178820848\n",
      "Epoch: 49, Step: 140, Loss: 0.0015732961473986506\n",
      "Epoch: 49, Step: 150, Loss: 0.0026096589863300323\n",
      "Epoch: 49, Step: 160, Loss: 0.0013414269778877497\n",
      "Epoch: 49, Step: 170, Loss: 0.0023460853844881058\n",
      "Epoch: 49, Step: 180, Loss: 0.0012402567081153393\n",
      "Epoch: 49, Step: 190, Loss: 0.0014664808986708522\n",
      "Epoch: 49, Step: 200, Loss: 0.0011545621091499925\n",
      "Epoch: 49, Step: 210, Loss: 0.0015433812513947487\n",
      "Epoch: 49, Step: 220, Loss: 0.0012518324656412005\n",
      "Epoch: 49, Step: 230, Loss: 0.0007344948826357722\n",
      "Epoch: 49, Step: 240, Loss: 0.0018164451466873288\n",
      "Epoch: 49, Step: 250, Loss: 0.0014414697652682662\n",
      "Epoch: 49, Step: 260, Loss: 0.0017167781479656696\n",
      "Epoch: 49, Step: 270, Loss: 0.0013667299645021558\n",
      "Epoch: 49, Step: 280, Loss: 0.0021943312603980303\n",
      "Epoch: 49, Step: 290, Loss: 0.0011845130939036608\n",
      "Epoch: 49, Step: 300, Loss: 0.0009652000735513866\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    for step, (input_ids, seq_len, attention_mask, labels) in enumerate(train_loader):\n",
    "        input_ids = input_ids\n",
    "        attention_mask = attention_mask\n",
    "        labels = labels.float()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {step}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     16\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     18\u001b[0m all_predictions\u001b[38;5;241m.\u001b[39mextend(predictions)\n\u001b[1;32m     19\u001b[0m all_labels\u001b[38;5;241m.\u001b[39mextend(labels)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, (input_ids, seq_len, attention_mask, labels) in enumerate(test_loader):\n",
    "        input_ids = input_ids\n",
    "        attention_mask = attention_mask\n",
    "        labels = labels.float()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # 将预测结果从张量转换为列表\n",
    "        predictions = logits.squeeze(-1).cpu().numpy().tolist()\n",
    "        labels = labels.cpu().numpy().tolist()\n",
    "        print(len(predictions[0]))\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels)\n",
    "print(all_predictions)\n",
    "# report = metrics.classification_report(all_labels, all_predictions, target_names=['负向', '中性', \"正向\"])\n",
    "# print(\"Precision, Recall and F1-Score...\")\n",
    "# print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
